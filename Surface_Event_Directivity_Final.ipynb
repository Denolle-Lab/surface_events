{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f6cb81",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Surface Event Analysis\n",
    "###### This notebook analyzes surface event waveforms and calculates location, directivity, and velocity\n",
    "###### Francesca Skene\n",
    "###### fskene@uw.edu\n",
    "###### Created: 7/22/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021dc6f",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ea10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do analysis for alaska earthquake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cf759fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/wsd01/pnwstore/')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.geodetics import *\n",
    "from obspy.signal.cross_correlation import *\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "import requests\n",
    "import glob\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "import richdem as rd\n",
    "from pathlib import Path\n",
    "from pyproj import Proj,transform,Geod\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30de3d",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c32de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = WaveformClient()\n",
    "client2 = Client('IRIS')\n",
    "\n",
    "t_before = 120 #number of seconds before pick time\n",
    "t_after = 120 #number of seconds after pick time\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 30 #window length of the signal\n",
    "pr = 98 #percentile\n",
    "thr = 12 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "v_s = 1000 #shear wave velocity at the surface\n",
    "t_beginning = UTCDateTime(2001,1,1,0,0,0)\n",
    "t_end = UTCDateTime(2021,12,31,23,59)\n",
    "smooth_length = 5\n",
    "low_cut = 2\n",
    "high_cut = 8\n",
    "az_thr = 1000 #threshold of distance in meters from source location\n",
    "step = 100 #step every 100 m\n",
    "t_step = 1 #step every second\n",
    "ratio = 5.6915196\n",
    "colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96853aa",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b603cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that calculates picktimes at each station\n",
    "def pick_time(time, ref_env, data_env_dict, st, t_diff, t_before, fs):\n",
    "    pick_times,offsets, starttimes = [],[],[]\n",
    "    for i,key in enumerate(data_env_dict):\n",
    "        starttimes.append(st[i].stats.starttime)\n",
    "        xcor = correlate(data_env_dict[key],ref_env,int(50*fs))\n",
    "        index = np.argmax(xcor)\n",
    "        cc = round(xcor[index],9) #correlation coefficient\n",
    "        shift = 50*fs-index #how much it is shifted from the reference envelope\n",
    "        offset_time = time - shift/fs\n",
    "        p = time - shift/fs  # p is the new phase pick for each station\n",
    "        pick_times.append(p+t_diff[key])\n",
    "        offsets.append(offset_time + t_diff[key])\n",
    "    return pick_times, offsets, starttimes\n",
    "    \n",
    "def shift(pick_times, offsets, starttimes, t_diff):\n",
    "    shifts, vals =[],[]\n",
    "    for i,ii in enumerate(t_diff):\n",
    "        t_shift = offsets[i]-min(offsets)\n",
    "        vals.append((-1*t_diff[ii])+t_shift)\n",
    "        shifts.append(t_shift)\n",
    "        #plt.vlines(val, ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "    return shifts, vals\n",
    "\n",
    "# define functon that resamples the data\n",
    "def resample(st, fs):\n",
    "    for i in st:\n",
    "        i.detrend(type='demean')\n",
    "        i.taper(0.05)\n",
    "        i.resample(fs)   \n",
    "    return st\n",
    "\n",
    "# define function to calculate number of surface events per month\n",
    "def events_per_month(starttimes, events):\n",
    "    num_events = {}\n",
    "    for year in range (2001, 2021):\n",
    "        for month in range (1, 13):\n",
    "            Nevt = []\n",
    "            period = str(year)+\"_\"+str(month)\n",
    "            t0 = UTCDateTime(year, month, 1)\n",
    "            t1 = t0+3600*24*30\n",
    "            for i in range(0, len(starttimes)):\n",
    "                if t0<starttimes[i]<t1:\n",
    "                    Nevt.append(events[i])\n",
    "            if len(Nevt) != 0:\n",
    "                num_events[period]=len(Nevt)\n",
    "            if len(Nevt) == 0:\n",
    "                num_events[period] = 0\n",
    "\n",
    "    periods = list(num_events.keys())\n",
    "    num_of_events = list(num_events.values())\n",
    "    return periods, num_of_events\n",
    "\n",
    "# define function to fit data to\n",
    "def test_func(theta, a,theta0, c):\n",
    "    return a * np.cos(theta-theta0)+c\n",
    "\n",
    "# define a function to make plots of weighted data\n",
    "def weight_data(x_data,y_data,weight,test_func,v_s,stas):    \n",
    "    #weighting the data\n",
    "    tempx, tempy = [],[]\n",
    "    for i,ii in enumerate(x_data):\n",
    "        tempx.append([])\n",
    "        tempx[i].append([ii for l in range(0,weight[i])])\n",
    "        tempy.append([])\n",
    "        tempy[i].append([y_data[i] for l in range(0,weight[i])])   \n",
    "    weighted_x = sum(sum(tempx, []),[])\n",
    "    weighted_y = sum(sum(tempy, []),[])\n",
    "   \n",
    "    #optimizing parameters to fit weighted data to test_function\n",
    "    params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(weighted_x), weighted_y, p0=None)\n",
    "    d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "    if params[0]<0:\n",
    "        direction = params[1]+pi \n",
    "    else:\n",
    "        direction = params[1]   \n",
    "    fmax = max(d)\n",
    "    fmin = min(d)\n",
    "    v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "    return v, direction, d\n",
    "\n",
    "# define function to predict synthetic arrival times\n",
    "def travel_time(t0, x, y, vs, sta_x, sta_y):\n",
    "    dist = np.sqrt((sta_x - x)**2 + (sta_y - y)**2)\n",
    "    tt = t0 + dist/vs\n",
    "    return tt\n",
    "\n",
    "# define function to compute residual sum of squares\n",
    "def error(synth_arrivals,arrivals, weight):\n",
    "    res = (arrivals - synth_arrivals)*weight \n",
    "    res_sqr = res**2\n",
    "    mse = np.mean(res_sqr)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# define function to iterate through grid and calculate travel time residuals\n",
    "def gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals, weight):\n",
    "    rss_mat = np.zeros((len(t0),len(x_vect),len(y_vect)))\n",
    "    rss_mat[:,:,:] = np.nan\n",
    "    for i in range(len(t0)):\n",
    "        for j in range(len(x_vect)):\n",
    "            for k in range(len(y_vect)):\n",
    "                synth_arrivals = []\n",
    "                for h in range(len(sta_x)):\n",
    "                    tt = travel_time(t0[i],x_vect[j],y_vect[k],vs,sta_x[h],sta_y[h])\n",
    "                    synth_arrivals.append(tt)\n",
    "                rss = error(np.array(synth_arrivals),np.array(arrivals), np.array(weight))\n",
    "                rss_mat[i,j,k] = rss\n",
    "    return rss_mat\n",
    "\n",
    "# define function to find lower-left corner of grid and grid size based on height of volcano\n",
    "def start_latlon(elevation, ratio, center_lat, center_lon):\n",
    "    side_length = elevation * ratio\n",
    "    l = side_length/2\n",
    "    hypotenuse = l*np.sqrt(2)\n",
    "    d = distance.geodesic(meters = hypotenuse)\n",
    "    start_lat = d.destination(point=[center_lat,center_lon], bearing=225)[0]\n",
    "    start_lon = d.destination(point=[center_lat,center_lon], bearing=225)[1]\n",
    "    return start_lat, start_lon, side_length\n",
    "\n",
    "# define function to convert the location index into latitude and longitude\n",
    "def location(x_dist, y_dist, start_lat, start_lon):\n",
    "    bearing = 90-np.rad2deg(np.arctan(y_dist/x_dist))\n",
    "    dist = np.sqrt((x_dist)**2 + (y_dist)**2)\n",
    "    d = distance.geodesic(meters = dist)\n",
    "    loc_lat = d.destination(point=[start_lat,start_lon], bearing=bearing)[0]\n",
    "    loc_lon = d.destination(point=[start_lat,start_lon], bearing=bearing)[1]\n",
    "    return loc_lat, loc_lon, d\n",
    "\n",
    "# define function to find diameter in meters of the error on the location\n",
    "def error_diameter(new_array):\n",
    "    min_idx = np.min(new_array[:,1])\n",
    "    max_idx = np.max(new_array[:,1])\n",
    "    difference = max_idx-min_idx\n",
    "    diameter_m = difference*1000\n",
    "    return diameter_m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d472ef4",
   "metadata": {},
   "source": [
    "##  Import and organize metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22a659",
   "metadata": {},
   "source": [
    "### 1. Volcano Data (network and station, labeled with volcano name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b099703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('Data/Volcano_Metadata_50km.csv')\n",
    "df_xd = pd.read_csv('Data/XD_Metadata_50km.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9656e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center latitude, center longitude, elevation(m), left_trim, right_trim, bottom_trim, top_trim \n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5, 10000, 3000, 15000, 7000]\n",
    "volc_lat_lon['Mt_Adams'] = [46.202621, -121.4906384, 3743.2, 5000, 3000, 4000, 2000]\n",
    "volc_lat_lon['Mt_Baker'] = [48.7773426,  -121.8132008, 3287.6, 0, 0, 0, 2000]\n",
    "# change the lat and lon of mt st helens to the middle of the dome instead of the highest point\n",
    "volc_lat_lon['Mt_St_Helens'] =[46.200472222222224,-122.18883611111112,2549, 10000, 10000, 17000, 15000] #[46.1912, -122.1944, 2549]\n",
    "volc_lat_lon['Glacier_Peak'] = [48.1112273, -121.1139922, 3213, 14000, 10000, 8000, 10000]\n",
    "volc_lat_lon['Crater_Lake']=[42.907745, -122.143494, 1883, 60000, 0, 90000, 0]\n",
    "volc_lat_lon['Mt_Hood']=[45.373221, -121.696509, 3428.7, 18000, 50000, 35000, 65000]\n",
    "volc_lat_lon['Newberry']=[43.7220653, -121.2344654, 2435, 53000, 12000, 70000, 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3886236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the lower left corner and grid size based on volcano elevation\n",
    "# [start_lat = lower_left latitude of gridsearch square, start lon = lower left longitude of gridsearhc square, side length of grid search square]\n",
    "volc_grid = {}\n",
    "for volc in volc_lat_lon:\n",
    "    elevation = volc_lat_lon[volc][2]\n",
    "    center_lat = volc_lat_lon[volc][0]\n",
    "    center_lon = volc_lat_lon[volc][1]\n",
    "    start_lat, start_lon, side_length = start_latlon(elevation, ratio, center_lat, center_lon)\n",
    "    volc_grid[volc] = [start_lat, start_lon, side_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEM data \n",
    "dem_data_dict = {}\n",
    "for name in volc_lat_lon:\n",
    "    if volc_lat_lon[name][0]>46:\n",
    "        dem = rio.open('DEM_data/'+str(name)+'/'+str(name)+'.tif') #washington volcanoes\n",
    "        dem_array = dem.read(1).astype('float64')\n",
    "        dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "        crs = dem.crs\n",
    "    else:\n",
    "        dem = rio.open('DEM_data/'+str(name)+'/_w001001.adf') #oregon volcanoes\n",
    "        dem_array = dem.read(1).astype('float64')\n",
    "        dem_array[dem_array == -3.4028234663852886e+38] = np.nan #gets rid of edge effects\n",
    "        crs = dem.crs\n",
    "#     volc = rd.rdarray(dem_array, no_data=-9999)\n",
    "#     slope = rd.TerrainAttribute(volc,attrib = 'slope_riserun')\n",
    "#     aspect = rd.TerrainAttribute(volc, attrib = 'aspect')\n",
    "#     dem_data_dict[name] = {'data':dem_array, 'elevation':volc, 'slope':slope, 'aspect':aspect}\n",
    "    dem_data_dict[name]={'data':dem_array, 'crs':crs, 'left':dem.bounds[0], 'right':dem.bounds[2], 'bottom':dem.bounds[1], 'top':dem.bounds[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f07df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_dict = {}\n",
    "lat_lon_dict['Mt_Rainier']={'tick_lons':[-121.65, -121.7, -121.75, -121.8, -121.85],\n",
    "                            'tick_lats':[46.75,46.8,46.85,46.9,46.95]}\n",
    "lat_lon_dict['Mt_St_Helens']={'tick_lons':[-122.10,-122.15,-122.2,-122.25],\n",
    "                              'tick_lats':[46.16, 46.18, 46.20, 46.22]}\n",
    "lat_lon_dict['Mt_Adams']={'tick_lons':[-121.6, -121.55, -121.5, -121.45, -121.4],\n",
    "                          'tick_lats':[46.16, 46.18, 46.20, 46.22]}\n",
    "lat_lon_dict['Mt_Baker']={'tick_lons':[ -121.7, -121.75, -121.80, -121.85, -121.90, -121.95],\n",
    "                          'tick_lats':[48.71, 48.74, 48.77, 48.80, 48.83, 48.86]}\n",
    "lat_lon_dict['Mt_Hood']={'tick_lons':[-121.58, -121.62, -121.66, -121.70, -121.74],\n",
    "                         'tick_lats':[45.3, 45.33, 45.36, 45.39, 45.42]}\n",
    "lat_lon_dict['Glacier_Peak']={'tick_lons':[ -121.07, -121.09, -121.11, -121.13, -121.15],\n",
    "                              'tick_lats':[48.08, 48.10, 48.12, 48.14, 48.16]}\n",
    "lat_lon_dict['Newberry']={'tick_lons':[-121.11, -121.15, -121.19, -121.23, -121.27, -121.31, -121.34],\n",
    "                          'tick_lats':[43.64, 43.67, 43.70, 43.73, 43.76, 43.79, 43.82]}\n",
    "lat_lon_dict['Crater_Lake']={'tick_lons':[ -121.98, -122.06, -122.14, -122.22, -122.30],\n",
    "                             'tick_lats':[42.80, 42.85, 42.90, 42.95, 43.00]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ca268",
   "metadata": {},
   "source": [
    "### 3. Surface Event Data from PNSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3894296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= pd.read_csv('Data/PNSN_Pick_Label.csv')\n",
    "label = df3['Label'].values.tolist()\n",
    "surface_label = df3[df3['Label']== 'su']['Label'].values.tolist()\n",
    "net_temp = df3[df3['Label']== 'su']['Network'].values.tolist()\n",
    "sta_temp = df3[df3['Label']== 'su']['Station'].values.tolist()\n",
    "evt_id_temp = df3[df3['Label']== 'su']['Event_ID'].values.tolist()\n",
    "start_time_temp = df3[df3['Label']== 'su']['Picktime'].values.tolist()                               \n",
    "\n",
    "net,sta,evt_id,start_time = [],[],[],[]\n",
    "for i,ii in enumerate(start_time_temp):\n",
    "    if t_beginning<UTCDateTime(ii)<t_end:\n",
    "        net.append(net_temp[i])\n",
    "        sta.append(sta_temp[i])\n",
    "        evt_id.append(evt_id_temp[i])\n",
    "        start_time.append(ii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63cef84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stas = set(sta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fa267",
   "metadata": {},
   "source": [
    "## Calculating directivity and velocity of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c702477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[328.6945241859738, 160.84213734220666, 307.69019535396717, 41.414377893618266, 191.29350042783122, 125.7463168995276, 228.2919263531945, 234.15872663672013]\n",
      "{'OBSR': 4.116914294270217, 'PARA': 3.4285718961042755, 'VOIT': 3.166789755147114, 'FMW': 4.0822309000135, 'LON': 4.931238679118249, 'RCM': 3.8708554094051806, 'RER': 4.235470646628232, 'STAR': 3.843288533504584}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1859745/1278450569.py:345: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  evt_data = evt_data.append({'event_ID':event_ID,\n",
      "/tmp/ipykernel_1859745/1278450569.py:362: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[351.4021281010925, 152.87231377141123, 284.7336677550344, 148.66861408792673, 179.69530637354566]\n",
      "[0.27400926807892106, 52.49050368141323, 167.0058535752869, 177.75456422477433, 307.55543514397993, 110.84595732294859, 160.91345384051843]\n",
      "{'HOA': 4.3494789781334875, 'LOO': 5.160197016572619, 'REM': 4.313669012370462, 'SEP': 4.673819751694761, 'STD': 4.589095642058308, 'SUG': 4.846784724990389, 'VALT': 4.668639678436948}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1859745/1278450569.py:345: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  evt_data = evt_data.append({'event_ID':event_ID,\n",
      "/tmp/ipykernel_1859745/1278450569.py:362: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[322.8210456930278, 74.62093000985384, 175.43352916256367, 108.43764311137173, 252.71209317399246, 290.92042450698693]\n",
      "{'OBSR': 3.8701118929937737, 'PANH': 3.9994459704398078, 'PARA': 3.0315893027432343, 'RCM': 3.6822959217661015, 'RER': 4.503004909111476, 'STAR': 3.5127784583680053}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1859745/1278450569.py:345: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  evt_data = evt_data.append({'event_ID':event_ID,\n",
      "/tmp/ipykernel_1859745/1278450569.py:362: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.032434491718478, 119.49142594651136, 337.16441044234, 62.81717958377496, 196.57278280821325, 50.79910217213227, 102.29924683210324, 241.4771206171851]\n",
      "{'HOA': 4.674844366715641, 'SEP': 5.304825149574877, 'STD': 5.610068476266837, 'SUG': 5.190113246599319, 'SWF2': 5.167221435038694, 'VALT': 5.222235015381343, 'EDM': 5.487324110440884, 'SHW': 4.555809500449931}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1859745/1278450569.py:345: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  evt_data = evt_data.append({'event_ID':event_ID,\n",
      "/tmp/ipykernel_1859745/1278450569.py:362: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201.60349740756072, 211.5437469185109, 167.47582574042104, 258.09592407402727, 137.2054331557018]\n",
      "[12.03872158309441, 30.014685064464256, 85.57970418927067, 201.1905095822004, 35.69245105425263, 255.71397184862542]\n",
      "{'HOA': 4.040831835131182, 'LOO': 4.275055880180992, 'SEP': 5.290169138955983, 'SWF2': 4.417330527667562, 'VALT': 5.061742990708041, 'SHW': 3.813328095976009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1859745/1278450569.py:345: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  evt_data = evt_data.append({'event_ID':event_ID,\n",
      "/tmp/ipykernel_1859745/1278450569.py:362: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[343.23876827004744, 196.62079063646715, 182.15808520530123, 190.72369106572862, 294.5418275188675, 147.5025145006419, 206.08085322627986, 199.3542050205068, 139.7118748551453, 256.51630624059965, 229.70010535638934]\n",
      "[344.01513661259725, 343.6930665109807, 182.23929163569957, 191.12124912434476, 144.58280587557846, 206.5302251365105, 303.4137159903471, 201.10354548288308, 138.7511081713318, 256.93650239185763, 230.53726657456315]\n",
      "[251.10413279300678, 152.22351632335244, 242.47055700448246, 61.33629201234672]\n",
      "{'MBW': 3.3351293609841375, 'RPW2': 3.277656957378474, 'SAXON': 3.0815217895017697, 'SHUK': 3.0842545645825936}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1859745/1278450569.py:345: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  evt_data = evt_data.append({'event_ID':event_ID,\n",
      "/tmp/ipykernel_1859745/1278450569.py:362: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "evt_data = pd.DataFrame(columns = ['event_ID','location_latitude','location_longitude','location_uncertainty(m)',\n",
    "                                   'origin_time','direction(degrees)', \n",
    "                                   'direction_sharpness(degrees)','direction_snr(degrees)','duration(sec)',\n",
    "                                   'params_std_deviation', 'velocity(m/s)','number_of_stations'])\n",
    "\n",
    "sta_freq=pd.DataFrame(columns = all_stas)\n",
    "reject_evts=pd.DataFrame(columns = ['event_ID'])\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "for n in range(0,10):    \n",
    "    event_ID = str(evt_id[n])\n",
    "    try:\n",
    "        time = UTCDateTime(start_time[n])\n",
    "        if net != 'CN' and evt_id[n]!=evt_id[n-1]:\n",
    "            reference = str(net[n]+'.'+sta[n])\n",
    "            try:\n",
    "                associated_volcano = df[df['Station']== sta[n]]['Volcano_Name'].values[0]\n",
    "            except: \n",
    "                pass\n",
    "            #get info for stations within 50km of volcano that event ocurred at\n",
    "            stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "            networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "            latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "            longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "            elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "            if stations.count(\"LON\")>0 and stations.count(\"LO2\")>0:\n",
    "                index = stations.index(\"LO2\")\n",
    "                del stations[index]\n",
    "                del networks[index]\n",
    "                del latitudes[index]\n",
    "                del longitudes[index]\n",
    "                del elevations[index]\n",
    "\n",
    "            #Get all waveforms for that event based on stations and times\n",
    "            bulk = [] \n",
    "            for m in range(0, len(networks)):\n",
    "                bulk.append([networks[m], stations[m], '*', '*', time-t_before, time+t_after])\n",
    "            st = client.get_waveforms_bulk(bulk)\n",
    "            #remove unwanted data\n",
    "            for tr in st:\n",
    "                cha = tr.stats.channel\n",
    "                if cha[0:2] != 'BH' and cha[0:2] != 'EH' and cha[0:2] != 'HH':\n",
    "                    st.remove(tr)\n",
    "                try:\n",
    "                    if len(tr.data)/tr.stats.sampling_rate < 239.9:\n",
    "                        st.remove(tr)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            #resampling the data to 40Hz for each trace\n",
    "            st = resample(st,fs) \n",
    "\n",
    "            #Plotting all traces for one event with channel z, SNR>10, and bandpasses between 2-12Hz\n",
    "            SNR,SNR_weight, stas,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],{},{}\n",
    "#             fig = plt.figure(figsize = (11,8), dpi=80)\n",
    "#             fig.suptitle('evtID:UW'+ event_ID+associated_volcano)\n",
    "#             ax1 = plt.subplot(1,1,1)\n",
    "            iplot = 0\n",
    "            for i,ii in enumerate(st):\n",
    "                network = ii.stats.network\n",
    "                station = ii.stats.station\n",
    "                ii.detrend(type = 'demean')\n",
    "                ii.filter('bandpass',freqmin=2.0,freqmax=12.0,corners=2,zerophase=True)\n",
    "                cha = ii.stats.channel\n",
    "                starttime = ii.stats.starttime\n",
    "                max_amp_time = np.argmax(ii.data)/fs\n",
    "                signal_window = ii.copy()\n",
    "                noise_window = ii.copy()\n",
    "                signal_window.trim(starttime + t_before - 20, starttime + t_before - 20 + window)\n",
    "                noise_window.trim(starttime + t_before - window -10, starttime + t_before - 10)\n",
    "                snr = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                               / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "\n",
    "                if cha[-1] == 'Z' and snr>thr and 100<max_amp_time<200:\n",
    "                    t = ii.times()\n",
    "                    t_diff[network+'.'+station] = starttime-time \n",
    "                    # enveloping the data \n",
    "                    data_envelope = obspy.signal.filter.envelope(ii.data[115*fs:150*fs])\n",
    "                    data_envelope /= np.max(data_envelope)\n",
    "                    data_envelope += iplot*1.5\n",
    "                    # finding the time of max amplitude of each event\n",
    "                    max_amp_times.append(max_amp_time)\n",
    "                    max_amp = np.max(ii.data)      \n",
    "                    # creating envelope data dictionary to calculate picktimes\n",
    "                    data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "                    data_env_dict[network+'.'+station]= data_envelope\n",
    "#                     b,e = 115,150\n",
    "#                     ax1.plot(t[b*fs:e*fs],ii.data[b*fs:e*fs]/np.max(np.abs(ii.data))+iplot*1.5)\n",
    "#                     ax1.plot(t[115*fs:150*fs], data_envelope, color = 'k')\n",
    "#                     ax1.set_xlabel('time (seconds)')\n",
    "#                     ax1.set_xlim([b,e])\n",
    "#                     plt.text(t[e*fs], iplot*1.5, 'SNR:'+str(round(snr,2)))\n",
    "#                     plt.text(t[b*fs], iplot*1.5, station)\n",
    "                    iplot = iplot+1\n",
    "                    stas.append(ii.stats.station)\n",
    "                    nets.append(ii.stats.network)\n",
    "                    SNR.append(snr)\n",
    "                    SNR_weight.append(int(snr))\n",
    "                else:\n",
    "                    st.remove(ii)\n",
    "\n",
    "            if len(st)<4:  \n",
    "                continue\n",
    "\n",
    "            # get peak frequency of each event\n",
    "            # read and preprocess data\n",
    "            st.taper(max_percentage=0.01,max_length=20)\n",
    "            st.trim(starttime=time-20,endtime=time+30) \n",
    "\n",
    "            # make plot of spectra\n",
    "            char_freq, sharp_weight= [],[]\n",
    "            for i in range(len(stas)):\n",
    "                data = st.select(station=stas[i],component=\"Z\")[0].data*100\n",
    "                f,psd=scipy.signal.welch(data,fs=st[0].stats.sampling_rate,nperseg=81,noverlap=1)\n",
    "                #just get the frequencies within the filter band\n",
    "                above_low_cut = [f>low_cut]\n",
    "                below_high_cut = [f<high_cut]\n",
    "                in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "                f = f[in_band]\n",
    "                psd = psd[in_band]\n",
    "\n",
    "                # calculate characteristic frequency and report\n",
    "                char_freq_max = f[np.argmax(psd)]\n",
    "                char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "                psd_cumsum = np.cumsum(psd)\n",
    "                psd_sum = np.sum(psd)\n",
    "                char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "                char_freq.append(char_freq_mean)\n",
    "\n",
    "                # weighting the data by the spikiness of the PSD vs frequency graphs\n",
    "                ratio = (np.mean(psd)/np.max(psd))\n",
    "                sharp_weight.append(int(1/(ratio**2)*20))\n",
    "\n",
    "            lats, lons, elevs, r, theta = ([] for i in range(5)) \n",
    "            ref = str(nets[0]+'.'+stas[0])\n",
    "            try:\n",
    "                ref_env = data_env_dict[reference]\n",
    "            except:\n",
    "                ref_env = data_env_dict[ref]\n",
    "\n",
    "            # calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "            pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs) #calculate picktimes\n",
    "            shifts, vals = shift(pick_times, offsets, starttimes, t_diff)\n",
    "\n",
    "#             iplot = 0 \n",
    "            durations = []\n",
    "            for i in range(len(stas)):\n",
    "                max_amp_time = max_amp_times[i]\n",
    "                duration = (max_amp_time-vals[i])*2\n",
    "                durations.append(duration)\n",
    "#                 ax1.vlines(vals[i], ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "#                 plt.text(t[110*fs], iplot*1.5, 'duration:'+str(int(duration))+'s')\n",
    "                a = stations.index(stas[i])\n",
    "                lats.append(latitudes[a])\n",
    "                lons.append(longitudes[a])\n",
    "                elevs.append(elevations[a])\n",
    "#                 iplot = iplot+1    \n",
    "            avg_duration = np.mean(durations)\n",
    "#             plt.saxvefig('../surface_events/wiggles/'+event_ID+associated_volcano+'.png')\n",
    "#             del fig\n",
    "            # input necessary data for grid search\n",
    "            arrivals = shifts\n",
    "            sta_lats = lats\n",
    "            sta_lons= lons\n",
    "\n",
    "            # define grid origin in lat,lon and grid dimensions in m\n",
    "            lat_start = volc_grid[associated_volcano][0]\n",
    "            lon_start = volc_grid[associated_volcano][1]\n",
    "            side_length = volc_grid[associated_volcano][2]\n",
    "\n",
    "            # create the grid of locations\n",
    "            sta_x = []\n",
    "            sta_y = []\n",
    "            for i in range(len(sta_lats)):\n",
    "                x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "                y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "                sta_x.append(x_dist)\n",
    "                sta_y.append(y_dist)\n",
    "            x_vect = np.arange(0, side_length, step)\n",
    "            y_vect = np.arange(0, side_length, step)\n",
    "            t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "\n",
    "            # carry out the gridsearch\n",
    "            rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,SNR_weight)\n",
    "            loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "            # find the latitude and longitude of the location index\n",
    "            loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "            err_thr = np.min(np.log10(rss_mat))+.05\n",
    "            thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "            diameter = error_diameter(thr_array)\n",
    "\n",
    "            # calculating azimuth for each station with respect to the middle of the volcano\n",
    "            for i in range(len(stas)):\n",
    "                u,b,c = (gps2dist_azimuth(loc_lat, loc_lon, lats[i], lons[i], a=6378137.0, f=0.0033528106647474805))\n",
    "                r.append(u)\n",
    "                theta.append(b)\n",
    "                \n",
    "            bin1,bin2,bin3 = [],[],[]\n",
    "            print(theta)\n",
    "            for i in theta:\n",
    "                if 0<=i<=120:\n",
    "                    bin1.append(i)\n",
    "                if 121<=i<=240:\n",
    "                    bin2.append(i)\n",
    "                if 241<=i<=360:\n",
    "                    bin3.append(i)\n",
    "\n",
    "            if bin1 == [] or bin2 == [] or bin3 == []:\n",
    "                continue\n",
    "            #manipulating the data\n",
    "            data = {'azimuth_deg':theta, 'freq':char_freq, 'station':stas, 'distance_m':r, \n",
    "                    'weight':sharp_weight, 'SNR':SNR, 'colors':colors[0:len(stas)]}\n",
    "            DF = pd.DataFrame(data, index = None)\n",
    "            DF2 = DF.sort_values('azimuth_deg')\n",
    "            #Taking out stations that are too close to the location when looking at azimuth \n",
    "            drops = []\n",
    "            for i in range(len(DF2)):\n",
    "                value = DF2.loc[i,'distance_m']\n",
    "                if value < az_thr:\n",
    "                    drops.append(i)\n",
    "            DF3 = DF2.drop(drops)\n",
    "            y_data =  DF3[\"freq\"].values.tolist()\n",
    "            Sta2 = DF3[\"station\"].values.tolist()\n",
    "            dist2 = DF3[\"distance_m\"].values.tolist()\n",
    "            spike_weight = DF3[\"weight\"].values.tolist()\n",
    "            SNR2 = DF3['SNR'].values.tolist()\n",
    "            colors2 = DF3['colors'].values.tolist()\n",
    "            x_data =  np.asarray(DF3[\"azimuth_deg\"].values.tolist())\n",
    "            x_points = np.linspace(0,360, 100)\n",
    "\n",
    "            #optimizing parameters to fit data to test_function\n",
    "            params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(x_data), y_data, p0=None)\n",
    "            perr = np.sqrt(np.diag(params_covariance))\n",
    "            std_deviation = str(round(perr[0],9))+','+str(round(perr[1],9))+','+str(round(perr[2],9))\n",
    "            d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "            len_r = int(max(r))\n",
    "\n",
    "            if params[0]<0:\n",
    "                direction = params[1]+pi \n",
    "            else:\n",
    "                direction = params[1]\n",
    "\n",
    "            fmax = max(d)\n",
    "            fmin = min(d)\n",
    "            v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "            # convert the direction from polar to cartesian coordinates\n",
    "#             dy = len_r*np.sin(direction)\n",
    "#             dx = len_r*np.cos(direction)     \n",
    "            # create figure showing effects of different weights on the data\n",
    "    #         fig, ax = plt.subplots(3, figsize=(8,11), sharex=True, sharey=True)\n",
    "\n",
    "            # weight the data\n",
    "            title = 'Sharpness'\n",
    "            v_sharp,direction_sharp,d_sharp = weight_data(x_data,y_data,sharp_weight,test_func,v_s,stas)\n",
    "#             dy_sharp = len_r*np.sin(direction_sharp)\n",
    "#             dx_sharp = len_r*np.cos(direction_sharp)    \n",
    "\n",
    "            title = 'SNR'\n",
    "            v_snr,direction_snr,d_snr = weight_data(x_data,y_data,SNR_weight,test_func,v_s,stas)  \n",
    "#             dy_snr = len_r*np.sin(direction_snr)\n",
    "#             dx_snr = len_r*np.cos(direction_snr) \n",
    "\n",
    "#             fig,ax = plt.subplots(1,1,figsize=(8,11))\n",
    "#             fig.suptitle('Fitted Cosine Curves')       \n",
    "#             ax.set_ylabel('characteristic frequency(Hz)')\n",
    "#             ax.set_xlabel(('azimuth(degrees)'))\n",
    "#             for i in range (0,len(Sta2)):\n",
    "#                 ax.scatter(x_data[i], y_data[i], s = (SNR_weight[i]**2),label=Sta2[i], color = colors2[i])\n",
    "#             ax.plot(x_data,y_data, '--', label='rawdata')\n",
    "#             ax.plot(x_points, d, label = 'original')\n",
    "#             ax.plot(x_points, d_sharp, label = 'sharpness')\n",
    "#             ax.plot(x_points, d_snr, label = 'snr')\n",
    "#             ax.legend(loc='upper right', fontsize = 10)\n",
    "#             plt.grid(True)\n",
    "#             plt.savefig('../surface_events/curves_freq_data/evtID:UW'+ event_ID+associated_volcano+'.png')\n",
    "\n",
    "#             making plots of directivity and location\n",
    "            crs = dem_data_dict[associated_volcano]['crs']\n",
    "            data = dem_data_dict[associated_volcano]['data']\n",
    "            info = volc_lat_lon[associated_volcano]\n",
    "            p2 = Proj(crs,preserve_units=False)\n",
    "            p1 = Proj(proj='latlong',preserve_units=False)\n",
    "            # gives the lower left grid point in the grid search\n",
    "            left_x,bottom_y = transform(p1,p2,volc_grid[associated_volcano][1],volc_grid[associated_volcano][0]) # p1,p2,lon,lat\n",
    "            # gives the left right, bottom, top of the grid\n",
    "            grid_bounds = [left_x, left_x+volc_grid[associated_volcano][2], bottom_y, bottom_y+volc_grid[associated_volcano][2]]\n",
    "            left, right = dem_data_dict[associated_volcano]['left'],dem_data_dict[associated_volcano]['right']\n",
    "            bottom, top = dem_data_dict[associated_volcano]['bottom'],dem_data_dict[associated_volcano]['top']\n",
    "            contour_x,contour_y = np.meshgrid(left_x+x_vect,bottom_y+y_vect)\n",
    "            center_x, center_y = transform(p1,p2,info[1],info[0])\n",
    "            loc_x,loc_y=transform(p1,p2,loc_lon,loc_lat)\n",
    "            duration=avg_duration\n",
    "            length_factor = duration/100\n",
    "\n",
    "            fig,ax = plt.subplots(1,1,figsize=(8,11))\n",
    "            a = ax.imshow(data,extent=[left, right, bottom, top],cmap='gist_earth')\n",
    "            contours = ax.contour(contour_x,contour_y,np.log10(rss_mat[int(loc_idx[0]),:,:].T),cmap='plasma')\n",
    "            ax.scatter(center_x, center_y, s=100,marker='*',c='r')\n",
    "            plt.arrow(loc_x,loc_y,dy*length_factor,dx*length_factor, color='w', width=170, label='no weight')\n",
    "            plt.arrow(loc_x,loc_y,dy_sharp*length_factor,dx_sharp*length_factor, color='k', width=170, label='sharpness')\n",
    "            plt.arrow(loc_x,loc_y,dy_snr*length_factor,dx_snr*length_factor, color='m', width=170, label='snr')\n",
    "            #plotting the stations on top of this as triangles\n",
    "            for i, ii in enumerate(stas):\n",
    "                sta_x,sta_y = transform(p1,p2,lons[i],lats[i])\n",
    "                if left+info[3]<sta_x<right-info[4] and bottom+info[5]<sta_y<top-info[6]:\n",
    "                    ax.plot(sta_x,sta_y, c='k', marker=\"^\")\n",
    "                    ax.text(sta_x,sta_y,ii, c='k')\n",
    "\n",
    "            getting lat and lon tick marks on the axis\n",
    "            tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "            tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "            ticks_x = []\n",
    "            ticks_y = []\n",
    "            for i in range(len(tick_lons)):\n",
    "                tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "                ticks_x.append(tick_x)\n",
    "                ticks_y.append(tick_y)\n",
    "                tick_lons[i]=str(tick_lons[i])\n",
    "                tick_lats[i]=str(tick_lats[i])\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax1 = divider.append_axes('right', size='4%', pad=0.1)\n",
    "            cax2 = divider.append_axes('right', size='4%', pad=1.3)\n",
    "            ax.set_title('Location and Directivity')\n",
    "            ax.set_xlabel('longitudes(DD)')\n",
    "            ax.set_ylabel('latitudes(DD)')\n",
    "            ax.set_xticks(ticks_x)\n",
    "            ax.set_xticklabels(tick_lons)\n",
    "            ax.set_yticks(ticks_y)\n",
    "            ax.set_yticklabels(tick_lats)\n",
    "            ax.clabel(contours)\n",
    "            cbar = plt.colorbar(a, cax=cax1)\n",
    "            cbar.ax.tick_params(labelsize=10)\n",
    "            cbar.set_label('elevation(m)\\n', rotation=270, labelpad = 13)\n",
    "            cbar2 = plt.colorbar(contours, cax=cax2)\n",
    "            cbar2.ax.tick_params(labelsize=10)\n",
    "            cbar2.set_label('RMS error on location\\n', rotation=270, labelpad = 13)\n",
    "            ax.set_xlim(left+info[3],right-info[4])\n",
    "            ax.set_ylim(bottom+info[5],top-info[6])\n",
    "            ax.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('../surface_events/loc_direction/evtID:UW'+ event_ID+associated_volcano+'.png',bbox_inches=\"tight\")\n",
    "\n",
    "            # make a dataframe of the data\n",
    "            evt_data = evt_data.append({'event_ID':event_ID, \n",
    "                        'location_latitude': loc_lat,\n",
    "                        'location_longitude': loc_lon,\n",
    "                        'location_uncertainty(m)':diameter,\n",
    "                        'origin_time': min(offsets)-int(loc_idx[0]),\n",
    "                        'direction(degrees)':np.rad2deg(direction),\n",
    "                        'direction_sharpness(degrees)':np.rad2deg(direction_sharp),\n",
    "                        'direction_snr(degrees)':np.rad2deg(direction_snr),\n",
    "                        'duration(sec)':avg_duration,\n",
    "                        'params_std_deviation':std_deviation, \n",
    "                        'velocity(m/s)':v, \n",
    "                        'number_of_stations':len(stas)}, ignore_index = True)\n",
    "                        \n",
    "            dict_temp = {}\n",
    "            for i in range(len(stas)):\n",
    "                dict_temp[stas[i]] = char_freq[i]\n",
    "            print(dict_temp)    \n",
    "            sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n",
    "\n",
    "            evt_data.to_csv('~/surface_events/Event_Data.csv', index=False)\n",
    "            sta_freq.to_csv('~/surface_events/Station_frequency_data.csv', index=False)\n",
    "    except:\n",
    "        reject_evts = reject_evts.append({'event_ID':[event_ID]}, ignore_index = True)\n",
    "        reject_evts.to_csv('~/surface_events/Rejects5.csv', index=False)\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6192672e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_ID</th>\n",
       "      <th>location_latitude</th>\n",
       "      <th>location_longitude</th>\n",
       "      <th>location_uncertainty(m)</th>\n",
       "      <th>origin_time</th>\n",
       "      <th>direction(degrees)</th>\n",
       "      <th>direction_sharpness(degrees)</th>\n",
       "      <th>direction_snr(degrees)</th>\n",
       "      <th>duration(sec)</th>\n",
       "      <th>params_std_deviation</th>\n",
       "      <th>velocity(m/s)</th>\n",
       "      <th>number_of_stations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3179368</td>\n",
       "      <td>46.858073</td>\n",
       "      <td>-121.778376</td>\n",
       "      <td>58000</td>\n",
       "      <td>2021-12-22T04:47:34.075000Z</td>\n",
       "      <td>177.360174</td>\n",
       "      <td>312.511566</td>\n",
       "      <td>176.525934</td>\n",
       "      <td>25.15</td>\n",
       "      <td>0.291803418,1.944002982,0.230200717</td>\n",
       "      <td>45.19483</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3179088</td>\n",
       "      <td>46.219705</td>\n",
       "      <td>-122.191982</td>\n",
       "      <td>11000</td>\n",
       "      <td>2021-12-19T07:26:45.000000Z</td>\n",
       "      <td>97.733349</td>\n",
       "      <td>112.788311</td>\n",
       "      <td>113.736889</td>\n",
       "      <td>22.407143</td>\n",
       "      <td>0.256038082,1.667656686,0.134776134</td>\n",
       "      <td>26.165764</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3178938</td>\n",
       "      <td>46.839141</td>\n",
       "      <td>-121.748277</td>\n",
       "      <td>36000</td>\n",
       "      <td>2021-12-17T16:47:07.000000Z</td>\n",
       "      <td>-13.480934</td>\n",
       "      <td>-39.834375</td>\n",
       "      <td>291.049492</td>\n",
       "      <td>18.233333</td>\n",
       "      <td>0.402479871,1.182150269,0.240952305</td>\n",
       "      <td>74.691366</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3177563</td>\n",
       "      <td>46.205321</td>\n",
       "      <td>-122.204963</td>\n",
       "      <td>18000</td>\n",
       "      <td>2021-12-07T09:42:15.775000Z</td>\n",
       "      <td>88.963761</td>\n",
       "      <td>90.634594</td>\n",
       "      <td>59.344675</td>\n",
       "      <td>17.38125</td>\n",
       "      <td>0.209694594,0.946468032,0.150203815</td>\n",
       "      <td>42.674987</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3177373</td>\n",
       "      <td>46.199023</td>\n",
       "      <td>-122.204972</td>\n",
       "      <td>15000</td>\n",
       "      <td>2021-12-06T19:05:48.475000Z</td>\n",
       "      <td>114.960697</td>\n",
       "      <td>115.317283</td>\n",
       "      <td>114.260319</td>\n",
       "      <td>12.075</td>\n",
       "      <td>0.286471291,0.177636134,0.140861335</td>\n",
       "      <td>218.203861</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3177603</td>\n",
       "      <td>48.804575</td>\n",
       "      <td>-121.810955</td>\n",
       "      <td>85000</td>\n",
       "      <td>2021-12-05T11:22:35.000000Z</td>\n",
       "      <td>184.275275</td>\n",
       "      <td>185.710833</td>\n",
       "      <td>203.119905</td>\n",
       "      <td>6.35</td>\n",
       "      <td>0.204428343,1.135221251,0.112181072</td>\n",
       "      <td>37.224629</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  event_ID location_latitude location_longitude location_uncertainty(m)  \\\n",
       "0  3179368         46.858073        -121.778376                   58000   \n",
       "1  3179088         46.219705        -122.191982                   11000   \n",
       "2  3178938         46.839141        -121.748277                   36000   \n",
       "3  3177563         46.205321        -122.204963                   18000   \n",
       "4  3177373         46.199023        -122.204972                   15000   \n",
       "5  3177603         48.804575        -121.810955                   85000   \n",
       "\n",
       "                   origin_time direction(degrees)  \\\n",
       "0  2021-12-22T04:47:34.075000Z         177.360174   \n",
       "1  2021-12-19T07:26:45.000000Z          97.733349   \n",
       "2  2021-12-17T16:47:07.000000Z         -13.480934   \n",
       "3  2021-12-07T09:42:15.775000Z          88.963761   \n",
       "4  2021-12-06T19:05:48.475000Z         114.960697   \n",
       "5  2021-12-05T11:22:35.000000Z         184.275275   \n",
       "\n",
       "  direction_sharpness(degrees) direction_snr(degrees) duration(sec)  \\\n",
       "0                   312.511566             176.525934         25.15   \n",
       "1                   112.788311             113.736889     22.407143   \n",
       "2                   -39.834375             291.049492     18.233333   \n",
       "3                    90.634594              59.344675      17.38125   \n",
       "4                   115.317283             114.260319        12.075   \n",
       "5                   185.710833             203.119905          6.35   \n",
       "\n",
       "                  params_std_deviation velocity(m/s) number_of_stations  \n",
       "0  0.291803418,1.944002982,0.230200717      45.19483                  8  \n",
       "1  0.256038082,1.667656686,0.134776134     26.165764                  7  \n",
       "2  0.402479871,1.182150269,0.240952305     74.691366                  6  \n",
       "3  0.209694594,0.946468032,0.150203815     42.674987                  8  \n",
       "4  0.286471291,0.177636134,0.140861335    218.203861                  6  \n",
       "5  0.204428343,1.135221251,0.112181072     37.224629                  4  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee21a9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPW2</th>\n",
       "      <th>BRKS</th>\n",
       "      <th>HUSB</th>\n",
       "      <th>PRLK</th>\n",
       "      <th>TMBU</th>\n",
       "      <th>GLK</th>\n",
       "      <th>MEW</th>\n",
       "      <th>GPW</th>\n",
       "      <th>PANH</th>\n",
       "      <th>PINE</th>\n",
       "      <th>...</th>\n",
       "      <th>BLIS</th>\n",
       "      <th>MDW</th>\n",
       "      <th>ASR</th>\n",
       "      <th>FRIS</th>\n",
       "      <th>LNO</th>\n",
       "      <th>ETW</th>\n",
       "      <th>SNB</th>\n",
       "      <th>CAVE</th>\n",
       "      <th>SNI</th>\n",
       "      <th>LOO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.160197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.999446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.275056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.277657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows  178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       RPW2 BRKS HUSB PRLK TMBU  GLK  MEW  GPW      PANH PINE  ... BLIS  MDW  \\\n",
       "0       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN       NaN  NaN  ...  NaN  NaN   \n",
       "1       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN       NaN  NaN  ...  NaN  NaN   \n",
       "2       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.999446  NaN  ...  NaN  NaN   \n",
       "3       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN       NaN  NaN  ...  NaN  NaN   \n",
       "4       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN       NaN  NaN  ...  NaN  NaN   \n",
       "5  3.277657  NaN  NaN  NaN  NaN  NaN  NaN  NaN       NaN  NaN  ...  NaN  NaN   \n",
       "\n",
       "   ASR FRIS  LNO  ETW  SNB CAVE  SNI       LOO  \n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN       NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  5.160197  \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN       NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN       NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  4.275056  \n",
       "5  NaN  NaN  NaN  NaN  NaN  NaN  NaN       NaN  \n",
       "\n",
       "[6 rows x 178 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sta_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e181f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
