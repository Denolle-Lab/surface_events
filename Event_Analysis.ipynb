{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8e775a",
   "metadata": {},
   "source": [
    "# Event Analysis\n",
    "This notebook analyzes the surface event data 10/10/22 fskene@uw.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.geodetics import *\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "import requests\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "import richdem as rd\n",
    "from pathlib import Path\n",
    "import os \n",
    "import glob\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import asarray as ar,exp\n",
    "from pyproj import Proj,transform,Geod\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "import warnings\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish parameters\n",
    "window = 30 #window length of the signal\n",
    "thr = 12 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "v_s = 1000 #shear wave velocity at the surface\n",
    "colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3\n",
    "ratio = 5.6915196\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def volc_loc_thr(left_lat, bottom_lon, sidelength):\n",
    "    d = distance.geodesic(meters = sidelength)\n",
    "    right_lat = d.destination(point=[left_lat,bottom_lon], bearing=0)[0]\n",
    "    top_lon = d.destination(point=[left_lat,bottom_lon], bearing=90)[1]\n",
    "    return right_lat, top_lon\n",
    "\n",
    "def start_latlon(elevation, ratio, center_lat, center_lon):\n",
    "    side_length = elevation * ratio\n",
    "    l = side_length/2\n",
    "    hypotenuse = l*np.sqrt(2)\n",
    "    d = distance.geodesic(meters = hypotenuse)\n",
    "    start_lat = d.destination(point=[center_lat,center_lon], bearing=225)[0]\n",
    "    start_lon = d.destination(point=[center_lat,center_lon], bearing=225)[1]\n",
    "    return start_lat, start_lon, side_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220231f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and organize location data\n",
    "\n",
    "# Rainier\n",
    "Event_Data_r = pd.read_csv(\"Analysis_Data/Event_Data_Rainier.csv\")\n",
    "new_list = [np.nan]*(len(Event_Data_r))\n",
    "Event_Data_r['Label'] = new_list\n",
    "\n",
    "# St Helens\n",
    "Event_Data_st = pd.read_csv(\"Analysis_Data/Event_Data_St_Helens.csv\")\n",
    "new_list = [np.nan]*(len(Event_Data_st))\n",
    "Event_Data_st['Label'] = new_list\n",
    "\n",
    "# Hood\n",
    "Event_Data_h = pd.read_csv(\"Analysis_Data/Event_Data_Hood.csv\")\n",
    "new_list = [np.nan]*(len(Event_Data_h))\n",
    "Event_Data_h['Label'] = new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62283d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read xcel file with ground truth events (from Wes)\n",
    "\n",
    "# Mt Rainier\n",
    "inputExcelFile =\"Data/surfaceFlows_cloud.xlsx\"\n",
    "# Reading an excel file\n",
    "excelFile = pd.read_excel (inputExcelFile)\n",
    "# Converting excel file into CSV file\n",
    "excelFile.to_csv (\"Data/Wes_Cat_rainier.csv\", index = None, header=True)\n",
    "# Reading and Converting the output csv file into a dataframe object\n",
    "known_events_r = pd.DataFrame(pd.read_csv(\"Data/Wes_Cat_rainier.csv\"))\n",
    "starttimes_rainier = []\n",
    "starttimes_rainier2 = []\n",
    "for i in range(len(known_events_r['Date'])):\n",
    "    try:\n",
    "        time = known_events_r['Time Start'][i].split(':')\n",
    "        if time[1][0] == '0':\n",
    "            time[1] = time[1][1]\n",
    "        if time[2][0] == '0':\n",
    "            time[2] = time[2][1]\n",
    "        date = known_events_r['Date'][i].split('-')\n",
    "        if date[1][0] == '0':\n",
    "            date[1] = date[1][1]\n",
    "        if date[2][0] == '0':\n",
    "            date[2] = date[2][1]\n",
    "        starttimes_rainier.append(str(UTCDateTime(int(date[0]),int(date[1]),int(date[2]),int(time[0]))))\n",
    "        starttimes_rainier2.append(str(UTCDateTime(int(date[0]),int(date[1]),int(date[2]),int(time[0]), int(time(1)))))\n",
    "    except:\n",
    "        continue \n",
    "        \n",
    "# Mt St Helens\n",
    "inputExcelFile =\"Data/surfaceFlows_cloud_st_helens.xlsx\"\n",
    "excelFile = pd.read_excel (inputExcelFile)\n",
    "excelFile.to_csv (\"Data/Wes_Cat_st_helens.csv\", index = None, header=True)\n",
    "known_events_sh = pd.DataFrame(pd.read_csv(\"Data/Wes_Cat_st_helens.csv\"))\n",
    "starttimes_st_helens = []\n",
    "starttimes_st_helens2 = []\n",
    "for i in range(len(known_events_sh['Date'])):\n",
    "    try:\n",
    "        time = known_events_sh['Time'][i].split(':')\n",
    "        if time[1][0] == '0':\n",
    "            time[1] = time[1][1]\n",
    "        if time[2][0] == '0':\n",
    "            time[2] = time[2][1]\n",
    "        date = known_events_sh['Date'][i].split('-')\n",
    "        if date[1][0] == '0':\n",
    "            date[1] = date[1][1]\n",
    "        if date[2][0] == '0':\n",
    "            date[2] = date[2][1]\n",
    "        starttimes_st_helens.append(str(UTCDateTime(int(date[0]),int(date[1]),int(date[2]),int(time[0]))))\n",
    "        starttimes_st_helens2.append(str(UTCDateTime(int(date[0]),int(date[1]),int(date[2]),int(time[0]),int(time[1]))))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "# Mt Hood\n",
    "inputExcelFile =\"Data/surfaceFlows_cloud_hood.xlsx\"\n",
    "excelFile = pd.read_excel (inputExcelFile)\n",
    "excelFile.to_csv (\"Data/Wes_Cat_hood.csv\", index = None, header=True)\n",
    "known_events_h = pd.DataFrame(pd.read_csv(\"Data/Wes_Cat_hood.csv\"))\n",
    "starttimes_hood = []\n",
    "for i in range(len(known_events_h['Date'])):\n",
    "    try:\n",
    "        time = known_events_h['Time'][i].split(':')\n",
    "        if time[1][0] == '0':\n",
    "            time[1] = time[1][1]\n",
    "        if time[2][0] == '0':\n",
    "            time[2] = time[2][1]\n",
    "        date = known_events_h['Date'][i].split('-')\n",
    "        if date[1][0] == '0':\n",
    "            date[1] = date[1][1]\n",
    "        if date[2][0] == '0':\n",
    "            date[2] = date[2][1]\n",
    "        starttimes_hood.append(UTCDateTime(int(date[0]),int(date[1]),int(date[2]),int(time[0])))#,int(time[1])))#,int(time[2])))\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/wes_events_r.json\", 'w') as f:\n",
    "    json.dump(starttimes_rainier2, f, indent=2) \n",
    "    \n",
    "with open(\"Data/wes_events_st.json\", 'w') as f:\n",
    "    json.dump(starttimes_st_helens2, f, indent=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aea5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volcano location data\n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5, 10000, 17000, 13500, 5500]\n",
    "volc_lat_lon['Mt_St_Helens'] =[46.200472222222224,-122.18883611111112,2549, 10000, 10000, 17000, 15000] #[46.1912, -122.1944, 2549]\n",
    "volc_lat_lon['Mt_Hood']=[45.373221, -121.696509, 3428.7, 18000, 50000, 35000, 65000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb37acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEM data \n",
    "dem_data_dict = {}\n",
    "for name in volc_lat_lon:\n",
    "    if volc_lat_lon[name][0]>46:\n",
    "        if name == 'Mt_Rainier':\n",
    "            dem = rio.open('Data/DEM_data/'+str(name)+'/'+str(name)+'1.tif') #washington volcanoes\n",
    "            dem_array = dem.read(1).astype('float64')\n",
    "            dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "            crs = dem.crs\n",
    "        else:\n",
    "            dem = rio.open('Data/DEM_data/'+str(name)+'/'+str(name)+'.tif')\n",
    "            dem_array = dem.read(1).astype('float64')\n",
    "            dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "            crs = dem.crs\n",
    "    else:\n",
    "        dem = rio.open(str(name)+'/_w001001.adf') # Mt Hood\n",
    "        dem_array = dem.read(1).astype('float64')\n",
    "        dem_array[dem_array == -3.4028234663852886e+38] = np.nan #gets rid of edge effects\n",
    "        crs = dem.crs\n",
    "    dem_data_dict[name]={'data':dem_array, 'crs':crs, 'left':dem.bounds[0], 'right':dem.bounds[2], 'bottom':dem.bounds[1], 'top':dem.bounds[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_dem_data_dict = {}\n",
    "# name = 'Mt_Rainier'\n",
    "# if volc_lat_lon[name][0]>46:\n",
    "#     dem = rio.open('Data/DEM_data/'+str(name)+'/'+str(name)+'1.tif') #washington volcanoes\n",
    "#     dem_array = dem.read(1).astype('float64')\n",
    "#     dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "#     crs = dem.crs\n",
    "\n",
    "# r_dem_data_dict[name]={'data':dem_array, 'crs':crs, 'left':dem.bounds[0], 'right':dem.bounds[2], 'bottom':dem.bounds[1], 'top':dem.bounds[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748aa2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_dict = {}\n",
    "lat_lon_dict['Mt_Rainier']={'tick_lons':[-121.65, -121.7, -121.75, -121.8, -121.85],\n",
    "                            'tick_lats':[46.75,46.8,46.85,46.9,46.95]}\n",
    "lat_lon_dict['Mt_St_Helens']={'tick_lons':[-122.10,-122.15,-122.2,-122.25],\n",
    "                              'tick_lats':[46.16, 46.18, 46.20, 46.22]}\n",
    "lat_lon_dict['Mt_Hood']={'tick_lons':[-121.58, -121.62, -121.66, -121.70, -121.74],\n",
    "                         'tick_lats':[45.3, 45.33, 45.36, 45.39, 45.42]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b7c84",
   "metadata": {},
   "source": [
    "## Add Ground truth labels from Wes' Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mt Rainier\n",
    "wes_events_r = []\n",
    "event_ids_r = []\n",
    "\n",
    "# turning the times from strings to UTCDateTime objects\n",
    "times = list(Event_Data_r['origin_time'])\n",
    "temp = []\n",
    "for i in times: temp.append(UTCDateTime(UTCDateTime(i).strftime(\"%Y-%m-%d, %H\")))\n",
    "    \n",
    "# aligning the timing of the cataloged events with those in the csv\n",
    "overlaps = []\n",
    "temp2 = []\n",
    "for i in range(len(starttimes_rainier)):\n",
    "    if starttimes_rainier[i] in temp:\n",
    "        overlaps.append(starttimes_rainier[i])\n",
    "        temp2.append(known_events_r['Remarks'][i])\n",
    "        \n",
    "for i in range(len(overlaps)): \n",
    "    index = temp.index(overlaps[i])\n",
    "    if index != 30:\n",
    "        Event_Data_r['Label'][index]= temp2[i]\n",
    "        wes_events_r.append(index)\n",
    "        event_ids_r.append(int(Event_Data_r['event_ID'][index]))\n",
    "    \n",
    "    \n",
    "wes_events_r.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033366d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error in the middle and easiest to soft code the last four\n",
    "Event_Data_r['Label'][27] = 'Good KAUT, less so on PARA. Nice event overall'\n",
    "Event_Data_r['Label'][11] = 'Strong seismic, impulsive on PARA/KAUT'\n",
    "Event_Data_r['Label'][10] = 'Strong seismic signal, amplitude locate near Carbon/Russel. Infrasound on west side and CRBN'\n",
    "Event_Data_r['Label'][6] = 'PARA did not detect, west side stas did'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd576606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events on Mt St Helens\n",
    "wes_events_st = []\n",
    "event_ids_st = []\n",
    "\n",
    "times = list(Event_Data_st['origin_time'])\n",
    "temp = []\n",
    "for i in times: temp.append(UTCDateTime(UTCDateTime(i).strftime(\"%Y-%m-%d, %H\")))\n",
    "    \n",
    "# aligning the timing of the cataloged events with those in the csv\n",
    "overlaps = []\n",
    "temp2 = []\n",
    "for i in range(len(starttimes_st_helens)):\n",
    "    if starttimes_st_helens[i] in temp:\n",
    "        overlaps.append(starttimes_st_helens[i])\n",
    "        temp2.append(known_events_sh['Remarks'][i])\n",
    "        \n",
    "for i in range(len(overlaps)):\n",
    "    index = temp.index(overlaps[i])\n",
    "    Event_Data_st['Label'][index]= temp2[i] \n",
    "    wes_events_st.append(index)\n",
    "    event_ids_st.append(int(Event_Data_st['event_ID'][index]))\n",
    "    \n",
    "wes_events_st.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events on Mt Hood\n",
    "wes_events_h = []\n",
    "\n",
    "times = list(Event_Data_h['origin_time'])\n",
    "temp = []\n",
    "for i in times: temp.append(UTCDateTime(UTCDateTime(i).strftime(\"%Y-%m-%d, %H\")))\n",
    "    \n",
    "# aligning the timing og the cataloged events with those in the csv\n",
    "overlaps = []\n",
    "temp2 = []\n",
    "for i in range(len(starttimes_hood)):\n",
    "    if starttimes_hood[i] in temp:\n",
    "        overlaps.append(starttimes_hood[i])\n",
    "        temp2.append(known_events_h['Remarks'][i])\n",
    "        print(starttimes_hood[i],known_events_h['Remarks'][i])\n",
    "for i in range(len(overlaps)):\n",
    "    index = temp.index(overlaps[i])\n",
    "    if index != 30:\n",
    "        Event_Data_h['Label'][index]= temp2[i] \n",
    "        wes_events_h.append(index)\n",
    "    \n",
    "wes_events_h.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving lists of event indices\n",
    "with open(\"Data/event_ids_r.json\", 'w') as f:\n",
    "    json.dump(event_ids_r, f, indent=2) \n",
    "    \n",
    "with open(\"Data/event_ids_st.json\", 'w') as f:\n",
    "    json.dump(event_ids_st, f, indent=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the full DataFrame\n",
    "Event_Data = pd.concat([Event_Data_r, Event_Data_st, Event_Data_h], axis=0, ignore_index=True)\n",
    "\n",
    "# Compile all data into a final CSV\n",
    "Event_Data.to_csv('~/surface_events/Analysis_Data/Event_Data/Event_Data_Final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6ff06",
   "metadata": {},
   "source": [
    "# Histogram of frequencies at each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_data = pd.read_csv('Analysis_Data/Station_frequency_data_10_24.csv') \n",
    "sta_freq = {}\n",
    "for i in freq_data.columns:\n",
    "    df2=freq_data.dropna(subset=[i])\n",
    "    med_freq = np.median(df2[i])\n",
    "    if med_freq>0:\n",
    "        sta_freq[i] = med_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b95f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (17,4))\n",
    "plt.title('median frequency at each station')\n",
    "plt.bar(np.linspace(0,90,len(sta_freq)), sta_freq.values(), color=(['m','c']*41)+['m'],tick_label = list(sta_freq.keys()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('frequency(Hz)')\n",
    "plt.xlabel('station code')\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c955d",
   "metadata": {},
   "source": [
    "## Scatterplot of event locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lower left corner and grid size based on volcano elevation\n",
    "volc_grid = {}\n",
    "volc_loc_dict = {}\n",
    "for volc in volc_lat_lon:\n",
    "    elevation = volc_lat_lon[volc][2]\n",
    "    center_lat = volc_lat_lon[volc][0]\n",
    "    center_lon = volc_lat_lon[volc][1]\n",
    "    start_lat, start_lon, side_length = start_latlon(elevation, ratio, center_lat, center_lon)\n",
    "    left_lat, bottom_lon, sidelength = start_latlon(elevation, ratio, center_lat, center_lon)\n",
    "    right_lat, top_lon = volc_loc_thr(left_lat, bottom_lon, sidelength)\n",
    "    volc_grid[volc] = [start_lat, start_lon, side_length]\n",
    "    volc_loc_dict[volc] = [left_lat, right_lat, bottom_lon, top_lon, sidelength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9267aea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mtn_list = ['Mt_Rainier','Mt_St_Helens','Mt_Hood']\n",
    "for n in mtn_list:\n",
    "    lats = []\n",
    "    lons = []\n",
    "    evt_id = []\n",
    "    dir_snr = []\n",
    "    dir_sharp = []\n",
    "    direc = []\n",
    "    sharp = []\n",
    "    times = []\n",
    "\n",
    "    # selects events for each specific volcano based on its location\n",
    "    for i in range(len(Event_Data)):\n",
    "        if volc_loc_dict[n][0]<Event_Data['location_latitude'][i]<volc_loc_dict[n][1] and volc_loc_dict[n][2]<Event_Data['location_longitude'][i]<volc_loc_dict[n][3]:\n",
    "            lats.append(Event_Data['location_latitude'][i])\n",
    "            lons.append(Event_Data['location_longitude'][i])\n",
    "            evt_id.append(Event_Data['event_ID'][i])\n",
    "            dir_snr.append(Event_Data['direction_snr(degrees)'][i])\n",
    "            direc.append(Event_Data['direction(degrees)'][i])\n",
    "            dir_sharp.append(Event_Data['direction_sharpness(degrees)'][i])\n",
    "            times.append(Event_Data['origin_time'][i])\n",
    "    \n",
    "    # prepare data for plots\n",
    "    data = dem_data_dict[n]['data']\n",
    "    volc = rd.rdarray(data, no_data=-9999)\n",
    "    aspect = np.array(rd.TerrainAttribute(volc, attrib = 'aspect'))\n",
    "    slope = rd.TerrainAttribute(volc,attrib = 'slope_riserun')\n",
    "    associated_volcano = n\n",
    "    crs = dem_data_dict[associated_volcano]['crs']\n",
    "    data = dem_data_dict[associated_volcano]['data']\n",
    "    info = volc_lat_lon[associated_volcano]\n",
    "    p2 = Proj(crs,preserve_units=False)\n",
    "    p1 = Proj(proj='latlong',preserve_units=False)\n",
    "    # gives the lower left grid point in the grid search\n",
    "    left_x,bottom_y = transform(p1,p2,volc_grid[associated_volcano][1],volc_grid[associated_volcano][0]) # p1,p2,lon,lat\n",
    "    # gives the left right, bottom, top of the grid\n",
    "    grid_bounds = [left_x, left_x+volc_grid[associated_volcano][2], bottom_y, bottom_y+volc_grid[associated_volcano][2]]\n",
    "    left, right = dem_data_dict[associated_volcano]['left'],dem_data_dict[associated_volcano]['right']\n",
    "    bottom, top = dem_data_dict[associated_volcano]['bottom'],dem_data_dict[associated_volcano]['top']\n",
    "    center_x, center_y = transform(p1,p2,info[1],info[0])\n",
    "    \n",
    "    # scatter plot of locations\n",
    "    loc_x,loc_y = [],[]\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    fig,ax = plt.subplots(1,1,figsize=(8,11),dpi = 200)\n",
    "    a = ax.imshow(data,extent=[left, right, bottom, top],cmap='gist_earth', alpha = 0.7)\n",
    "    b = ax.imshow(aspect,extent=[left, right, bottom, top],cmap='bone', alpha = .2)\n",
    "    topo_contours = ax.contour(data, extent=[left, right, bottom, top],origin=\"upper\", colors = 'k',linewidths = 0.5, alpha = 0.6)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], marker='*', color='w', label='center of volcano',\n",
    "                              markerfacecolor='r', markersize=15),\n",
    "                       Line2D([0], [0], marker='.', color='w', label='estimated event location',\n",
    "                              markerfacecolor='k', markersize=15)]\n",
    "\n",
    "    for i, ii in enumerate(evt_id):\n",
    "        loc_lon,loc_lat = transform(p1,p2,lons[i],lats[i])\n",
    "        loc_x.append(loc_lon)\n",
    "        loc_y.append(loc_lat)\n",
    "\n",
    "    #getting lat and lon tick marks on the axis\n",
    "    tick_lons = lat_lon_dict[n]['tick_lons']\n",
    "    tick_lats = lat_lon_dict[n]['tick_lats']\n",
    "    ticks_x = []\n",
    "    ticks_y = []\n",
    "    for i in range(len(tick_lons)):\n",
    "        tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "        ticks_x.append(tick_x)\n",
    "        ticks_y.append(tick_y)\n",
    "        tick_lons[i]=str(tick_lons[i])\n",
    "        tick_lats[i]=str(tick_lats[i])\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax1 = divider.append_axes('right', size='2%', pad=0.1)\n",
    "    ax.set_xlabel('longitudes(DD)', fontsize = 10)\n",
    "    ax.set_ylabel('latitudes(DD)', fontsize = 10)\n",
    "    ax.set_xticks(ticks_x)\n",
    "    ax.set_xticklabels(tick_lons, fontsize = 10)\n",
    "    ax.set_yticks(ticks_y)\n",
    "    ax.set_yticklabels(tick_lats, fontsize = 10)\n",
    "    cbar = plt.colorbar(a, cax=cax1)\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    cbar.set_label('elevation(m)\\n', rotation=270, labelpad = 13, fontsize = 10)\n",
    "    ax.set_xlim(left+info[3],right-info[4])\n",
    "    ax.set_ylim(bottom+info[5],top-info[6])\n",
    "    #ax.set_ylim(bottom+info[5]+1000,top-info[6])\n",
    "    ax.scatter(loc_x,loc_y,s = 5, c='k', marker=\".\")\n",
    "    ax.clabel(topo_contours, topo_contours.levels, fontsize = 10, inline = True, inline_spacing = 0.5)\n",
    "    #ax.scatter(center_x, center_y, s=100,marker='*',c='r')\n",
    "\n",
    "    if n == 'Mt_Rainier' or n =='Mt_Hood':\n",
    "        ax.set_title(n.split('_')[0]+' '+n.split('_')[1]+' Locations', fontsize = 20)\n",
    "        cbar.set_label('elevation(m)\\n', rotation=270, labelpad = 13, fontsize = 10)\n",
    "    else:\n",
    "        ax.set_title(n.split('_')[0]+' '+n.split('_')[1]+' '+n.split('_')[2]+ ' Locations', fontsize= 20)\n",
    "        cbar.set_label('elevation(ft)\\n', rotation=270, labelpad = 13, fontsize = 10)\n",
    "\n",
    "    #ax.legend(handles=legend_elements, loc = 'upper right', fontsize = 10)\n",
    "    plt.savefig('locs_'+n+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f028cdae",
   "metadata": {},
   "source": [
    "## Histograms of Velocities at each Volcano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44164542",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mtn_list = ['Mt_Rainier','Mt_St_Helens', 'Mt_Hood']\n",
    "for n in mtn_list:\n",
    "    vel = []\n",
    "    times = []\n",
    "    new_vel = []\n",
    "    for i in range(len(Event_Data)):\n",
    "        if volc_loc_dict[n][0]<Event_Data['location_latitude'][i]<volc_loc_dict[n][1] and volc_loc_dict[n][2]<Event_Data['location_longitude'][i]<volc_loc_dict[n][3]:\n",
    "            vel.append(Event_Data['velocity(m/s)'][i])\n",
    "            times.append(Event_Data['origin_time'][i])\n",
    "    for i in vel:\n",
    "        if int(i) <= 300:\n",
    "            new_vel.append(i)\n",
    "    # histogram of velocities\n",
    "    a = np.median(new_vel)\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    fig,ax = plt.subplots(figsize = [11,8], dpi = 200)\n",
    "    if n == 'Mt_Rainier' or n =='Mt_Hood':\n",
    "        ax.set_title(n.split('_')[0]+' '+n.split('_')[1]+' Flow Velocities')\n",
    "    else:\n",
    "         ax.set_title(n.split('_')[0]+' '+n.split('_')[1]+' '+n.split('_')[2]+' Flow Velocities')   \n",
    "    ax.set_ylabel('number of events', fontsize = 10)\n",
    "    ax.set_xlabel('velocity (m/s)', fontsize = 10)\n",
    "    binwidth = 5\n",
    "    num_of_events = ax.hist(new_vel,bins=range(int(min(new_vel)), int(max(new_vel)) + binwidth, binwidth), color = 'lightskyblue',edgecolor = \"black\")\n",
    "    height = int(num_of_events[0].max()+5)\n",
    "    ax.grid('True')\n",
    "    ax.vlines(a,0,height-1,'firebrick','--', label = 'Median Velocity', linewidth = 3)\n",
    "    ax.set_xlim([0,300])\n",
    "    ax.set_ylim([0,70])\n",
    "    ax.legend(fontsize = 10)\n",
    "    ax.set_ylim(0,height)\n",
    "    plt.xticks(fontsize = 10)\n",
    "    plt.yticks(fontsize = 10)\n",
    "    plt.savefig('vels'+n+'.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd829ef4",
   "metadata": {},
   "source": [
    "## Aspect plotted versus azimuth, and Slope plotted versus velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e0bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEM data \n",
    "# r_dem_data_dict = {}\n",
    "# name = 'Mt_Rainier'\n",
    "# if volc_lat_lon[name][0]>46:\n",
    "#     dem = rio.open('Data/DEM_data/'+str(name)+'/'+str(name)+'1.tif') #washington volcanoes\n",
    "#     dem_array = dem.read(1).astype('float64')\n",
    "#     dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "#     crs = dem.crs\n",
    "\n",
    "# r_dem_data_dict[name]={'data':dem_array, 'crs':crs, 'left':dem.bounds[0], 'right':dem.bounds[2], 'bottom':dem.bounds[1], 'top':dem.bounds[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for n in mtn_list:\n",
    "    crs = dem_data_dict[n]['crs']\n",
    "    data = dem_data_dict[n]['data']\n",
    "    volc = rd.rdarray(data, no_data=-9999)\n",
    "    slope = rd.TerrainAttribute(volc,attrib = 'slope_riserun')\n",
    "    aspect = np.array(rd.TerrainAttribute(volc, attrib = 'aspect'))\n",
    "    p2 = Proj(crs,preserve_units=False)\n",
    "    p1 = Proj(proj='latlong',preserve_units=False)\n",
    "    # gives the lower left grid point in the grid search\n",
    "    left_x,bottom_y = transform(p1,p2,volc_grid[n][1],volc_grid[n][0])\n",
    "    # gives the left right, bottom, top of the grid\n",
    "    left, right = dem_data_dict[n]['left'],dem_data_dict[n]['right']\n",
    "    bottom, top = dem_data_dict[n]['bottom'],dem_data_dict[n]['top']\n",
    "    \n",
    "    side = int(volc_grid[n][2]/10)\n",
    "    \n",
    "    # interpolating the slope and aspect matrices to 100m by 100m boxes \n",
    "    a = int((left_x-left)/10)\n",
    "    b = a+side\n",
    "    c = (aspect.shape[0] - int((bottom_y-bottom)/10))-side\n",
    "    d = aspect.shape[0] - int((bottom_y-bottom)/10)\n",
    "\n",
    "    x = np.arange(a,b,1)\n",
    "    y = np.arange(c,d,1)\n",
    "\n",
    "    x2 = np.arange(a,b,10) # every 100m\n",
    "    y2 = np.arange(c,d,10) # every 100m\n",
    "\n",
    "    aspect_data = np.array(aspect[c:d,a:b])\n",
    "\n",
    "    aspect_interp_mat = RectBivariateSpline(y,x,aspect_data, s = 0)\n",
    "\n",
    "    interp = aspect_interp_mat(x2,y2)\n",
    "\n",
    "    slope_data = np.array(slope[c:d,a:b])\n",
    "\n",
    "    slope_interp_mat = RectBivariateSpline(y,x,slope_data, s = 0)\n",
    "\n",
    "    interp = slope_interp_mat(x2,y2)\n",
    "    \n",
    "    # Finding the aspect and slope at the given locations of each event\n",
    "    aspects = []\n",
    "    for i in range(0, len(Event_Data_r)):\n",
    "        lon_x = Event_Data_r['location_longitude'][i]\n",
    "        lat_y = Event_Data_r['location_latitude'][i]\n",
    "        x, y = transform(p1,p2,lon_x, lat_y)\n",
    "\n",
    "        asp = interp[int((x-left_x)/100),int((y-bottom_y)/100)]  \n",
    "        aspects.append(asp)\n",
    "\n",
    "    slopes = []\n",
    "    for i in range(0, len(Event_Data_r)):\n",
    "        lon_x = Event_Data_r['location_longitude'][i]\n",
    "        lat_y = Event_Data_r['location_latitude'][i]\n",
    "        x, y = transform(p1,p2,lon_x, lat_y)\n",
    "    \n",
    "        slo = interp[int((x-left_x)/100),int((y-bottom_y)/100)]  \n",
    "        slopes.append(slo)\n",
    "        \n",
    "    azimuths = Event_Data_r['direction_snr(degrees)']\n",
    "    linear_x = np.arange(0,360,1)\n",
    "    linear_y = np.arange(0,360,1)\n",
    "\n",
    "    velocities = Event_Data_r['velocity(m/s)']\n",
    "    linear_x = np.arange(0,600,30)\n",
    "    linear_y = np.arange(0,20,1)\n",
    "\n",
    "    # plotting the results\n",
    "    fig,ax = plt.subplots(1,1,figsize=(6,6),dpi = 200)\n",
    "    for i in range(len(azimuths)):\n",
    "        if 0<azimuths[i]<360:\n",
    "            ax.scatter(aspects[i], azimuths[i], c = 'k')\n",
    "    ax.plot(linear_x, linear_y)\n",
    "    ax.set_title(str(name)+'aspect vs azimuth')\n",
    "    ax.set_xlabel('aspect')\n",
    "    ax.set_ylabel('calculated azimuth')\n",
    "\n",
    "    fig,ax = plt.subplots(1,1,figsize=(6,6),dpi = 200)\n",
    "    for i in range(len(velocities)):\n",
    "        ax.scatter(slopes[i], velocities[i], c = 'k')\n",
    "    ax.set_title(str(name)+'slope vs velocity')    \n",
    "    ax.set_xlabel('slope')\n",
    "    ax.set_ylabel('calculated_velocity')\n",
    "    ax.plot(linear_y, linear_x)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3075d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = int((left_x-left)/10)\n",
    "b = a+2500\n",
    "c = (aspect.shape[0] - int((bottom_y-bottom)/10))-2500\n",
    "d = aspect.shape[0] - int((bottom_y-bottom)/10)\n",
    "\n",
    "x = np.arange(a,b,1)\n",
    "y = np.arange(c,d,1)\n",
    "\n",
    "x2 = np.arange(a,b,10) # every 100m\n",
    "y2 = np.arange(c,d,10) # every 100m\n",
    "\n",
    "aspect_data = np.array(aspect[c:d,a:b])\n",
    "\n",
    "aspect_interp_mat = RectBivariateSpline(y,x,aspect_data, s = 0)\n",
    "\n",
    "interp = aspect_interp_mat(x2,y2)\n",
    "\n",
    "slope_data = np.array(slope[c:d,a:b])\n",
    "\n",
    "slope_interp_mat = RectBivariateSpline(y,x,slope_data, s = 0)\n",
    "\n",
    "interp = slope_interp_mat(x2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c979760",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = []\n",
    "for i in range(0, len(Event_Data_r)):\n",
    "    lon_x = Event_Data_r['location_longitude'][i]\n",
    "    lat_y = Event_Data_r['location_latitude'][i]\n",
    "    x, y = transform(p1,p2,lon_x, lat_y)\n",
    "    \n",
    "    asp = interp[int((x-left_x)/100),int((y-bottom_y)/100)]  \n",
    "    aspects.append(asp)\n",
    "    \n",
    "slopes = []\n",
    "for i in range(0, len(Event_Data_r)):\n",
    "    lon_x = Event_Data_r['location_longitude'][i]\n",
    "    lat_y = Event_Data_r['location_latitude'][i]\n",
    "    x, y = transform(p1,p2,lon_x, lat_y)\n",
    "    \n",
    "    slo = interp[int((x-left_x)/100),int((y-bottom_y)/100)]  \n",
    "    slopes.append(slo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "azimuths = Event_Data_r['direction_snr(degrees)']\n",
    "linear_x = np.arange(0,360,1)\n",
    "linear_y = np.arange(0,360,1)\n",
    "\n",
    "velocities = Event_Data_r['velocity(m/s)']\n",
    "linear_x = np.arange(0,600,30)\n",
    "linear_y = np.arange(0,20,1)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(6,6),dpi = 200)\n",
    "for i in range(len(azimuths)):\n",
    "    if 0<azimuths[i]<360:\n",
    "        ax.scatter(aspects[i], azimuths[i], c = 'k')\n",
    "ax.plot(linear_x, linear_y)\n",
    "ax.set_xlabel('aspect')\n",
    "ax.set_ylabel('calculated azimuth')\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(6,6),dpi = 200)\n",
    "for i in range(len(velocities)):\n",
    "    ax.scatter(slopes[i], velocities[i], c = 'k')\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('calculated_velocity')\n",
    "ax.plot(linear_y, linear_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
