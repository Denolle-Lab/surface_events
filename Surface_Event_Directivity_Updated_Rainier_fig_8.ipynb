{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f6cb81",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Surface Event Analysis\n",
    "###### This notebook analyzes surface event waveforms on Mt Rainier and calculates location, directivity, and velocity, it is updated from the previous as the gridsearch algorithm is weighted by slope,and it analyzes labeled events\n",
    "###### Francesca Skene\n",
    "###### fskene@uw.edu\n",
    "###### Created: 5/23/23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021dc6f",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf759fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/wsd01/pnwstore/')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.geodetics import *\n",
    "from obspy.signal.cross_correlation import *\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "import requests\n",
    "import glob\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "import richdem as rd\n",
    "from pathlib import Path\n",
    "from pyproj import Proj,transform,Geod\n",
    "import os \n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "import json\n",
    "import matplotlib\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30de3d",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clients to download the station data\n",
    "client = WaveformClient()\n",
    "client2 = Client('IRIS')\n",
    "\n",
    "t_before = 120 #number of seconds before pick time\n",
    "t_after = 120 #number of seconds after pick time\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 30 #window length of the signal\n",
    "pr = 98 #percentile\n",
    "thr = 12 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "v_s = 1000 #shear wave velocity at the surface\n",
    "\n",
    "# range of dates that we are looking at\n",
    "t_beginning = UTCDateTime(2001,1,1,0,0,0) \n",
    "t_end = UTCDateTime(2021,12,31,23,59)\n",
    "\n",
    "smooth_length = 5 # constant for smoothing the waveform envelopes\n",
    "low_cut = 2 #low frequency threshold\n",
    "high_cut = 12 #high frequency threshold\n",
    "az_thr = 1000 #threshold of distance in meters from source location\n",
    "step = 100 #step every 100 m\n",
    "t_step = 1 #step every second\n",
    "ratio = 5.6915196 #used to define the grid \n",
    "colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3 # radius of the earth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96853aa",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that calculates picktimes at each station\n",
    "def pick_time(time, ref_env, data_env_dict, st, t_diff, t_before, fs):\n",
    "    # time; picktime from the PNSN\n",
    "    # ref_env; reference envelope\n",
    "    # data_env_dict; dictionary of the envelope data, key is the network+station\n",
    "    # st; stream of traces of the waveforms\n",
    "    # t_diff; 120\n",
    "    # t_before; 120\n",
    "    # fs sample rate\n",
    "    pick_times,offsets, starttimes = [],[],[]\n",
    "    for i,key in enumerate(data_env_dict):\n",
    "        starttimes.append(st[i].stats.starttime)\n",
    "        xcor = correlate(data_env_dict[key],ref_env,int(50*fs))\n",
    "        index = np.argmax(xcor)\n",
    "        cc = round(xcor[index],9) #correlation coefficient\n",
    "        shift = 50*fs-index #how much it is shifted from the reference envelope\n",
    "        offset_time = time - shift/fs # shift from one envelope to the reference envelope in seconds\n",
    "        offsets.append(offset_time) # number of seconds from the beginning of the trace\n",
    "        pick_times.append(offset_time + 120)\n",
    "    return pick_times, offsets, starttimes\n",
    "\n",
    "# calculate the \n",
    "def shift(offsets, starttimes, t_diff):\n",
    "    shifts, vals =[],[]\n",
    "    for i,ii in enumerate(t_diff):\n",
    "        t_shift = offsets[i]-min(offsets)\n",
    "        vals.append((-1*t_diff[ii])+t_shift)\n",
    "        shifts.append(t_shift)\n",
    "    return shifts, vals\n",
    "\n",
    "\n",
    "\n",
    "# resamples the data\n",
    "def resample(st, fs):\n",
    "    for i in st:\n",
    "        i.detrend(type='demean')\n",
    "        i.taper(0.05)\n",
    "        i.resample(fs)   \n",
    "    return st\n",
    "\n",
    "# calculate number of surface events per month\n",
    "def events_per_month(starttimes, events):\n",
    "    num_events = {}\n",
    "    for year in range (2001, 2021):\n",
    "        for month in range (1, 13):\n",
    "            Nevt = []\n",
    "            period = str(year)+\"_\"+str(month)\n",
    "            t0 = UTCDateTime(year, month, 1)\n",
    "            t1 = t0+3600*24*30\n",
    "            for i in range(0, len(starttimes)):\n",
    "                if t0<starttimes[i]<t1:\n",
    "                    Nevt.append(events[i])\n",
    "            if len(Nevt) != 0:\n",
    "                num_events[period]=len(Nevt)\n",
    "            if len(Nevt) == 0:\n",
    "                num_events[period] = 0\n",
    "\n",
    "    periods = list(num_events.keys())\n",
    "    num_of_events = list(num_events.values())\n",
    "    return periods, num_of_events\n",
    "\n",
    "# fit data to a cosine curve\n",
    "def test_func(theta, a,theta0, c):\n",
    "    return a * np.cos(theta-theta0)+c\n",
    "\n",
    "# make plots of weighted data\n",
    "def weight_data(x_data,y_data,weight,test_func,v_s,stas):    \n",
    "    #weighting the data\n",
    "    tempx, tempy = [],[]\n",
    "    for i,ii in enumerate(x_data):\n",
    "        tempx.append([])\n",
    "        tempx[i].append([ii for l in range(0,weight[i])])\n",
    "        tempy.append([])\n",
    "        tempy[i].append([y_data[i] for l in range(0,weight[i])])   \n",
    "    weighted_x = sum(sum(tempx, []),[])\n",
    "    weighted_y = sum(sum(tempy, []),[])\n",
    "   \n",
    "    #optimizing parameters to fit weighted data to test_function\n",
    "    params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(weighted_x), weighted_y, p0=None)\n",
    "    d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "    if params[0]<0:\n",
    "        direction = params[1]+pi \n",
    "    else:\n",
    "        direction = params[1]   \n",
    "    fmax = max(d)\n",
    "    fmin = min(d)\n",
    "    v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "    return v, direction, d\n",
    "\n",
    "# predict synthetic arrival times\n",
    "def travel_time(t0, x, y, vs, sta_x, sta_y):\n",
    "    dist = np.sqrt((sta_x - x)**2 + (sta_y - y)**2)\n",
    "    tt = t0 + dist/vs\n",
    "    return tt\n",
    "\n",
    "# compute residual sum of squares\n",
    "def error(synth_arrivals,arrivals, weight):\n",
    "    res = (arrivals - synth_arrivals)* weight \n",
    "    res_sqr = res**2\n",
    "    mse = np.mean(res_sqr)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# iterate through grid and calculate travel time residuals\n",
    "def gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals, weight):\n",
    "    rss_mat = np.zeros((len(t0),len(x_vect),len(y_vect)))\n",
    "    rss_mat[:,:,:] = np.nan\n",
    "    for i in range(len(t0)):\n",
    "        for j in range(len(x_vect)):\n",
    "            for k in range(len(y_vect)):\n",
    "                synth_arrivals = []\n",
    "                for h in range(len(sta_x)):\n",
    "                    tt = travel_time(t0[i],x_vect[j],y_vect[k],vs,sta_x[h],sta_y[h])\n",
    "                    synth_arrivals.append(tt)\n",
    "                rss = error(np.array(synth_arrivals),np.array(arrivals), np.array(weight))\n",
    "                rss_mat[i,j,k] = rss\n",
    "    return rss_mat\n",
    "\n",
    "# find lower-left corner of grid and grid size based on height of volcano\n",
    "def start_latlon(elevation, ratio, center_lat, center_lon):\n",
    "    side_length = elevation * ratio\n",
    "    l = side_length/2\n",
    "    hypotenuse = l*np.sqrt(2)\n",
    "    d = distance.geodesic(meters = hypotenuse)\n",
    "    start_lat = d.destination(point=[center_lat,center_lon], bearing=225)[0]\n",
    "    start_lon = d.destination(point=[center_lat,center_lon], bearing=225)[1]\n",
    "    return start_lat, start_lon, side_length\n",
    "\n",
    "# convert the location index into latitude and longitude\n",
    "def location(x_dist, y_dist, start_lat, start_lon):\n",
    "    bearing = 90-np.rad2deg(np.arctan(y_dist/x_dist)) \n",
    "    dist = np.sqrt((x_dist)**2 + (y_dist)**2)\n",
    "    d = distance.geodesic(meters = dist)\n",
    "    loc_lat = d.destination(point=[start_lat,start_lon], bearing=bearing)[0]\n",
    "    loc_lon = d.destination(point=[start_lat,start_lon], bearing=bearing)[1]\n",
    "    return loc_lat, loc_lon, d\n",
    "\n",
    "# find diameter in meters of the error on the location\n",
    "def error_diameter(new_array):\n",
    "    min_idx = np.min(new_array[:,1]) # get the left most index \n",
    "    max_idx = np.max(new_array[:,1]) # get the right most index\n",
    "    difference = max_idx-min_idx # take the difference\n",
    "    diameter_m = difference*1000 # convert to meters\n",
    "    return diameter_m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d472ef4",
   "metadata": {},
   "source": [
    "##  Import and organize metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22a659",
   "metadata": {},
   "source": [
    "### 1. Volcano Data (network and station, labeled with volcano name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b099703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('Data/Volcano_Metadata_50km.csv')\n",
    "df_xd = pd.read_csv('Data/XD_Metadata_50km.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center latitude, center longitude, elevation(m), left_trim, right_trim, bottom_trim, top_trim \n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5, 10000, 17000, 13500, 5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the lower left corner and grid size based on volcano elevation\n",
    "# start_lat; lower_left latitude of gridsearch square, \n",
    "# start_lon; lower left longitude of gridsearch square\n",
    "# side_length; side length of grid search square\n",
    "volc_grid = {}\n",
    "for volc in volc_lat_lon:\n",
    "    elevation = volc_lat_lon[volc][2]\n",
    "    center_lat = volc_lat_lon[volc][0]\n",
    "    center_lon = volc_lat_lon[volc][1]\n",
    "    start_lat, start_lon, side_length = start_latlon(elevation, ratio, center_lat, center_lon)\n",
    "    volc_grid[volc] = [start_lat, start_lon, side_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d569229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download DEM data \n",
    "r_dem_data_dict = {}\n",
    "name = 'Mt_Rainier'\n",
    "if volc_lat_lon[name][0]>46:\n",
    "    dem = rio.open('Data/DEM_data/'+str(name)+'/'+str(name)+'1.tif') #washington volcanoes\n",
    "    dem_array = dem.read(1).astype('float64')\n",
    "    dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "    crs = dem.crs\n",
    "    \n",
    "# create a dictionary of the needed data to easily call it later\n",
    "r_dem_data_dict[name]={'data':dem_array, 'crs':crs, 'left':dem.bounds[0], 'right':dem.bounds[2], 'bottom':dem.bounds[1], 'top':dem.bounds[3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ca268",
   "metadata": {},
   "source": [
    "### 3. Surface Event Data from PNSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3894296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download necessary data from PNSN catalog\n",
    "df3 = pd.read_csv('Data/PNSN_Pick_Label.csv')\n",
    "label = df3['Label'].values.tolist()\n",
    "surface_label = df3[df3['Label']== 'su']['Label'].values.tolist()\n",
    "net_temp = df3[df3['Label']== 'su']['Network'].values.tolist()\n",
    "sta_temp = df3[df3['Label']== 'su']['Station'].values.tolist()\n",
    "evt_id_temp = df3[df3['Label']== 'su']['Event_ID'].values.tolist()\n",
    "start_time_temp = df3[df3['Label']== 'su']['Picktime'].values.tolist()                               \n",
    "\n",
    "# saving only the data from between 2001-2021\n",
    "net,sta,evt_id,start_time = [],[],[],[]\n",
    "for i,ii in enumerate(start_time_temp):\n",
    "    if t_beginning<UTCDateTime(ii)<t_end:\n",
    "        net.append(net_temp[i])\n",
    "        sta.append(sta_temp[i])\n",
    "        evt_id.append(evt_id_temp[i])\n",
    "        start_time.append(ii)\n",
    "\n",
    "all_stas = set(sta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c410c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_volcano = 'Mt_Rainier'\n",
    "\n",
    "# getting the map info\n",
    "p2 = Proj(crs,preserve_units=False)\n",
    "p1 = Proj(proj='latlong',preserve_units=False)\n",
    "\n",
    "# hardcode the latitude and longitude ticks to be placed on the maps\n",
    "lat_lon_dict = {}\n",
    "lat_lon_dict['Mt_Rainier']={'tick_lons':[-121.65, -121.7, -121.75, -121.8, -121.85],\n",
    "                            'tick_lats':[46.75,46.8,46.85,46.9,46.95]}\n",
    "\n",
    "# replaces the manual ticks with wanted lat/lon ticks\n",
    "tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "ticks_x = []\n",
    "ticks_y = []\n",
    "for i in range(len(tick_lons)):\n",
    "    tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "    ticks_x.append(tick_x)\n",
    "    ticks_y.append(tick_y)\n",
    "    tick_lons[i]=str(tick_lons[i])\n",
    "    tick_lats[i]=str(tick_lats[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives indices of events in Wes' catalog\n",
    "with open(\"Data/wes_events_r.json\", 'r') as f:\n",
    "    wes_events_r = json.load(f)\n",
    "    \n",
    "# gives event ids of events in Wes' catalog\n",
    "with open(\"Data/event_ids_r.json\", 'r') as f:\n",
    "    event_ids_r = json.load(f)\n",
    "\n",
    "# gives the indices of the cataloged events to run\n",
    "to_run = []\n",
    "for i in event_ids_r:\n",
    "    if i in evt_id:\n",
    "        index = evt_id.index(i)\n",
    "        to_run.append(index)\n",
    "        \n",
    "# all events in wes' catalog to run\n",
    "with open(\"Data/event_labels_r.json\", 'r') as f:\n",
    "    event_labels_r = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract information from the DEM data\n",
    "crs = r_dem_data_dict[associated_volcano]['crs']\n",
    "data = r_dem_data_dict[associated_volcano]['data']\n",
    "volc = rd.rdarray(data, no_data=-9999)\n",
    "slope = rd.TerrainAttribute(volc,attrib = 'slope_riserun')\n",
    "aspect = np.array(rd.TerrainAttribute(volc, attrib = 'aspect'))\n",
    "info = volc_lat_lon[associated_volcano]\n",
    "# gives the lower left grid point in the grid search\n",
    "left_x,bottom_y = transform(p1,p2,volc_grid[associated_volcano][1],volc_grid[associated_volcano][0]) # p1,p2,lon,lat\n",
    "# gives the left right, bottom, top of the grid\n",
    "grid_bounds = [left_x, left_x+volc_grid[associated_volcano][2], bottom_y, bottom_y+volc_grid[associated_volcano][2]]\n",
    "left, right = r_dem_data_dict[associated_volcano]['left'],r_dem_data_dict[associated_volcano]['right']\n",
    "bottom, top = r_dem_data_dict[associated_volcano]['bottom'],r_dem_data_dict[associated_volcano]['top']\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fa267",
   "metadata": {},
   "source": [
    "## Calculating directivity and velocity of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c702477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evt_data = pd.DataFrame(columns = ['event_ID','location_latitude','location_longitude','location_uncertainty(m)',\n",
    "#                                    'origin_time','direction(degrees)', \n",
    "#                                    'direction_sharpness(degrees)','direction_snr(degrees)','duration(sec)',\n",
    "#                                    'params_std_deviation', 'velocity(m/s)','number_of_stations'])\n",
    "\n",
    "# sta_freq=pd.DataFrame(columns = list(all_stas))\n",
    "# reject_evts=pd.DataFrame(columns = ['event_ID'])\n",
    "\n",
    "\n",
    "for n in to_run[0:4]:    \n",
    "    event_ID = str(evt_id[n])\n",
    "#     try:\n",
    "    time = UTCDateTime(start_time[n])\n",
    "    print(time)\n",
    "    if net != 'CN' and evt_id[n]!=evt_id[n-1]:\n",
    "        reference = str(net[n]+'.'+sta[n])\n",
    "#         try:\n",
    "        associated_volcano = df[df['Station']== sta[n]]['Volcano_Name'].values[0]\n",
    "#         except: \n",
    "#             pass\n",
    "        if associated_volcano == 'Mt_Rainier': \n",
    "            #get info for stations within 50km of volcano that event ocurred at\n",
    "            stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "            networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "            latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "            longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "            elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "            if stations.count(\"LON\")>0 and stations.count(\"LO2\")>0:\n",
    "                index = stations.index(\"LO2\")\n",
    "                del stations[index]\n",
    "                del networks[index]\n",
    "                del latitudes[index]\n",
    "                del longitudes[index]\n",
    "                del elevations[index]\n",
    "            \n",
    "            #Get all waveforms for that event based on stations and times\n",
    "            bulk = [] \n",
    "            for m in range(0, len(networks)):\n",
    "                bulk.append([networks[m], stations[m], '*', '*', time-t_before, time+t_after])\n",
    "            st = client2.get_waveforms_bulk(bulk)\n",
    "\n",
    "            #remove unwanted data\n",
    "            for tr in st:\n",
    "                cha = tr.stats.channel\n",
    "                if cha[0:2] != 'BH' and cha[0:2] != 'EH' and cha[0:2] != 'HH':\n",
    "                    st.remove(tr)\n",
    "                try:\n",
    "                    if len(tr.data)/tr.stats.sampling_rate < 239.9:\n",
    "                        st.remove(tr)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            #resampling the data to 40Hz for each trace\n",
    "            st = resample(st,fs) \n",
    "\n",
    "            #Plotting all traces for one event with channel z, SNR>10, and bandpasses between 2-12Hz\n",
    "            SNR,SNR_weight, no_weight,stas,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],[],{},{}\n",
    "            fig = plt.figure(figsize = (11,8), dpi=200)\n",
    "            fig.suptitle('evtID:UW'+ event_ID+associated_volcano)\n",
    "            plt.rcParams.update({'font.size': 20})\n",
    "            ax1 = plt.subplot(1,1,1)\n",
    "            iplot = 0\n",
    "            for i,ii in enumerate(st):\n",
    "                network = ii.stats.network\n",
    "                station = ii.stats.station\n",
    "                ii.detrend(type = 'demean')\n",
    "                ii.filter('bandpass',freqmin=2.0,freqmax=12.0,corners=2,zerophase=True)\n",
    "                cha = ii.stats.channel\n",
    "                starttime = ii.stats.starttime\n",
    "                max_amp_time = np.argmax(ii.data)/fs\n",
    "                signal_window = ii.copy()\n",
    "                noise_window = ii.copy()\n",
    "                signal_window.trim(starttime + t_before - 20, starttime + t_before - 20 + window)\n",
    "                noise_window.trim(starttime + t_before - window -10, starttime + t_before - 10)\n",
    "                snr = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                               / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "\n",
    "                if cha[-1] == 'Z' and snr>thr and 100<max_amp_time<200:\n",
    "                    t = ii.times()\n",
    "                    t_diff[network+'.'+station] = starttime-time \n",
    "                    # enveloping the data \n",
    "                    data_envelope = obspy.signal.filter.envelope(ii.data[115*fs:150*fs])\n",
    "                    data_envelope /= np.max(data_envelope)\n",
    "                    data_envelope += iplot*1.5\n",
    "                    # finding the time of max amplitude of each event\n",
    "                    max_amp_times.append(max_amp_time)\n",
    "                    max_amp = np.max(ii.data)      \n",
    "                    # creating envelope data dictionary to calculate picktimes\n",
    "                    data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "                    data_env_dict[network+'.'+station]= data_envelope\n",
    "                    b,e = 0,239\n",
    "                    ax1.plot(t[b*fs:e*fs],ii.data[b*fs:e*fs]/np.max(np.abs(ii.data))+iplot*1.5)\n",
    "                    ax1.plot(t[115*fs:150*fs], data_envelope, color = 'k')\n",
    "                    ax1.set_xlabel('time (seconds)', fontsize = 10)\n",
    "                    plt.xticks(fontsize = 10)\n",
    "                    ax1.set_xlim([b,e])\n",
    "                    ax1.set_yticks([])\n",
    "                    plt.text(t[e*fs], iplot*1.5, 'SNR:'+str(int(snr)), fontsize = 15)\n",
    "                    plt.text(t[b*fs], (iplot*1.5)+0.2, station, fontsize = 15)\n",
    "                    iplot = iplot+1\n",
    "                    stas.append(ii.stats.station)\n",
    "                    nets.append(ii.stats.network)\n",
    "                    SNR.append(snr)\n",
    "                    SNR_weight.append(int(snr))\n",
    "                    no_weight.append(1)\n",
    "                else:\n",
    "                    st.remove(ii)\n",
    "\n",
    "            if len(st)<4:  \n",
    "                continue\n",
    "\n",
    "            # get peak frequency of each event\n",
    "            # read and preprocess data\n",
    "            st.taper(max_percentage=0.01,max_length=20)\n",
    "            st.trim(starttime=time-20,endtime=time+30)\n",
    "            \n",
    "            lats, lons, elevs, r, theta = ([] for i in range(5)) \n",
    "            ref = str(nets[0]+'.'+stas[0])\n",
    "            try:\n",
    "                ref_env = data_env_dict[reference]\n",
    "            except:\n",
    "                ref_env = data_env_dict[ref]\n",
    "\n",
    "            # calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "            pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs) #calculate picktimes\n",
    "            shifts, vals = shift(pick_times, offsets, starttimes, t_diff)\n",
    "\n",
    "            iplot = 0 \n",
    "            durations = []\n",
    "            for i in range(len(stas)):\n",
    "                max_amp_time = max_amp_times[i]\n",
    "                duration = (max_amp_time-vals[i])*2\n",
    "                durations.append(duration)\n",
    "                ax1.vlines(vals[i], ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "#                 plt.text(t[110*fs], iplot*1.5, 'duration:'+str(int(duration))+'s')\n",
    "                a = stations.index(stas[i])\n",
    "                lats.append(latitudes[a])\n",
    "                lons.append(longitudes[a])\n",
    "                elevs.append(elevations[a])\n",
    "                iplot = iplot+1\n",
    "            avg_duration = np.mean(durations)\n",
    "            \n",
    "            #plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'wiggles'+'.png')\n",
    "\n",
    "#             # make plot of spectra\n",
    "            char_freq, sharp_weight= [],[]\n",
    "            fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "        \n",
    "            matplotlib.rc('xtick', labelsize = 10)\n",
    "            for i in range(len(stas)):\n",
    "                data = st.select(station=stas[i],component=\"Z\")[0].data*100\n",
    "                f,psd=scipy.signal.welch(data,fs=st[0].stats.sampling_rate,nperseg=81,noverlap=1)\n",
    "                #just get the frequencies within the filter band\n",
    "                above_low_cut = [f>low_cut]\n",
    "                below_high_cut = [f<high_cut]\n",
    "                in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "                f = f[in_band]\n",
    "                psd = psd[in_band]\n",
    "\n",
    "                # calculate characteristic frequency and report\n",
    "                char_freq_max = f[np.argmax(psd)]\n",
    "                char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "                psd_cumsum = np.cumsum(psd)\n",
    "                psd_sum = np.sum(psd)\n",
    "                char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "                char_freq.append(char_freq_mean)\n",
    "\n",
    "                plt.rcParams.update({'font.size': 10})\n",
    "                plt.yticks(fontsize = 10)\n",
    "                ax.plot(f,psd,label=stas[i],linewidth=1.5)\n",
    "                ax.set_xscale('log')\n",
    "                ax.set_yscale('log')\n",
    "                ax.tick_params(axis = 'x', which = 'both', labelsize = 10)\n",
    "                ax.grid('True')\n",
    "                ax.set_xlabel('Frequency [Hz]', fontsize = 10)\n",
    "                ax.set_ylabel('PSD [$(mm/s)^2$/Hz]', fontsize = 10)\n",
    "                ax.vlines(char_freq_mean,ymin=np.min(psd)/10,ymax=np.max(psd)*10,linestyle=\"--\",colors=colors[i])\n",
    "\n",
    "#                 weighting the data by the spikiness of the PSD vs frequency graphs\n",
    "                ratio = (np.mean(psd)/np.max(psd))\n",
    "                sharp_weight.append(int(1/(ratio**2)*20))\n",
    "\n",
    "            ax.legend() \n",
    "            #plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'psd'+'.png')\n",
    "\n",
    "#             lats, lons, elevs, r, theta = ([] for i in range(5)) \n",
    "#             ref = str(nets[0]+'.'+stas[0])\n",
    "#             try:\n",
    "#                 ref_env = data_env_dict[reference]\n",
    "#             except:\n",
    "#                 ref_env = data_env_dict[ref]\n",
    "\n",
    "#             # calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "#             pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs) #calculate picktimes\n",
    "#             shifts, vals = shift(pick_times, offsets, starttimes, t_diff)\n",
    "\n",
    "#             iplot = 0 \n",
    "#             durations = []\n",
    "#             for i in range(len(stas)):\n",
    "#                 max_amp_time = max_amp_times[i]\n",
    "#                 duration = (max_amp_time-vals[i])*2\n",
    "#                 durations.append(duration)\n",
    "# #                     ax1.vlines(vals[i], ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "# #                 plt.text(t[110*fs], iplot*1.5, 'duration:'+str(int(duration))+'s')\n",
    "#                 a = stations.index(stas[i])\n",
    "#                 lats.append(latitudes[a])\n",
    "#                 lons.append(longitudes[a])\n",
    "#                 elevs.append(elevations[a])\n",
    "#                 iplot = iplot+1\n",
    "#             avg_duration = np.mean(durations)\n",
    "# #                 plt.savefig('wiggles'+event_ID+associated_volcano+'.png')\n",
    "\n",
    "            # input necessary data for grid search\n",
    "            arrivals = shifts\n",
    "            sta_lats = lats\n",
    "            sta_lons= lons\n",
    "\n",
    "            # define grid origin in lat,lon and grid dimensions in m\n",
    "            lat_start = volc_grid[associated_volcano][0]\n",
    "            lon_start = volc_grid[associated_volcano][1]\n",
    "            side_length = volc_grid[associated_volcano][2]\n",
    "\n",
    "            # create the grid of locations\n",
    "            sta_x = []\n",
    "            sta_y = []\n",
    "            for i in range(len(sta_lats)):\n",
    "                x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "                y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "                sta_x.append(x_dist)\n",
    "                sta_y.append(y_dist)\n",
    "            x_vect = np.arange(0, side_length, step)\n",
    "            y_vect = np.arange(0, side_length, step)\n",
    "            t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "\n",
    "            # gridsearch with no weight\n",
    "            weight = [1 for i in range(len(SNR_weight))]\n",
    "            rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "            loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "            # find the latitude and longitude of the location index \n",
    "            loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "            err_thr = np.min(np.log10(rss_mat))+.05\n",
    "            thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "            diameter = error_diameter(thr_array)\n",
    "\n",
    "            # gridsearch weighted by SNR\n",
    "            weight = np.array(SNR_weight)/np.max(SNR_weight)\n",
    "            rss_mat_snr = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "            loc_idx_snr = np.unravel_index([np.argmin(rss_mat_snr)], rss_mat_snr.shape)\n",
    "            loc_lat_snr, loc_lon_snr, test_d = location(x_vect[loc_idx_snr[1]], y_vect[loc_idx_snr[2]], lat_start, lon_start)\n",
    "\n",
    "            # gridsearch weighted with SNR and Slope\n",
    "            # gives the lower left grid point in the grid search\n",
    "            # gives the left right, bottom, top of the grid\n",
    "            left, right = r_dem_data_dict[associated_volcano]['left'],r_dem_data_dict[associated_volcano]['right']\n",
    "            bottom, top = r_dem_data_dict[associated_volcano]['bottom'],r_dem_data_dict[associated_volcano]['top']\n",
    "\n",
    "            a = int((left_x-left)/10)\n",
    "            b = a+2500\n",
    "            c = (slope.shape[0] - int((bottom_y-bottom)/10))-2500\n",
    "            d = slope.shape[0] - int((bottom_y-bottom)/10)\n",
    "\n",
    "            x = np.arange(a,b,1)\n",
    "            y = np.arange(c,d,1)\n",
    "\n",
    "            x2 = np.arange(a,b,10) # every 100m\n",
    "            y2 = np.arange(c,d,10) # every 100m\n",
    "\n",
    "            slope_data = np.array(slope[c:d,a:b])\n",
    "\n",
    "            slope_data[slope_data < 1] = 1\n",
    "            slope_data[slope_data > 90] = 80\n",
    "\n",
    "            slope_norm = 1/slope_data\n",
    "\n",
    "            slope_interp_mat = RectBivariateSpline(y,x,slope_norm, s = 0)\n",
    "            interp = (slope_interp_mat(x2,y2)/np.max(slope_interp_mat(x2,y2)))*0.1+.9\n",
    "\n",
    "            # gridsearch weighted with slope\n",
    "            rss_mat_slope = np.multiply(rss_mat[loc_idx[0],:,:],(interp))\n",
    "            loc_idx_slope = np.unravel_index([np.argmin(rss_mat_slope)], rss_mat_slope.shape)\n",
    "            loc_lat_slope, loc_lon_slope, test_d = location(x_vect[loc_idx_slope[1]], y_vect[loc_idx_slope[2]], lat_start, lon_start)\n",
    "\n",
    "            # gridsearch weighted with snr and slope\n",
    "            rss_mat_slopesnr = np.multiply(rss_mat_snr[loc_idx[0],:,:],(interp))\n",
    "            loc_idx_slopesnr = np.unravel_index([np.argmin(rss_mat_slopesnr)], rss_mat_slopesnr.shape)\n",
    "            loc_lat_slopesnr, loc_lon_slopesnr, test_d = location(x_vect[loc_idx_slopesnr[1]], y_vect[loc_idx_slopesnr[2]], lat_start, lon_start)\n",
    "            \n",
    "            # plot heatmap\n",
    "#             fig,ax = plt.subplots(1,1,figsize=(8,8), dpi = 200)\n",
    "#             ax.scatter(x_vect[loc_idx[1]],y_vect[loc_idx[2]],s=100,marker='*',c='r')\n",
    "#             ax.scatter(x_vect[loc_idx_slope[1]],y_vect[loc_idx_slope[2]],s=50,marker='*',c='b')\n",
    "#             ax.scatter(x_vect[loc_idx_snr[1]],y_vect[loc_idx_snr[2]],s=25,marker='*',c='w')\n",
    "#             ax.scatter(x_vect[loc_idx_slopesnr[1]],y_vect[loc_idx_slopesnr[2]],s=50,marker='*',c='k')\n",
    "#             im = ax.imshow(np.log10(rss_mat_slope[loc_idx[0],:,:].T),origin=\"lower\",extent=[0,side_length,0,side_length])\n",
    "#             ax.set_ylabel('(m)')\n",
    "#             ax.set_ylabel('(m)')\n",
    "#             cbar = plt.colorbar(im)\n",
    "#             cbar.ax.tick_params()\n",
    "#             cbar.set_label('RMS error on location', rotation=270)\n",
    "            #plt.savefig('heatmap'+ event_ID+associated_volcano+'.png')\n",
    "\n",
    "            # calculating azimuth for each station with respect to the location of the event\n",
    "            for i in range(len(stas)):\n",
    "                u,b,c = (gps2dist_azimuth(loc_lat_slope, loc_lon_slope, lats[i], lons[i], a=6378137.0, f=0.0033528106647474805))\n",
    "                r.append(u)\n",
    "                theta.append(b)\n",
    "\n",
    "            bin1,bin2,bin3 = [],[],[]\n",
    "            for i in theta:\n",
    "                if 0<=i<=120:\n",
    "                    bin1.append(i)\n",
    "                if 121<=i<=240:\n",
    "                    bin2.append(i)\n",
    "                if 241<=i<=360:\n",
    "                    bin3.append(i)\n",
    "\n",
    "            if bin1 == [] or bin2 == [] or bin3 == []:\n",
    "                continue\n",
    "\n",
    "            #manipulating the data\n",
    "            data = {'azimuth_deg':theta, 'freq':char_freq, 'station':stas, 'distance_m':r, \n",
    "                    'weight':sharp_weight, 'SNR':SNR, 'colors':colors[0:len(stas)]}\n",
    "            DF = pd.DataFrame(data, index = None)\n",
    "            DF2 = DF.sort_values('azimuth_deg')\n",
    "\n",
    "            #Taking out stations that are too close to the location when looking at azimuth \n",
    "            drops = []\n",
    "            for i in range(len(DF2)):\n",
    "                value = DF2.loc[i,'distance_m']\n",
    "                if value < az_thr:\n",
    "                    drops.append(i)\n",
    "            DF3 = DF2.drop(drops)\n",
    "            y_data =  DF3[\"freq\"].values.tolist()\n",
    "            Sta2 = DF3[\"station\"].values.tolist()\n",
    "            dist2 = DF3[\"distance_m\"].values.tolist()\n",
    "            spike_weight = DF3[\"weight\"].values.tolist()\n",
    "            SNR2 = DF3['SNR'].values.tolist()\n",
    "            colors2 = DF3['colors'].values.tolist()\n",
    "            x_data =  np.asarray(DF3[\"azimuth_deg\"].values.tolist())\n",
    "            x_points = np.linspace(0,360, 100)\n",
    "\n",
    "            #optimizing parameters to fit data to test_function\n",
    "            params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(x_data), y_data, p0=None)\n",
    "            perr = np.sqrt(np.diag(params_covariance))\n",
    "            std_deviation = str(round(perr[0],9))+','+str(round(perr[1],9))+','+str(round(perr[2],9))\n",
    "            d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "            len_r = int(max(r))\n",
    "\n",
    "            if params[0]<0:\n",
    "                direction = params[1]+pi \n",
    "            else:\n",
    "                direction = params[1]\n",
    "\n",
    "            fmax = max(d)\n",
    "            fmin = min(d)\n",
    "            v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "\n",
    "            #convert the direction from polar to cartesian coordinates\n",
    "            dy = len_r*np.sin(direction)\n",
    "            dx = len_r*np.cos(direction)     \n",
    "\n",
    "            # weight the data\n",
    "            title = 'Sharpness'\n",
    "            v_sharp,direction_sharp,d_sharp = weight_data(x_data,y_data,sharp_weight,test_func,v_s,stas)\n",
    "            dy_sharp = len_r*np.sin(direction_sharp)\n",
    "            dx_sharp = len_r*np.cos(direction_sharp)    \n",
    "\n",
    "            title = 'SNR'\n",
    "            v_snr,direction_snr,d_snr = weight_data(x_data,y_data,SNR_weight,test_func,v_s,stas)  \n",
    "            dy_snr = len_r*np.sin(direction_snr)\n",
    "            dx_snr = len_r*np.cos(direction_snr) \n",
    "\n",
    "            fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "            fig.suptitle('Fitted Cosine Curves')       \n",
    "            ax.set_ylabel('characteristic frequency(Hz)', fontsize = 10)\n",
    "            ax.set_xlabel('azimuth(degrees)', fontsize = 10)\n",
    "            for i in range (0,len(Sta2)):\n",
    "                ax.scatter(x_data[i], y_data[i], s = (SNR_weight[i]**2),label=Sta2[i], color = colors2[i])\n",
    "            ax.plot(x_data,y_data, '--', label='rawdata')\n",
    "            ax.plot(x_points, d, label = 'original')\n",
    "            ax.plot(x_points, d_sharp, label = 'sharpness')\n",
    "            ax.plot(x_points, d_snr, label = 'snr')\n",
    "            ax.legend(loc='upper right', fontsize = 10)\n",
    "            plt.grid(True)\n",
    "            #plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'curves_freq_data'+'.png')\n",
    "\n",
    "            #making plots of directivity and location\n",
    "#             crs = r_dem_data_dict[associated_volcano]['crs']\n",
    "            data = r_dem_data_dict[associated_volcano]['data']\n",
    "\n",
    "            # convert loc data onto the DEM data\n",
    "            contour_x,contour_y = np.meshgrid(left_x+x_vect,bottom_y+y_vect)\n",
    "            center_x, center_y = transform(p1,p2,info[1],info[0])\n",
    "            loc_x,loc_y=transform(p1,p2,loc_lon_slope,loc_lat_slope)\n",
    "            duration=avg_duration\n",
    "            length_factor = duration/100\n",
    "            length_factor = v_snr/(np.max(v_snr)*4)\n",
    "\n",
    "            fig,ax = plt.subplots(1,1,figsize=(8,11), dpi = 200)\n",
    "            \n",
    "            dem = ax.imshow(data,extent=[left, right, bottom, top],cmap='gist_earth', alpha = 0.8)\n",
    "            contours = ax.contour(contour_x,contour_y,np.log10(rss_mat_slope[int(loc_idx[0]),:,:].T),cmap='plasma', linewidths = 0.5)\n",
    "            topo_contours = ax.contour(data, levels = [1000,2000,3000,4000], extent=[left, right, bottom, top],origin=\"upper\", colors = 'k',linewidths = 0.3, alpha = 0.6)\n",
    "            ax.scatter(loc_x, loc_y, s=150,marker='*',c='aqua', zorder = 5)\n",
    "#             plt.arrow(loc_x,loc_y,dy*length_factor,dx*length_factor, color='m', width=170, label='no weight')\n",
    "#             plt.arrow(loc_x,loc_y,dy_sharp*length_factor,dx_sharp*length_factor, color='k', width=170, label='sharpness')\n",
    "            plt.arrow(loc_x,loc_y,dy_snr*length_factor,dx_snr*length_factor, color='w', width=100, zorder = 4)\n",
    "            \n",
    "            #plotting the stations on top of this as triangles\n",
    "            for i, ii in enumerate(stas):\n",
    "                sta_x,sta_y = transform(p1,p2,lons[i],lats[i])\n",
    "                if left+info[3]<sta_x<right-info[4] and bottom+info[5]<sta_y<top-info[6]:\n",
    "                    ax.plot(sta_x,sta_y, c='k', marker=\"^\")\n",
    "                    ax.text(sta_x,sta_y,ii, c='k', fontsize = 15)\n",
    "\n",
    "#             #getting lat and lon tick marks on the axis\n",
    "#             tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "#             tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "#             ticks_x = []\n",
    "#             ticks_y = []\n",
    "#             for i in range(len(tick_lons)):\n",
    "#                 tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "#                 ticks_x.append(tick_x)\n",
    "#                 ticks_y.append(tick_y)\n",
    "#                 tick_lons[i]=str(tick_lons[i])\n",
    "#                 tick_lats[i]=str(tick_lats[i])\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax1 = divider.append_axes('right', size='2%', pad=0.1)\n",
    "            ax.set_title('Location and Directivity', fontsize = 20)\n",
    "            ax.set_xlabel('longitudes(DD)', fontsize = 10)\n",
    "            ax.set_ylabel('latitudes(DD)', fontsize = 10)\n",
    "            ax.set_xticks(ticks_x)\n",
    "            ax.set_xticklabels(tick_lons, fontsize = 10)\n",
    "            ax.set_yticks(ticks_y)\n",
    "            ax.set_yticklabels(tick_lats, fontsize = 10)\n",
    "            ax.clabel(contours, contours.levels, fontsize = 15, inline = True, inline_spacing = 0.5)\n",
    "            cbar = plt.colorbar(dem, cax=cax1)\n",
    "            cbar.ax.tick_params(labelsize=10)\n",
    "            cbar.set_label('Elevation(m)\\n', rotation=270, labelpad = 13, fontsize = 10)\n",
    "            ax.set_xlim(left+info[3],right-info[4])\n",
    "            ax.set_ylim(bottom+info[5],top-info[6])\n",
    "            plt.tight_layout()\n",
    "            #plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'loc_direction.png',bbox_inches=\"tight\")\n",
    "            \n",
    "            # make a dataframe of the data\n",
    "#             evt_data = evt_data.append({'event_ID':event_ID, \n",
    "#                         'location_latitude': loc_lat_slope,\n",
    "#                         'location_longitude': loc_lon_slope,\n",
    "#                         'location_uncertainty(m)':diameter/10,\n",
    "#                         'origin_time': min(offsets)-int(loc_idx[0]),\n",
    "#                         'direction(degrees)':np.rad2deg(direction),\n",
    "#                         'direction_sharpness(degrees)':np.rad2deg(direction_sharp),\n",
    "#                         'direction_snr(degrees)':np.rad2deg(direction_snr),\n",
    "#                         'duration(sec)':avg_duration,\n",
    "#                         'params_std_deviation':std_deviation, \n",
    "#                         'velocity(m/s)':v, \n",
    "#                         'number_of_stations':len(stas)}, ignore_index = True)\n",
    "\n",
    "#             dict_temp = {}\n",
    "#             for i in range(len(stas)):\n",
    "#                 dict_temp[stas[i]] = char_freq[i]    \n",
    "#             sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n",
    "\n",
    "#             evt_data.to_csv('~/surface_events/Analysis_Data/Event_Data_Rainier.csv', index=False)\n",
    "#             sta_freq.to_csv('~/surface_events/Analysis_Data/Station_frequency_data_Rainier.csv', index=False)\n",
    "#     except:\n",
    "#         reject_evts = reject_evts.append({'event_ID':[event_ID]}, ignore_index = True)\n",
    "#         reject_evts.to_csv('~/surface_events/Rejects5.csv', index=False)\n",
    "#         continue\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed041a",
   "metadata": {},
   "source": [
    "## Run Analysis on Labeled Events\n",
    "\n",
    "#### Labels are from Wes at CVO\n",
    "\n",
    "#### Makes final figures of maps and waveforms with label and saves into a zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,n in enumerate(to_run):\n",
    "    event_ID = str(evt_id[n])\n",
    "    label = event_labels_r[i]\n",
    "    time = UTCDateTime(start_time[n]) #picktime from PNSN\n",
    "    thr = 15\n",
    "    print(time)\n",
    "    if net != 'CN' and evt_id[n]!=evt_id[n-1]:\n",
    "        reference = str(net[n]+'.'+sta[n])\n",
    "        try:\n",
    "            associated_volcano = df[df['Station']== sta[n]]['Volcano_Name'].values[0]\n",
    "        except: \n",
    "            pass\n",
    "        if associated_volcano == 'Mt_Rainier':\n",
    "\n",
    "        #get info for stations within 50km of volcano that event ocurred at\n",
    "            stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "            networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "            latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "            longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "            elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "            #Get all waveforms for that event based on stations and times\n",
    "            bulk = [] \n",
    "            for m in range(0, len(networks)):\n",
    "                bulk.append([networks[m], stations[m], '*', '*', time-t_before, time+t_after])\n",
    "            st = client.get_waveforms_bulk(bulk)\n",
    "\n",
    "            #remove unwanted data\n",
    "            for tr in st:\n",
    "                cha = tr.stats.channel\n",
    "                if cha[0:2] != 'BH' and cha[0:2] != 'EH' and cha[0:2] != 'HH':\n",
    "                    st.remove(tr)\n",
    "                try:\n",
    "                    if len(tr.data)/tr.stats.sampling_rate < 239.9:\n",
    "                        st.remove(tr)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            #resampling the data to 40Hz for each trace\n",
    "            st = resample(st,fs) \n",
    "\n",
    "            #Plotting all traces for one event with channel z, SNR>10, and bandpasses between 2-12Hz\n",
    "            SNR,SNR_weight, no_weight,stas,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],[],{},{}\n",
    "\n",
    "            fig, (ax1, ax) = plt.subplots(1, 2, figsize = (14,6), dpi = 200)\n",
    "            \n",
    "            if type(label) == str:\n",
    "                fig.suptitle('evtID:UW'+ event_ID +' '+ associated_volcano+' \\n  '+label, fontsize = 15)\n",
    "\n",
    "            else:\n",
    "                fig.suptitle('evtID:UW'+ event_ID +' '+ associated_volcano+' \\n  '+str(label), fontsize = 15)\n",
    "                \n",
    "            iplot = 0\n",
    "            for i,ii in enumerate(st):\n",
    "                network = ii.stats.network\n",
    "                station = ii.stats.station\n",
    "                ii.detrend(type = 'demean')\n",
    "                ii.filter('bandpass',freqmin=2.0,freqmax=12.0,corners=2,zerophase=True)\n",
    "                ii2 = ii.copy()\n",
    "                ii2.filter('bandpass',freqmin=0.5,freqmax=2.0,corners=2,zerophase=True)\n",
    "                cha = ii.stats.channel\n",
    "                starttime = ii.stats.starttime\n",
    "                max_amp_time = np.argmax(ii.data)/fs\n",
    "                signal_window = ii.copy()\n",
    "                noise_window = ii.copy()\n",
    "                signal_window.trim(starttime + t_before - 20 +20 , starttime + t_before - 20 + window + 40)\n",
    "                noise_window.trim(starttime + t_before - window -15, starttime + t_before - 15)\n",
    "                \n",
    "                snr = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                               / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "\n",
    "                if cha[-1] == 'Z' and snr>thr and 100<max_amp_time<200 and station != 'CINE' and station != 'PUPY' and station != 'TDL':\n",
    "                    t = ii.times()\n",
    "                    t_diff[network+'.'+station] = starttime-time \n",
    "                    # enveloping the data \n",
    "                    b,e = 0,239\n",
    "                    data_envelope = obspy.signal.filter.envelope(ii.data[70*fs:190*fs])\n",
    "                    data_envelope /= np.max(data_envelope)\n",
    "\n",
    "                    # finding the time of max amplitude of each event\n",
    "                    max_amp_times.append(max_amp_time)\n",
    "                    max_amp = np.max(ii.data)      \n",
    "                    # creating envelope data dictionary to calculate picktimes\n",
    "                    data_envelope = obspy.signal.util.smooth(data_envelope,20)\n",
    "                    data_env_dict[network+'.'+station]= data_envelope\n",
    "                    ax1.plot(t[b*fs:e*fs],ii.data[b*fs:e*fs]/np.max(np.abs(ii.data))+iplot*1.5)\n",
    "                    ax1.plot(t[70*fs:190*fs], data_envelope+(iplot*1.5), color = 'k')\n",
    "                    ax1.set_xlabel('time (seconds)', fontsize = 10)\n",
    "                    plt.xticks(fontsize = 10)\n",
    "                    ax1.set_xlim([b,e])\n",
    "                    ax1.set_yticks([])\n",
    "                    ax1.text(t[e*fs], iplot*1.5, 'SNR:'+str(int(snr)), fontsize = 15)\n",
    "                    ax1.text(t[b*fs], (iplot*1.5)+0.2, station, fontsize = 15)\n",
    "                    ax1.grid(True)\n",
    "                    iplot = iplot+1\n",
    "                    stas.append(ii.stats.station)\n",
    "                    nets.append(ii.stats.network)\n",
    "                    SNR.append(snr)\n",
    "                    SNR_weight.append(int(snr))\n",
    "                    no_weight.append(1)\n",
    "                else:\n",
    "                    st.remove(ii)\n",
    "\n",
    "            # delete repeating stations\n",
    "            index = 0\n",
    "            for i in range(len(stas)):\n",
    "                if stas[i] == stas[i-1]:\n",
    "                    net_sta = nets[i]+'.'+stas[i]\n",
    "                    index = stas.index(stas[i])\n",
    "            if index != 0:        \n",
    "                del stas[index]\n",
    "                del nets[index]\n",
    "                del SNR[index]\n",
    "                del SNR_weight[index]\n",
    "                del no_weight[index]\n",
    "\n",
    "            #ax1.text(t[b*fs],(iplot*1.5)+1, 'SNR Threshold:'+str(np.max(SNR)*0.75))\n",
    "            \n",
    "            if len(st)<4:  \n",
    "                continue\n",
    "\n",
    "            # read and preprocess data\n",
    "            st.taper(max_percentage=0.01,max_length=20)\n",
    "            st.trim(starttime=time-20,endtime=time+30) \n",
    "\n",
    "            lats, lons, elevs, r, theta = ([] for i in range(5))\n",
    "\n",
    "            ref_index = SNR.index(np.max(SNR))\n",
    "            ref = str(nets[ref_index]+'.'+stas[ref_index])\n",
    "            try:\n",
    "                ref_env = data_env_dict[reference]\n",
    "            except:\n",
    "                print(ref)\n",
    "                ref_env = data_env_dict[ref]\n",
    "\n",
    "            # calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "            pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs)#calculate picktimes\n",
    "\n",
    "            shifts, vals = shift(offsets, starttimes, t_diff)\n",
    "\n",
    "            iplot = 0 \n",
    "            durations = []\n",
    "            for i in range(len(stas)):\n",
    "                max_amp_time = max_amp_times[i]\n",
    "                duration = (max_amp_time-vals[i])*2\n",
    "                durations.append(duration)\n",
    "                ax1.vlines(vals[i], ymin = iplot*1.5-1, ymax = iplot*1.5+1, color = colors[i])\n",
    "                a = stations.index(stas[i])\n",
    "                lats.append(latitudes[a])\n",
    "                lons.append(longitudes[a])\n",
    "                elevs.append(elevations[a])\n",
    "                iplot = iplot+1\n",
    "            avg_duration = np.mean(durations)\n",
    "\n",
    "        # Determine the peak frequency and make plot of the spectra\n",
    "        char_freq, sharp_weight= [],[]\n",
    "        for i in range(len(stas)):\n",
    "            data = st.select(station=stas[i],component=\"Z\")[0].data*100\n",
    "            f,psd=scipy.signal.welch(data,fs=st[0].stats.sampling_rate,nperseg=81,noverlap=1)\n",
    "            #just get the frequencies within the filter band\n",
    "            above_low_cut = [f>low_cut]\n",
    "            below_high_cut = [f<high_cut]\n",
    "            in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "            f = f[in_band]\n",
    "            psd = psd[in_band]\n",
    "\n",
    "            # calculate characteristic frequency and report\n",
    "            char_freq_max = f[np.argmax(psd)]\n",
    "            char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "            psd_cumsum = np.cumsum(psd)\n",
    "            psd_sum = np.sum(psd)\n",
    "            char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "            char_freq.append(char_freq_mean)\n",
    "\n",
    "#                 plt.yticks(fontsize = 10)\n",
    "#                 ax.plot(f,psd,label=stas[i],linewidth=1.5)\n",
    "#                 ax.set_xscale('log')\n",
    "#                 ax.set_yscale('log')\n",
    "#                 ax.tick_params(axis = 'x', which = 'both', labelsize = 10)\n",
    "#                 ax.grid('True')\n",
    "#                 plt.xticks(fontsize = 10)\n",
    "#                 ax.set_xlabel('Frequency [Hz]', fontsize = 10)\n",
    "#                 ax.set_ylabel('PSD [$(mm/s)^2$/Hz]', fontsize = 10)\n",
    "#                 ax.vlines(char_freq_mean,ymin=np.min(psd)/10,ymax=np.max(psd)*10,linestyle=\"--\",colors=colors[i])\n",
    "#         ax.legend(fontsize = 10) \n",
    "\n",
    "        # input necessary data for grid search\n",
    "        arrivals = shifts\n",
    "        sta_lats = lats\n",
    "        sta_lons= lons\n",
    "\n",
    "        # define grid origin in lat,lon and grid dimensions in m\n",
    "        lat_start = volc_grid[associated_volcano][0]\n",
    "        lon_start = volc_grid[associated_volcano][1]\n",
    "        side_length = volc_grid[associated_volcano][2]\n",
    "\n",
    "        # create the grid of locations\n",
    "        sta_x = []\n",
    "        sta_y = []\n",
    "        for i in range(len(sta_lats)):\n",
    "            x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "            y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "            sta_x.append(x_dist)\n",
    "            sta_y.append(y_dist)\n",
    "        x_vect = np.arange(0, side_length, step)\n",
    "        y_vect = np.arange(0, side_length, step)\n",
    "        t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "\n",
    "        # carry out the gridsearch weighted by SNR\n",
    "        weight = np.array(SNR_weight)/np.max(SNR_weight)\n",
    "        rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "        loc_idx_snr = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "\n",
    "        # gridsearch with no weight\n",
    "        weight = [1 for i in range(len(SNR_weight))]\n",
    "        rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "        loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "\n",
    "        # gridsearch weighted with SNR and Slope\n",
    "\n",
    "        slen = round(volc_grid['Mt_Rainier'][2]/10)\n",
    "        # get area to index the data over\n",
    "        a = int((left_x-left)/10) \n",
    "        b = a+slen\n",
    "        c = (slope.shape[0] - int((bottom_y-bottom)/10))-slen\n",
    "        d = slope.shape[0] - int((bottom_y-bottom)/10)\n",
    "\n",
    "        x = np.arange(a,b,1)\n",
    "        y = np.arange(c,d,1)\n",
    "\n",
    "        x2 = np.arange(a,b,10) # every 100m\n",
    "        y2 = np.arange(c,d,10) # every 100m\n",
    "        \n",
    "        # index the data \n",
    "        slope_data = np.array(slope[c:d,a:b])\n",
    "        \n",
    "        # take out outliars that skew the data\n",
    "        slope_data[slope_data < 1] = 1\n",
    "        slope_data[slope_data > 90] = 80\n",
    "        \n",
    "        # normalize so that higher slope values give low error values\n",
    "        slope_norm = 1/slope_data\n",
    "        \n",
    "        # interpolate the slope data into 100m grid to match grid search \n",
    "        slope_interp_mat = RectBivariateSpline(y,x,slope_norm, s = 0)\n",
    "        interp = slope_interp_mat(x2,y2)/np.max(slope_interp_mat(x2,y2))*0.1+.9\n",
    "        \n",
    "        # calculate the location index \n",
    "        rss_mat_slope = np.multiply(rss_mat[loc_idx[0],:,:],interp)\n",
    "        loc_idx_slope = np.unravel_index([np.argmin(rss_mat_slope)], rss_mat_slope.shape)\n",
    "        loc_lat_slope, loc_lon_slope, test_d = location(x_vect[loc_idx_slope[1]], y_vect[loc_idx_slope[2]], lat_start, lon_start)\n",
    "\n",
    "        # find the latitude and longitude of the location index\n",
    "        loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "        err_thr = np.min(np.log10(rss_mat))+.05\n",
    "        thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "        diameter = error_diameter(thr_array)\n",
    "\n",
    "        # calculating azimuth for each station with respect to the middle of the volcano\n",
    "        for i in range(len(stas)):\n",
    "            u,b,c = (gps2dist_azimuth(loc_lat_slope, loc_lon_slope, lats[i], lons[i], a=6378137.0, f=0.0033528106647474805))\n",
    "            r.append(u)\n",
    "            theta.append(b)\n",
    "\n",
    "        bin1,bin2,bin3 = [],[],[]\n",
    "        for i in theta:\n",
    "            if 0<=i<=120:\n",
    "                bin1.append(i)\n",
    "            if 121<=i<=240:\n",
    "                bin2.append(i)\n",
    "            if 241<=i<=360:\n",
    "                bin3.append(i)\n",
    "        if bin1 == [] or bin2 == [] or bin3 == []:\n",
    "            continue\n",
    "\n",
    "        #manipulating the data\n",
    "        data = {'azimuth_deg':theta, 'freq':char_freq, 'station':stas, 'distance_m':r, \n",
    "                'SNR':SNR, 'colors':colors[0:len(stas)]}\n",
    "        DF = pd.DataFrame(data, index = None)\n",
    "        DF2 = DF.sort_values('azimuth_deg')\n",
    "\n",
    "        #Taking out stations that are too close to the location when looking at azimuth \n",
    "        drops = []\n",
    "        for i in range(len(DF2)):\n",
    "            value = DF2.loc[i,'distance_m']\n",
    "            if value < az_thr:\n",
    "                drops.append(i)\n",
    "        DF3 = DF2.drop(drops)\n",
    "        y_data =  DF3[\"freq\"].values.tolist()\n",
    "        Sta2 = DF3[\"station\"].values.tolist()\n",
    "        dist2 = DF3[\"distance_m\"].values.tolist()\n",
    "        #spike_weight = DF3[\"weight\"].values.tolist()\n",
    "        SNR2 = DF3['SNR'].values.tolist()\n",
    "        colors2 = DF3['colors'].values.tolist()\n",
    "        x_data =  np.asarray(DF3[\"azimuth_deg\"].values.tolist())\n",
    "        x_points = np.linspace(0, 360, 100)\n",
    "\n",
    "        # optimizing parameters to fit data to test_function\n",
    "        params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(x_data), y_data, p0=None)\n",
    "        perr = np.sqrt(np.diag(params_covariance))\n",
    "        std_deviation = str(round(perr[0],9))+','+str(round(perr[1],9))+','+str(round(perr[2],9))\n",
    "        d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "        len_r = int(max(r))\n",
    "\n",
    "        # ensures no negative values\n",
    "        if params[0]<0:\n",
    "            direction = params[1]+pi \n",
    "        else:\n",
    "            direction = params[1]\n",
    "\n",
    "        # calculating the velocity \n",
    "        fmax = max(d)\n",
    "        fmin = min(d)\n",
    "        v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "\n",
    "        # convert the direction from polar to cartesian coordinates\n",
    "        dy = len_r*np.sin(direction)\n",
    "        dx = len_r*np.cos(direction)         \n",
    "\n",
    "        # weight data by SNR\n",
    "        title = 'SNR'\n",
    "        v_snr,direction_snr,d_snr = weight_data(x_data,y_data,SNR_weight,test_func,v_s,stas)  \n",
    "        dy_snr = len_r*np.sin(direction_snr)\n",
    "        dx_snr = len_r*np.cos(direction_snr) \n",
    "\n",
    "#             fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "#             fig.suptitle('Fitted Cosine Curves', fontsize = 20)       \n",
    "#             ax.set_ylabel('characteristic frequency(Hz)', fontsize = 10)\n",
    "#             ax.set_xlabel('azimuth(degrees)', fontsize = 10)\n",
    "#             for i in range (0,len(Sta2)):\n",
    "#                 ax.scatter(x_data[i], y_data[i], s = (SNR_weight[i]**2),label=Sta2[i], color = colors2[i])\n",
    "#             ax.plot(x_data,y_data, '--', label='rawdata')\n",
    "#             ax.plot(x_points, d, label = 'original')\n",
    "#             ax.plot(x_points, d_sharp, label = 'sharpness')\n",
    "#             ax.plot(x_points, d_snr, label = 'snr')\n",
    "#             ax.legend(loc='upper right', fontsize = 10)\n",
    "#             plt.grid(True)\n",
    "\n",
    "        # convert loc data onto the DEM data\n",
    "        contour_x,contour_y = np.meshgrid(left_x+x_vect,bottom_y+y_vect)\n",
    "        center_x, center_y = transform(p1,p2,info[1],info[0])\n",
    "        loc_x,loc_y=transform(p1,p2,loc_lon_slope,loc_lat_slope)\n",
    "        duration=avg_duration\n",
    "        length_factor = duration/100\n",
    "        length_factor = v_snr/(np.max(v_snr)*5)\n",
    "\n",
    "        data = r_dem_data_dict[associated_volcano]['data']\n",
    "\n",
    "        dem = ax.imshow(data,extent=[left, right, bottom, top],cmap='gist_earth', alpha = 0.8)\n",
    "        contours = ax.contour(contour_x,contour_y,np.log10(rss_mat_slope[int(loc_idx_slope[0]),:,:].T),cmap='plasma', linewidths = 0.7)\n",
    "        topo_countours = ax.contour(data,levels = [2000,4000,6000,8000], extent=[left, right, bottom, top],origin=\"upper\", colors = 'k',linewidths = 0.4, alpha = 0.6)\n",
    "        ax.arrow(loc_x,loc_y,dy_snr*length_factor,dx_snr*length_factor, color='w', width=100, zorder = 4)\n",
    "        ax.scatter(loc_x, loc_y, s=150,marker='*',c='aqua', zorder = 5)\n",
    "\n",
    "        #plotting the stations on top of this as triangles\n",
    "        for i, ii in enumerate(stas):\n",
    "            sta_x,sta_y = transform(p1,p2,lons[i],lats[i])\n",
    "            if left+info[3]<sta_x<right-info[4] and bottom+info[5]<sta_y<top-info[6]:\n",
    "                ax.plot(sta_x,sta_y, c='k', marker=\"^\")\n",
    "                ax.text(sta_x,sta_y,ii, c='k', fontsize = 15)\n",
    "\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax1 = divider.append_axes('right', size='2%', pad=0.1)\n",
    "        ax.set_title('Location and Directivity', fontsize = 20)\n",
    "        ax.set_xlabel('longitudes(DD)', fontsize = 10)\n",
    "        ax.set_ylabel('latitudes(DD)', fontsize = 10)\n",
    "        ax.set_xticks(ticks_x)\n",
    "        ax.set_xticklabels(tick_lons, fontsize = 10)\n",
    "        ax.set_yticks(ticks_y)\n",
    "        ax.set_yticklabels(tick_lats, fontsize = 10)\n",
    "        ax.clabel(contours, contours.levels, fontsize = 15, inline = True, inline_spacing = 0.5)\n",
    "\n",
    "        cbar = plt.colorbar(dem, cax=cax1)\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "        cbar.set_label('Elevation(ft)\\n', rotation=270, labelpad = 13, fontsize = 10)\n",
    "        ax.set_xlim(left+info[3],right-info[4])\n",
    "        ax.set_ylim(bottom+info[5],top-info[6])\n",
    "        plt.tight_layout()\n",
    "\n",
    "        #getting lat and lon tick marks on the axis\n",
    "        tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "        tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "        ticks_x = []\n",
    "        ticks_y = []\n",
    "        for i in range(len(tick_lons)):\n",
    "            tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "            ticks_x.append(tick_x)\n",
    "            ticks_y.append(tick_y)\n",
    "            tick_lons[i]=str(tick_lons[i])\n",
    "            tick_lats[i]=str(tick_lats[i])\n",
    "\n",
    "        plt.savefig('./Analysis_Data/r_events_finalfigs/_'+event_ID +'finalfigs.png',bbox_inches=\"tight\")\n",
    "        \n",
    "# saving the figures into a zip file\n",
    "save_path = './Analysis_Data/r_events_finalfigs'\n",
    "image_list = []\n",
    "for file in os.listdir(save_path):\n",
    "    if file.endswith(\".png\"):\n",
    "        image_list.append(os.path.join(save_path, file))\n",
    "\n",
    "with ZipFile(os.path.join(save_path, 'Rainier_labeled_event_figs.zip'), 'w') as zip:\n",
    "    for file in image_list:\n",
    "        zip.write(file)# making a final figure of the map and the waveforms with the label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
