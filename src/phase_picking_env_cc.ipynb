{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eabb7544",
   "metadata": {},
   "source": [
    "# This notebook tests the use of a class to organize the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dfe7f3",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85bf35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/wsd01/pnwstore/')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.geodetics import *\n",
    "from obspy.signal.cross_correlation import *\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "import requests\n",
    "import glob\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "import richdem as rd\n",
    "from pathlib import Path\n",
    "from pyproj import Proj,transform,Geod\n",
    "import os \n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "import json\n",
    "import matplotlib\n",
    "import class_run_event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18cbae3",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61505ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = WaveformClient()\n",
    "client2 = Client('IRIS')\n",
    "\n",
    "t_before = 120 #number of seconds before pick time\n",
    "t_after = 120 #number of seconds after pick time\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 30 #window length of the signal\n",
    "pr = 98 #percentile\n",
    "thr = 12 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "v_s = 1000 #shear wave velocity at the surface\n",
    "\n",
    "# range of dates that we are looking at\n",
    "t_beginning = UTCDateTime(2001,1,1,0,0,0) \n",
    "t_end = UTCDateTime(2021,12,31,23,59)\n",
    "\n",
    "smooth_length = 5 # constant for smoothing the waveform envelopes\n",
    "low_cut = 2 #low frequency threshold\n",
    "high_cut = 12 #high frequency threshold\n",
    "az_thr = 1000 #threshold of distance in meters from source location\n",
    "step = 100 #step every 100 m\n",
    "t_step = 1 #step every second\n",
    "ratio = 5.6915196 #used to define the grid \n",
    "colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3 # radius of the earth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652613e4",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c7e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that calculates picktimes at each station\n",
    "def pick_time(time, ref_env, data_env_dict, st, t_diff, t_before, fs):\n",
    "    # time; picktime from the PNSN\n",
    "    # ref_env; reference envelope\n",
    "    # data_env_dict; dictionary of the envelope data, key is the network+station\n",
    "    # st; stream of traces of the waveforms\n",
    "    # t_diff; 120\n",
    "    # t_before; 120\n",
    "    # fs sample rate\n",
    "    pick_times,offsets, starttimes = [],[],[]\n",
    "    for i,key in enumerate(data_env_dict):\n",
    "        starttimes.append(st[i].stats.starttime)\n",
    "        xcor = correlate(data_env_dict[key],ref_env,int(50*fs))\n",
    "        index = np.argmax(xcor)\n",
    "        cc = round(xcor[index],9) #correlation coefficient\n",
    "        shift = 50*fs-index #how much it is shifted from the reference envelope\n",
    "        offset_time = time - shift/fs # shift from one envelope to the reference envelope in seconds\n",
    "        offsets.append(offset_time) # number of seconds from the beginning of the trace\n",
    "        pick_times.append(offset_time + 120)\n",
    "    return pick_times, offsets, starttimes\n",
    "\n",
    "# calculate the \n",
    "def shift(offsets, starttimes, t_diff):\n",
    "    shifts, vals =[],[]\n",
    "    for i,ii in enumerate(t_diff):\n",
    "        t_shift = offsets[i]-min(offsets)\n",
    "        vals.append((-1*t_diff[ii])+t_shift)\n",
    "        shifts.append(t_shift)\n",
    "    return shifts, vals\n",
    "\n",
    "\n",
    "\n",
    "# resamples the data\n",
    "def resample(st, fs):\n",
    "    for i in st:\n",
    "        i.detrend(type='demean')\n",
    "        i.taper(0.05)\n",
    "        i.resample(fs)   \n",
    "    return st\n",
    "\n",
    "# calculate number of surface events per month\n",
    "def events_per_month(starttimes, events):\n",
    "    num_events = {}\n",
    "    for year in range (2001, 2021):\n",
    "        for month in range (1, 13):\n",
    "            Nevt = []\n",
    "            period = str(year)+\"_\"+str(month)\n",
    "            t0 = UTCDateTime(year, month, 1)\n",
    "            t1 = t0+3600*24*30\n",
    "            for i in range(0, len(starttimes)):\n",
    "                if t0<starttimes[i]<t1:\n",
    "                    Nevt.append(events[i])\n",
    "            if len(Nevt) != 0:\n",
    "                num_events[period]=len(Nevt)\n",
    "            if len(Nevt) == 0:\n",
    "                num_events[period] = 0\n",
    "\n",
    "    periods = list(num_events.keys())\n",
    "    num_of_events = list(num_events.values())\n",
    "    return periods, num_of_events\n",
    "\n",
    "# fit data to a cosine curve\n",
    "def test_func(theta, a,theta0, c):\n",
    "    return a * np.cos(theta-theta0)+c\n",
    "\n",
    "# make plots of weighted data\n",
    "def weight_data(x_data,y_data,weight,test_func,v_s,stas):    \n",
    "    #weighting the data\n",
    "    tempx, tempy = [],[]\n",
    "    for i,ii in enumerate(x_data):\n",
    "        tempx.append([])\n",
    "        tempx[i].append([ii for l in range(0,weight[i])])\n",
    "        tempy.append([])\n",
    "        tempy[i].append([y_data[i] for l in range(0,weight[i])])   \n",
    "    weighted_x = sum(sum(tempx, []),[])\n",
    "    weighted_y = sum(sum(tempy, []),[])\n",
    "   \n",
    "    #optimizing parameters to fit weighted data to test_function\n",
    "    params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(weighted_x), weighted_y, p0=None)\n",
    "    d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "    if params[0]<0:\n",
    "        direction = params[1]+pi \n",
    "    else:\n",
    "        direction = params[1]   \n",
    "    fmax = max(d)\n",
    "    fmin = min(d)\n",
    "    v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "    return v, direction, d\n",
    "\n",
    "# predict synthetic arrival times\n",
    "def travel_time(t0, x, y, vs, sta_x, sta_y):\n",
    "    dist = np.sqrt((sta_x - x)**2 + (sta_y - y)**2)\n",
    "    tt = t0 + dist/vs\n",
    "    return tt\n",
    "\n",
    "# compute residual sum of squares\n",
    "def error(synth_arrivals,arrivals, weight):\n",
    "    res = (arrivals - synth_arrivals)* weight \n",
    "    res_sqr = res**2\n",
    "    mse = np.mean(res_sqr)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# iterate through grid and calculate travel time residuals\n",
    "def gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals, weight):\n",
    "    rss_mat = np.zeros((len(t0),len(x_vect),len(y_vect)))\n",
    "    rss_mat[:,:,:] = np.nan\n",
    "    for i in range(len(t0)):\n",
    "        for j in range(len(x_vect)):\n",
    "            for k in range(len(y_vect)):\n",
    "                synth_arrivals = []\n",
    "                for h in range(len(sta_x)):\n",
    "                    tt = travel_time(t0[i],x_vect[j],y_vect[k],vs,sta_x[h],sta_y[h])\n",
    "                    synth_arrivals.append(tt)\n",
    "                rss = error(np.array(synth_arrivals),np.array(arrivals), np.array(weight))\n",
    "                rss_mat[i,j,k] = rss\n",
    "    return rss_mat\n",
    "\n",
    "# find lower-left corner of grid and grid size based on height of volcano\n",
    "def start_latlon(elevation, ratio, center_lat, center_lon):\n",
    "    side_length = elevation * ratio\n",
    "    l = side_length/2\n",
    "    hypotenuse = l*np.sqrt(2)\n",
    "    d = distance.geodesic(meters = hypotenuse)\n",
    "    start_lat = d.destination(point=[center_lat,center_lon], bearing=225)[0]\n",
    "    start_lon = d.destination(point=[center_lat,center_lon], bearing=225)[1]\n",
    "    return start_lat, start_lon, side_length\n",
    "\n",
    "# convert the location index into latitude and longitude\n",
    "def location(x_dist, y_dist, start_lat, start_lon):\n",
    "    bearing = 90-np.rad2deg(np.arctan(y_dist/x_dist)) \n",
    "    dist = np.sqrt((x_dist)**2 + (y_dist)**2)\n",
    "    d = distance.geodesic(meters = dist)\n",
    "    loc_lat = d.destination(point=[start_lat,start_lon], bearing=bearing)[0]\n",
    "    loc_lon = d.destination(point=[start_lat,start_lon], bearing=bearing)[1]\n",
    "    return loc_lat, loc_lon, d\n",
    "\n",
    "# find diameter in meters of the error on the location\n",
    "def error_diameter(new_array):\n",
    "    min_idx = np.min(new_array[:,1]) # get the left most index \n",
    "    max_idx = np.max(new_array[:,1]) # get the right most index\n",
    "    difference = max_idx-min_idx # take the difference\n",
    "    diameter_m = difference*1000 # convert to meters\n",
    "    return diameter_m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3eeea1",
   "metadata": {},
   "source": [
    "## Import and organize metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc921fa",
   "metadata": {},
   "source": [
    "### 1. Volcano Data (network and station, labeled with volcano name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6a402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('Data/Volcano_Metadata_50km.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a3c06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center latitude, center longitude, elevation(m), left_trim, right_trim, bottom_trim, top_trim \n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5, 10000, 17000, 13500, 5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9c3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the lower left corner and grid size based on volcano elevation\n",
    "# start_lat; lower_left latitude of gridsearch square, \n",
    "# start_lon; lower left longitude of gridsearch square\n",
    "# side_length; side length of grid search square\n",
    "volc_grid = {}\n",
    "for volc in volc_lat_lon:\n",
    "    elevation = volc_lat_lon[volc][2]\n",
    "    center_lat = volc_lat_lon[volc][0]\n",
    "    center_lon = volc_lat_lon[volc][1]\n",
    "    start_lat, start_lon, side_length = start_latlon(elevation, ratio, center_lat, center_lon)\n",
    "    volc_grid[volc] = [start_lat, start_lon, side_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca90d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download DEM data \n",
    "r_dem_data_dict = {}\n",
    "name = 'Mt_Rainier'\n",
    "if volc_lat_lon[name][0]>46:\n",
    "    dem = rio.open('Data/DEM_data/'+str(name)+'/'+str(name)+'1.tif') #washington volcanoes\n",
    "    dem_array = dem.read(1).astype('float64')\n",
    "    dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "    crs = dem.crs\n",
    "    \n",
    "# create a dictionary of the needed data to easily call it later\n",
    "r_dem_data_dict[name]={'data':dem_array, 'crs':crs, 'left':dem.bounds[0], 'right':dem.bounds[2], 'bottom':dem.bounds[1], 'top':dem.bounds[3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb11b5",
   "metadata": {},
   "source": [
    "### 3. Surface Event Data from PNSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e35b79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download necessary data from PNSN catalog\n",
    "df3 = pd.read_csv('Data/PNSN_Pick_Label.csv')\n",
    "label = df3['Label'].values.tolist()\n",
    "surface_label = df3[df3['Label']== 'su']['Label'].values.tolist()\n",
    "net_temp = df3[df3['Label']== 'su']['Network'].values.tolist()\n",
    "sta_temp = df3[df3['Label']== 'su']['Station'].values.tolist()\n",
    "evt_id_temp = df3[df3['Label']== 'su']['Event_ID'].values.tolist()\n",
    "start_time_temp = df3[df3['Label']== 'su']['Picktime'].values.tolist()                               \n",
    "\n",
    "# saving only the data from between 2001-2021\n",
    "net,sta,evt_id,start_time = [],[],[],[]\n",
    "for i,ii in enumerate(start_time_temp):\n",
    "    if t_beginning<UTCDateTime(ii)<t_end:\n",
    "        net.append(net_temp[i])\n",
    "        sta.append(sta_temp[i])\n",
    "        evt_id.append(evt_id_temp[i])\n",
    "        start_time.append(ii)\n",
    "\n",
    "all_stas = set(sta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21509ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3939044/2862177002.py:18: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n"
     ]
    }
   ],
   "source": [
    "associated_volcano = 'Mt_Rainier'\n",
    "\n",
    "# getting the map info\n",
    "p2 = Proj(crs,preserve_units=False)\n",
    "p1 = Proj(proj='latlong',preserve_units=False)\n",
    "\n",
    "# hardcode the latitude and longitude ticks to be placed on the maps\n",
    "lat_lon_dict = {}\n",
    "lat_lon_dict['Mt_Rainier']={'tick_lons':[-121.65, -121.7, -121.75, -121.8, -121.85],\n",
    "                            'tick_lats':[46.75,46.8,46.85,46.9,46.95]}\n",
    "\n",
    "# replaces the manual ticks with wanted lat/lon ticks\n",
    "tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "ticks_x = []\n",
    "ticks_y = []\n",
    "for i in range(len(tick_lons)):\n",
    "    tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "    ticks_x.append(tick_x)\n",
    "    ticks_y.append(tick_y)\n",
    "    tick_lons[i]=str(tick_lons[i])\n",
    "    tick_lats[i]=str(tick_lats[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a9a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives indices of events in Wes' catalog\n",
    "with open(\"Data/wes_events_r.json\", 'r') as f:\n",
    "    wes_events_r = json.load(f)\n",
    "    \n",
    "# gives event ids of events in Wes' catalog\n",
    "with open(\"Data/event_ids_r.json\", 'r') as f:\n",
    "    event_ids_r = json.load(f)\n",
    "\n",
    "# gives the indices of the cataloged events to run\n",
    "to_run = []\n",
    "for i in event_ids_r:\n",
    "    if i in evt_id:\n",
    "        index = evt_id.index(i)\n",
    "        to_run.append(index)\n",
    "        \n",
    "# all events in wes' catalog to run\n",
    "with open(\"Data/event_labels_r.json\", 'r') as f:\n",
    "    event_labels_r = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97936e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! No geotransform defined. Choosing a standard one! (Top left cell's top let corner at <0,0>; cells are 1x1.)\n",
      "Warning! No geotransform defined. Choosing a standard one! (Top left cell's top let corner at <0,0>; cells are 1x1.)\n",
      "Warning! No geotransform defined. Choosing a standard one! (Top left cell's top let corner at <0,0>; cells are 1x1.)\n",
      "Warning! No geotransform defined. Choosing a standard one! (Top left cell's top let corner at <0,0>; cells are 1x1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A Slope calculation (rise/run)\u001b[39m\n",
      "C Horn, B.K.P., 1981. Hill shading and the reflectance map. Proceedings of the IEEE 69, 14–47. doi:10.1109/PROC.1981.11918\u001b[39m\n",
      "\n",
      "\r",
      "\u001b[2K\r",
      "\u001b[2K[                                                  ] (1% - 0.5s - 1 threads)\r",
      "\u001b[2K[=                                                 ] (2% - 0.5s - 1 threads)\r",
      "\u001b[2K[=                                                 ] (3% - 0.5s - 1 threads)\r",
      "\u001b[2K[==                                                ] (4% - 0.5s - 1 threads)\r",
      "\u001b[2K[==                                                ] (5% - 0.5s - 1 threads)\r",
      "\u001b[2K[===                                               ] (6% - 0.5s - 1 threads)\r",
      "\u001b[2K[===                                               ] (7% - 0.5s - 1 threads)\r",
      "\u001b[2K[====                                              ] (8% - 0.5s - 1 threads)\r",
      "\u001b[2K[====                                              ] (9% - 0.4s - 1 threads)\r",
      "\u001b[2K[=====                                             ] (10% - 0.4s - 1 threads)\r",
      "\u001b[2K[=====                                             ] (11% - 0.4s - 1 threads)\r",
      "\u001b[2K[======                                            ] (12% - 0.4s - 1 threads)\r",
      "\u001b[2K[======                                            ] (13% - 0.4s - 1 threads)\r",
      "\u001b[2K[=======                                           ] (14% - 0.4s - 1 threads)\r",
      "\u001b[2K[=======                                           ] (15% - 0.4s - 1 threads)\r",
      "\u001b[2K[========                                          ] (16% - 0.4s - 1 threads)\r",
      "\u001b[2K[========                                          ] (17% - 0.4s - 1 threads)\r",
      "\u001b[2K[=========                                         ] (18% - 0.4s - 1 threads)\r",
      "\u001b[2K[=========                                         ] (19% - 0.4s - 1 threads)\r",
      "\u001b[2K[==========                                        ] (20% - 0.4s - 1 threads)\r",
      "\u001b[2K[==========                                        ] (21% - 0.4s - 1 threads)\r",
      "\u001b[2K[===========                                       ] (22% - 0.4s - 1 threads)\r",
      "\u001b[2K[===========                                       ] (23% - 0.4s - 1 threads)\r",
      "\u001b[2K[============                                      ] (24% - 0.4s - 1 threads)\r",
      "\u001b[2K[============                                      ] (25% - 0.4s - 1 threads)\r",
      "\u001b[2K[=============                                     ] (26% - 0.4s - 1 threads)\r",
      "\u001b[2K[=============                                     ] (27% - 0.4s - 1 threads)\r",
      "\u001b[2K[==============                                    ] (28% - 0.4s - 1 threads)\r",
      "\u001b[2K[==============                                    ] (29% - 0.3s - 1 threads)\r",
      "\u001b[2K[===============                                   ] (30% - 0.3s - 1 threads)\r",
      "\u001b[2K[===============                                   ] (31% - 0.3s - 1 threads)\r",
      "\u001b[2K[================                                  ] (32% - 0.3s - 1 threads)\r",
      "\u001b[2K[================                                  ] (33% - 0.3s - 1 threads)\r",
      "\u001b[2K[=================                                 ] (34% - 0.3s - 1 threads)\r",
      "\u001b[2K[=================                                 ] (35% - 0.3s - 1 threads)\r",
      "\u001b[2K[==================                                ] (36% - 0.3s - 1 threads)\r",
      "\u001b[2K[==================                                ] (37% - 0.3s - 1 threads)\r",
      "\u001b[2K[===================                               ] (38% - 0.3s - 1 threads)\r",
      "\u001b[2K[===================                               ] (39% - 0.3s - 1 threads)\r",
      "\u001b[2K[====================                              ] (40% - 0.3s - 1 threads)\r",
      "\u001b[2K[====================                              ] (41% - 0.3s - 1 threads)\r",
      "\u001b[2K[=====================                             ] (42% - 0.3s - 1 threads)\r",
      "\u001b[2K[=====================                             ] (43% - 0.3s - 1 threads)\r",
      "\u001b[2K[======================                            ] (44% - 0.3s - 1 threads)\r",
      "\u001b[2K[======================                            ] (45% - 0.3s - 1 threads)\r",
      "\u001b[2K[=======================                           ] (46% - 0.3s - 1 threads)\r",
      "\u001b[2K[=======================                           ] (47% - 0.3s - 1 threads)\r",
      "\u001b[2K[========================                          ] (48% - 0.3s - 1 threads)\r",
      "\u001b[2K[========================                          ] (49% - 0.2s - 1 threads)\r",
      "\u001b[2K[=========================                         ] (50% - 0.2s - 1 threads)\r",
      "\u001b[2K[=========================                         ] (51% - 0.2s - 1 threads)\r",
      "\u001b[2K[==========================                        ] (52% - 0.2s - 1 threads)\r",
      "\u001b[2K[==========================                        ] (53% - 0.2s - 1 threads)\r",
      "\u001b[2K[===========================                       ] (54% - 0.2s - 1 threads)\r",
      "\u001b[2K[===========================                       ] (55% - 0.2s - 1 threads)\r",
      "\u001b[2K[============================                      ] (56% - 0.2s - 1 threads)\r",
      "\u001b[2K[============================                      ] (57% - 0.2s - 1 threads)\r",
      "\u001b[2K[=============================                     ] (58% - 0.2s - 1 threads)\r",
      "\u001b[2K[=============================                     ] (59% - 0.2s - 1 threads)\r",
      "\u001b[2K[==============================                    ] (60% - 0.2s - 1 threads)\r",
      "\u001b[2K[==============================                    ] (61% - 0.2s - 1 threads)\r",
      "\u001b[2K[===============================                   ] (62% - 0.2s - 1 threads)\r",
      "\u001b[2K[===============================                   ] (63% - 0.2s - 1 threads)\r",
      "\u001b[2K[================================                  ] (64% - 0.2s - 1 threads)\r",
      "\u001b[2K[================================                  ] (65% - 0.2s - 1 threads)\r",
      "\u001b[2K[=================================                 ] (66% - 0.2s - 1 threads)\r",
      "\u001b[2K[=================================                 ] (67% - 0.2s - 1 threads)\r",
      "\u001b[2K[==================================                ] (68% - 0.2s - 1 threads)\r",
      "\u001b[2K[==================================                ] (69% - 0.1s - 1 threads)\r",
      "\u001b[2K[===================================               ] (70% - 0.1s - 1 threads)\r",
      "\u001b[2K[===================================               ] (71% - 0.1s - 1 threads)\r",
      "\u001b[2K[====================================              ] (72% - 0.1s - 1 threads)\r",
      "\u001b[2K[====================================              ] (73% - 0.1s - 1 threads)\r",
      "\u001b[2K[=====================================             ] (74% - 0.1s - 1 threads)\r",
      "\u001b[2K[=====================================             ] (75% - 0.1s - 1 threads)\r",
      "\u001b[2K[======================================            ] (76% - 0.1s - 1 threads)\r",
      "\u001b[2K[======================================            ] (77% - 0.1s - 1 threads)\r",
      "\u001b[2K[=======================================           ] (78% - 0.1s - 1 threads)\r",
      "\u001b[2K[=======================================           ] (79% - 0.1s - 1 threads)\r",
      "\u001b[2K[========================================          ] (80% - 0.1s - 1 threads)\r",
      "\u001b[2K[========================================          ] (81% - 0.1s - 1 threads)\r",
      "\u001b[2K[=========================================         ] (82% - 0.1s - 1 threads)\r",
      "\u001b[2K[=========================================         ] (83% - 0.1s - 1 threads)\r",
      "\u001b[2K[==========================================        ] (84% - 0.1s - 1 threads)\r",
      "\u001b[2K[==========================================        ] (85% - 0.1s - 1 threads)\r",
      "\u001b[2K[===========================================       ] (86% - 0.1s - 1 threads)\r",
      "\u001b[2K[===========================================       ] (87% - 0.1s - 1 threads)\r",
      "\u001b[2K[============================================      ] (88% - 0.1s - 1 threads)\r",
      "\u001b[2K[============================================      ] (89% - 0.1s - 1 threads)\r",
      "\u001b[2K[=============================================     ] (90% - 0.0s - 1 threads)\r",
      "\u001b[2K[=============================================     ] (91% - 0.0s - 1 threads)\r",
      "\u001b[2K[==============================================    ] (92% - 0.0s - 1 threads)\r",
      "\u001b[2K[==============================================    ] (93% - 0.0s - 1 threads)\r",
      "\u001b[2K[===============================================   ] (94% - 0.0s - 1 threads)\r",
      "\u001b[2K[===============================================   ] (95% - 0.0s - 1 threads)\r",
      "\u001b[2K[================================================  ] (96% - 0.0s - 1 threads)\r",
      "\u001b[2K[================================================  ] (97% - 0.0s - 1 threads)\r",
      "\u001b[2K[================================================= ] (98% - 0.0s - 1 threads)\r",
      "\u001b[2K[================================================= ] (99% - 0.0s - 1 threads)\r",
      "\u001b[2Kt Wall-time = 0.466257\u001b[39m\n",
      "\n",
      "A Aspect attribute calculation\u001b[39m\n",
      "C Horn, B.K.P., 1981. Hill shading and the reflectance map. Proceedings of the IEEE 69, 14–47. doi:10.1109/PROC.1981.11918\u001b[39m\n",
      "\n",
      "\r",
      "\u001b[2K\r",
      "\u001b[2K[                                                  ] (1% - 0.6s - 1 threads)\r",
      "\u001b[2K[=                                                 ] (2% - 0.7s - 1 threads)\r",
      "\u001b[2K[=                                                 ] (3% - 0.8s - 1 threads)\r",
      "\u001b[2K[==                                                ] (4% - 0.8s - 1 threads)\r",
      "\u001b[2K[==                                                ] (5% - 0.8s - 1 threads)\r",
      "\u001b[2K[===                                               ] (6% - 0.8s - 1 threads)\r",
      "\u001b[2K[===                                               ] (7% - 0.8s - 1 threads)\r",
      "\u001b[2K[====                                              ] (8% - 0.8s - 1 threads)\r",
      "\u001b[2K[====                                              ] (9% - 0.8s - 1 threads)\r",
      "\u001b[2K[=====                                             ] (10% - 0.8s - 1 threads)\r",
      "\u001b[2K[=====                                             ] (11% - 0.8s - 1 threads)\r",
      "\u001b[2K[======                                            ] (12% - 0.8s - 1 threads)\r",
      "\u001b[2K[======                                            ] (13% - 0.8s - 1 threads)\r",
      "\u001b[2K[=======                                           ] (14% - 0.8s - 1 threads)\r",
      "\u001b[2K[=======                                           ] (15% - 0.8s - 1 threads)\r",
      "\u001b[2K[========                                          ] (16% - 0.8s - 1 threads)\r",
      "\u001b[2K[========                                          ] (17% - 0.8s - 1 threads)\r",
      "\u001b[2K[=========                                         ] (18% - 0.8s - 1 threads)\r",
      "\u001b[2K[=========                                         ] (19% - 0.8s - 1 threads)\r",
      "\u001b[2K[==========                                        ] (20% - 0.8s - 1 threads)\r",
      "\u001b[2K[==========                                        ] (21% - 0.8s - 1 threads)\r",
      "\u001b[2K[===========                                       ] (22% - 0.7s - 1 threads)\r",
      "\u001b[2K[===========                                       ] (23% - 0.7s - 1 threads)\r",
      "\u001b[2K[============                                      ] (24% - 0.7s - 1 threads)\r",
      "\u001b[2K[============                                      ] (25% - 0.7s - 1 threads)\r",
      "\u001b[2K[=============                                     ] (26% - 0.7s - 1 threads)\r",
      "\u001b[2K[=============                                     ] (27% - 0.7s - 1 threads)\r",
      "\u001b[2K[==============                                    ] (28% - 0.7s - 1 threads)\r",
      "\u001b[2K[==============                                    ] (29% - 0.7s - 1 threads)\r",
      "\u001b[2K[===============                                   ] (30% - 0.7s - 1 threads)\r",
      "\u001b[2K[===============                                   ] (31% - 0.7s - 1 threads)\r",
      "\u001b[2K[================                                  ] (32% - 0.7s - 1 threads)\r",
      "\u001b[2K[================                                  ] (33% - 0.6s - 1 threads)\r",
      "\u001b[2K[=================                                 ] (34% - 0.6s - 1 threads)\r",
      "\u001b[2K[=================                                 ] (35% - 0.6s - 1 threads)\r",
      "\u001b[2K[==================                                ] (36% - 0.6s - 1 threads)\r",
      "\u001b[2K[==================                                ] (37% - 0.6s - 1 threads)\r",
      "\u001b[2K[===================                               ] (38% - 0.6s - 1 threads)\r",
      "\u001b[2K[===================                               ] (39% - 0.6s - 1 threads)\r",
      "\u001b[2K[====================                              ] (40% - 0.6s - 1 threads)\r",
      "\u001b[2K[====================                              ] (41% - 0.6s - 1 threads)\r",
      "\u001b[2K[=====================                             ] (42% - 0.6s - 1 threads)\r",
      "\u001b[2K[=====================                             ] (43% - 0.6s - 1 threads)\r",
      "\u001b[2K[======================                            ] (44% - 0.5s - 1 threads)\r",
      "\u001b[2K[======================                            ] (45% - 0.5s - 1 threads)\r",
      "\u001b[2K[=======================                           ] (46% - 0.5s - 1 threads)\r",
      "\u001b[2K[=======================                           ] (47% - 0.5s - 1 threads)\r",
      "\u001b[2K[========================                          ] (48% - 0.5s - 1 threads)\r",
      "\u001b[2K[========================                          ] (49% - 0.5s - 1 threads)\r",
      "\u001b[2K[=========================                         ] (50% - 0.5s - 1 threads)\r",
      "\u001b[2K[=========================                         ] (51% - 0.5s - 1 threads)\r",
      "\u001b[2K[==========================                        ] (52% - 0.5s - 1 threads)\r",
      "\u001b[2K[==========================                        ] (53% - 0.5s - 1 threads)\r",
      "\u001b[2K[===========================                       ] (54% - 0.4s - 1 threads)\r",
      "\u001b[2K[===========================                       ] (55% - 0.4s - 1 threads)\r",
      "\u001b[2K[============================                      ] (56% - 0.4s - 1 threads)\r",
      "\u001b[2K[============================                      ] (57% - 0.4s - 1 threads)\r",
      "\u001b[2K[=============================                     ] (58% - 0.4s - 1 threads)\r",
      "\u001b[2K[=============================                     ] (59% - 0.4s - 1 threads)\r",
      "\u001b[2K[==============================                    ] (60% - 0.4s - 1 threads)\r",
      "\u001b[2K[==============================                    ] (61% - 0.4s - 1 threads)\r",
      "\u001b[2K[===============================                   ] (62% - 0.4s - 1 threads)\r",
      "\u001b[2K[===============================                   ] (63% - 0.4s - 1 threads)\r",
      "\u001b[2K[================================                  ] (64% - 0.4s - 1 threads)\r",
      "\u001b[2K[================================                  ] (65% - 0.3s - 1 threads)\r",
      "\u001b[2K[=================================                 ] (66% - 0.3s - 1 threads)\r",
      "\u001b[2K[=================================                 ] (67% - 0.3s - 1 threads)\r",
      "\u001b[2K[==================================                ] (68% - 0.3s - 1 threads)\r",
      "\u001b[2K[==================================                ] (69% - 0.3s - 1 threads)\r",
      "\u001b[2K[===================================               ] (70% - 0.3s - 1 threads)\r",
      "\u001b[2K[===================================               ] (71% - 0.3s - 1 threads)\r",
      "\u001b[2K[====================================              ] (72% - 0.3s - 1 threads)\r",
      "\u001b[2K[====================================              ] (73% - 0.3s - 1 threads)\r",
      "\u001b[2K[=====================================             ] (74% - 0.3s - 1 threads)\r",
      "\u001b[2K[=====================================             ] (75% - 0.2s - 1 threads)\r",
      "\u001b[2K[======================================            ] (76% - 0.2s - 1 threads)\r",
      "\u001b[2K[======================================            ] (77% - 0.2s - 1 threads)\r",
      "\u001b[2K[=======================================           ] (78% - 0.2s - 1 threads)\r",
      "\u001b[2K[=======================================           ] (79% - 0.2s - 1 threads)\r",
      "\u001b[2K[========================================          ] (80% - 0.2s - 1 threads)\r",
      "\u001b[2K[========================================          ] (81% - 0.2s - 1 threads)\r",
      "\u001b[2K[=========================================         ] (82% - 0.2s - 1 threads)\r",
      "\u001b[2K[=========================================         ] (83% - 0.2s - 1 threads)\r",
      "\u001b[2K[==========================================        ] (84% - 0.2s - 1 threads)\r",
      "\u001b[2K[==========================================        ] (85% - 0.1s - 1 threads)\r",
      "\u001b[2K[===========================================       ] (86% - 0.1s - 1 threads)\r",
      "\u001b[2K[===========================================       ] (87% - 0.1s - 1 threads)\r",
      "\u001b[2K[============================================      ] (88% - 0.1s - 1 threads)\r",
      "\u001b[2K[============================================      ] (89% - 0.1s - 1 threads)\r",
      "\u001b[2K[=============================================     ] (90% - 0.1s - 1 threads)\r",
      "\u001b[2K[=============================================     ] (91% - 0.1s - 1 threads)\r",
      "\u001b[2K[==============================================    ] (92% - 0.1s - 1 threads)\r",
      "\u001b[2K[==============================================    ] (93% - 0.1s - 1 threads)\r",
      "\u001b[2K[===============================================   ] (94% - 0.1s - 1 threads)\r",
      "\u001b[2K[===============================================   ] (95% - 0.0s - 1 threads)\r",
      "\u001b[2K[================================================  ] (96% - 0.0s - 1 threads)\r",
      "\u001b[2K[================================================  ] (97% - 0.0s - 1 threads)\r",
      "\u001b[2K[================================================= ] (98% - 0.0s - 1 threads)\r",
      "\u001b[2K[================================================= ] (99% - 0.0s - 1 threads)\r",
      "\u001b[2Kt Wall-time = 0.976253\u001b[39m\n",
      "/tmp/ipykernel_3939044/109582656.py:9: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  left_x,bottom_y = transform(p1,p2,volc_grid[associated_volcano][1],volc_grid[associated_volcano][0]) # p1,p2,lon,lat\n"
     ]
    }
   ],
   "source": [
    "# extract information from the DEM data\n",
    "crs = r_dem_data_dict[associated_volcano]['crs']\n",
    "data = r_dem_data_dict[associated_volcano]['data']\n",
    "volc = rd.rdarray(data, no_data=-9999)\n",
    "slope = rd.TerrainAttribute(volc,attrib = 'slope_riserun')\n",
    "aspect = np.array(rd.TerrainAttribute(volc, attrib = 'aspect'))\n",
    "info = volc_lat_lon[associated_volcano]\n",
    "# gives the lower left grid point in the grid search\n",
    "left_x,bottom_y = transform(p1,p2,volc_grid[associated_volcano][1],volc_grid[associated_volcano][0]) # p1,p2,lon,lat\n",
    "# gives the left right, bottom, top of the grid\n",
    "grid_bounds = [left_x, left_x+volc_grid[associated_volcano][2], bottom_y, bottom_y+volc_grid[associated_volcano][2]]\n",
    "left, right = r_dem_data_dict[associated_volcano]['left'],r_dem_data_dict[associated_volcano]['right']\n",
    "bottom, top = r_dem_data_dict[associated_volcano]['bottom'],r_dem_data_dict[associated_volcano]['top']\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bbd70",
   "metadata": {},
   "source": [
    "## Initialize class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b0baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_volcano = \"Mt_Rainier\"\n",
    "event_id = evt_id[3]\n",
    "time = UTCDateTime(start_time[3])\n",
    "re = class_run_event.run_event(associated_volcano,event_id,time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc69f6f",
   "metadata": {},
   "source": [
    "## Run Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee6047f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.6202213093926, 20.149802079317528, 18.831598646493752, 43.147324803041016, 21.31499229088673, 25.760884948655367]\n"
     ]
    }
   ],
   "source": [
    "reference = str(net[3]+'.'+sta[3])\n",
    "\n",
    "#get info for stations within 50km of volcano that event ocurred at\n",
    "re.download_volc_data(df)\n",
    "\n",
    "#Get all waveforms for that event based on stations and times\n",
    "re.download_waveform_data(t_before, t_after, client2, fs,\n",
    "                          window, thr, smooth_length, pr)\n",
    "\n",
    "\n",
    "lats, lons, elevs, r, theta = ([] for i in range(5)) \n",
    "ref = str(nets[0]+'.'+stas[0])\n",
    "try:\n",
    "    ref_env = data_env_dict[reference]\n",
    "except:\n",
    "    ref_env = data_env_dict[ref]\n",
    "    \n",
    "# calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs) #calculate picktimes\n",
    "#         shifts, vals = shift(pick_times, offsets, starttimes, t_diff)\n",
    "\n",
    "#         iplot = 0 \n",
    "#         durations = []\n",
    "#         for i in range(len(stas)):\n",
    "#             max_amp_time = max_amp_times[i]\n",
    "#             duration = (max_amp_time-vals[i])*2\n",
    "#             durations.append(duration)\n",
    "#             ax1.vlines(vals[i], ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "# #                 plt.text(t[110*fs], iplot*1.5, 'duration:'+str(int(duration))+'s')\n",
    "#             a = stations.index(stas[i])\n",
    "#             lats.append(latitudes[a])\n",
    "#             lons.append(longitudes[a])\n",
    "#             elevs.append(elevations[a])\n",
    "#             iplot = iplot+1\n",
    "#         avg_duration = np.mean(durations)\n",
    "\n",
    "#         #plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'wiggles'+'.png')\n",
    "\n",
    "# #             # make plot of spectra\n",
    "#         char_freq, sharp_weight= [],[]\n",
    "#         fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "\n",
    "#         matplotlib.rc('xtick', labelsize = 10)\n",
    "#         for i in range(len(stas)):\n",
    "#             data = st.select(station=stas[i],component=\"Z\")[0].data*100\n",
    "#             f,psd=scipy.signal.welch(data,fs=st[0].stats.sampling_rate,nperseg=81,noverlap=1)\n",
    "#             #just get the frequencies within the filter band\n",
    "#             above_low_cut = [f>low_cut]\n",
    "#             below_high_cut = [f<high_cut]\n",
    "#             in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "#             f = f[in_band]\n",
    "#             psd = psd[in_band]\n",
    "\n",
    "#             # calculate characteristic frequency and report\n",
    "#             char_freq_max = f[np.argmax(psd)]\n",
    "#             char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "#             psd_cumsum = np.cumsum(psd)\n",
    "#             psd_sum = np.sum(psd)\n",
    "#             char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "#             char_freq.append(char_freq_mean)\n",
    "\n",
    "#             plt.rcParams.update({'font.size': 10})\n",
    "#             plt.yticks(fontsize = 10)\n",
    "#             ax.plot(f,psd,label=stas[i],linewidth=1.5)\n",
    "#             ax.set_xscale('log')\n",
    "#             ax.set_yscale('log')\n",
    "#             ax.tick_params(axis = 'x', which = 'both', labelsize = 10)\n",
    "#             ax.grid('True')\n",
    "#             ax.set_xlabel('Frequency [Hz]', fontsize = 10)\n",
    "#             ax.set_ylabel('PSD [$(mm/s)^2$/Hz]', fontsize = 10)\n",
    "#             ax.vlines(char_freq_mean,ymin=np.min(psd)/10,ymax=np.max(psd)*10,linestyle=\"--\",colors=colors[i])\n",
    "\n",
    "# #                 weighting the data by the spikiness of the PSD vs frequency graphs\n",
    "#             ratio = (np.mean(psd)/np.max(psd))\n",
    "#             sharp_weight.append(int(1/(ratio**2)*20))\n",
    "\n",
    "#         ax.legend() \n",
    "#         #plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'psd'+'.png')\n",
    "\n",
    "# #             lats, lons, elevs, r, theta = ([] for i in range(5)) \n",
    "# #             ref = str(nets[0]+'.'+stas[0])\n",
    "# #             try:\n",
    "# #                 ref_env = data_env_dict[reference]\n",
    "# #             except:\n",
    "# #                 ref_env = data_env_dict[ref]\n",
    "\n",
    "# #             # calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "# #             pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs) #calculate picktimes\n",
    "# #             shifts, vals = shift(pick_times, offsets, starttimes, t_diff)\n",
    "\n",
    "# #             iplot = 0 \n",
    "# #             durations = []\n",
    "# #             for i in range(len(stas)):\n",
    "# #                 max_amp_time = max_amp_times[i]\n",
    "# #                 duration = (max_amp_time-vals[i])*2\n",
    "# #                 durations.append(duration)\n",
    "# # #                     ax1.vlines(vals[i], ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "# # #                 plt.text(t[110*fs], iplot*1.5, 'duration:'+str(int(duration))+'s')\n",
    "# #                 a = stations.index(stas[i])\n",
    "# #                 lats.append(latitudes[a])\n",
    "# #                 lons.append(longitudes[a])\n",
    "# #                 elevs.append(elevations[a])\n",
    "# #                 iplot = iplot+1\n",
    "# #             avg_duration = np.mean(durations)\n",
    "# # #                 plt.savefig('wiggles'+event_ID+associated_volcano+'.png')\n",
    "\n",
    "#         # input necessary data for grid search\n",
    "#         arrivals = shifts\n",
    "#         sta_lats = lats\n",
    "#         sta_lons= lons\n",
    "\n",
    "#         # define grid origin in lat,lon and grid dimensions in m\n",
    "#         lat_start = volc_grid[associated_volcano][0]\n",
    "#         lon_start = volc_grid[associated_volcano][1]\n",
    "#         side_length = volc_grid[associated_volcano][2]\n",
    "\n",
    "#         # create the grid of locations\n",
    "#         sta_x = []\n",
    "#         sta_y = []\n",
    "#         for i in range(len(sta_lats)):\n",
    "#             x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "#             y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "#             sta_x.append(x_dist)\n",
    "#             sta_y.append(y_dist)\n",
    "#         x_vect = np.arange(0, side_length, step)\n",
    "#         y_vect = np.arange(0, side_length, step)\n",
    "#         t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "\n",
    "#         # gridsearch with no weight\n",
    "#         weight = [1 for i in range(len(SNR_weight))]\n",
    "#         rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "#         loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "#         # find the latitude and longitude of the location index \n",
    "#         loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "#         err_thr = np.min(np.log10(rss_mat))+.05\n",
    "#         thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "#         diameter = error_diameter(thr_array)\n",
    "\n",
    "#         # gridsearch weighted by SNR\n",
    "#         weight = np.array(SNR_weight)/np.max(SNR_weight)\n",
    "#         rss_mat_snr = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "#         loc_idx_snr = np.unravel_index([np.argmin(rss_mat_snr)], rss_mat_snr.shape)\n",
    "#         loc_lat_snr, loc_lon_snr, test_d = location(x_vect[loc_idx_snr[1]], y_vect[loc_idx_snr[2]], lat_start, lon_start)\n",
    "\n",
    "#         # gridsearch weighted with SNR and Slope\n",
    "#         # gives the lower left grid point in the grid search\n",
    "#         # gives the left right, bottom, top of the grid\n",
    "#         left, right = r_dem_data_dict[associated_volcano]['left'],r_dem_data_dict[associated_volcano]['right']\n",
    "#         bottom, top = r_dem_data_dict[associated_volcano]['bottom'],r_dem_data_dict[associated_volcano]['top']\n",
    "\n",
    "#         a = int((left_x-left)/10)\n",
    "#         b = a+2500\n",
    "#         c = (slope.shape[0] - int((bottom_y-bottom)/10))-2500\n",
    "#         d = slope.shape[0] - int((bottom_y-bottom)/10)\n",
    "\n",
    "#         x = np.arange(a,b,1)\n",
    "#         y = np.arange(c,d,1)\n",
    "\n",
    "#         x2 = np.arange(a,b,10) # every 100m\n",
    "#         y2 = np.arange(c,d,10) # every 100m\n",
    "\n",
    "#         slope_data = np.array(slope[c:d,a:b])\n",
    "\n",
    "#         slope_data[slope_data < 1] = 1\n",
    "#         slope_data[slope_data > 90] = 80\n",
    "\n",
    "#         slope_norm = 1/slope_data\n",
    "\n",
    "#         slope_interp_mat = RectBivariateSpline(y,x,slope_norm, s = 0)\n",
    "#         interp = (slope_interp_mat(x2,y2)/np.max(slope_interp_mat(x2,y2)))*0.1+.9\n",
    "\n",
    "#         # gridsearch weighted with slope\n",
    "#         rss_mat_slope = np.multiply(rss_mat[loc_idx[0],:,:],(interp))\n",
    "#         loc_idx_slope = np.unravel_index([np.argmin(rss_mat_slope)], rss_mat_slope.shape)\n",
    "#         loc_lat_slope, loc_lon_slope, test_d = location(x_vect[loc_idx_slope[1]], y_vect[loc_idx_slope[2]], lat_start, lon_start)\n",
    "\n",
    "#         # gridsearch weighted with snr and slope\n",
    "#         rss_mat_slopesnr = np.multiply(rss_mat_snr[loc_idx[0],:,:],(interp))\n",
    "#         loc_idx_slopesnr = np.unravel_index([np.argmin(rss_mat_slopesnr)], rss_mat_slopesnr.shape)\n",
    "#         loc_lat_slopesnr, loc_lon_slopesnr, test_d = location(x_vect[loc_idx_slopesnr[1]], y_vect[loc_idx_slopesnr[2]], lat_start, lon_start)\n",
    "\n",
    "#         # plot heatmap\n",
    "# #             fig,ax = plt.subplots(1,1,figsize=(8,8), dpi = 200)\n",
    "# #             ax.scatter(x_vect[loc_idx[1]],y_vect[loc_idx[2]],s=100,marker='*',c='r')\n",
    "# #             ax.scatter(x_vect[loc_idx_slope[1]],y_vect[loc_idx_slope[2]],s=50,marker='*',c='b')\n",
    "# #             ax.scatter(x_vect[loc_idx_snr[1]],y_vect[loc_idx_snr[2]],s=25,marker='*',c='w')\n",
    "# #             ax.scatter(x_vect[loc_idx_slopesnr[1]],y_vect[loc_idx_slopesnr[2]],s=50,marker='*',c='k')\n",
    "# #             im = ax.imshow(np.log10(rss_mat_slope[loc_idx[0],:,:].T),origin=\"lower\",extent=[0,side_length,0,side_length])\n",
    "# #             ax.set_ylabel('(m)')\n",
    "# #             ax.set_ylabel('(m)')\n",
    "# #             cbar = plt.colorbar(im)\n",
    "# #             cbar.ax.tick_params()\n",
    "# #             cbar.set_label('RMS error on location', rotation=270)\n",
    "#         #plt.savefig('heatmap'+ event_ID+associated_volcano+'.png')\n",
    "\n",
    "#         # calculating azimuth for each station with respect to the location of the event\n",
    "#         for i in range(len(stas)):\n",
    "#             u,b,c = (gps2dist_azimuth(loc_lat_slope, loc_lon_slope, lats[i], lons[i], a=6378137.0, f=0.0033528106647474805))\n",
    "#             r.append(u)\n",
    "#             theta.append(b)\n",
    "\n",
    "#         bin1,bin2,bin3 = [],[],[]\n",
    "#         for i in theta:\n",
    "#             if 0<=i<=120:\n",
    "#                 bin1.append(i)\n",
    "#             if 121<=i<=240:\n",
    "#                 bin2.append(i)\n",
    "#             if 241<=i<=360:\n",
    "#                 bin3.append(i)\n",
    "\n",
    "#         if bin1 == [] or bin2 == [] or bin3 == []:\n",
    "#             continue\n",
    "\n",
    "#         #manipulating the data\n",
    "#         data = {'azimuth_deg':theta, 'freq':char_freq, 'station':stas, 'distance_m':r, \n",
    "#                 'weight':sharp_weight, 'SNR':SNR, 'colors':colors[0:len(stas)]}\n",
    "#         DF = pd.DataFrame(data, index = None)\n",
    "#         DF2 = DF.sort_values('azimuth_deg')\n",
    "\n",
    "#         #Taking out stations that are too close to the location when looking at azimuth \n",
    "#         drops = []\n",
    "#         for i in range(len(DF2)):\n",
    "#             value = DF2.loc[i,'distance_m']\n",
    "#             if value < az_thr:\n",
    "#                 drops.append(i)\n",
    "#         DF3 = DF2.drop(drops)\n",
    "#         y_data =  DF3[\"freq\"].values.tolist()\n",
    "#         Sta2 = DF3[\"station\"].values.tolist()\n",
    "#         dist2 = DF3[\"distance_m\"].values.tolist()\n",
    "#         spike_weight = DF3[\"weight\"].values.tolist()\n",
    "#         SNR2 = DF3['SNR'].values.tolist()\n",
    "#         colors2 = DF3['colors'].values.tolist()\n",
    "#         x_data =  np.asarray(DF3[\"azimuth_deg\"].values.tolist())\n",
    "#         x_points = np.linspace(0,360, 100)\n",
    "\n",
    "#         #optimizing parameters to fit data to test_function\n",
    "#         params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(x_data), y_data, p0=None)\n",
    "#         perr = np.sqrt(np.diag(params_covariance))\n",
    "#         std_deviation = str(round(perr[0],9))+','+str(round(perr[1],9))+','+str(round(perr[2],9))\n",
    "#         d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "#         len_r = int(max(r))\n",
    "\n",
    "#         if params[0]<0:\n",
    "#             direction = params[1]+pi \n",
    "#         else:\n",
    "#             direction = params[1]\n",
    "\n",
    "#         fmax = max(d)\n",
    "#         fmin = min(d)\n",
    "#         v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "\n",
    "#         #convert the direction from polar to cartesian coordinates\n",
    "#         dy = len_r*np.sin(direction)\n",
    "#         dx = len_r*np.cos(direction)     \n",
    "\n",
    "#         # weight the data\n",
    "#         title = 'Sharpness'\n",
    "#         v_sharp,direction_sharp,d_sharp = weight_data(x_data,y_data,sharp_weight,test_func,v_s,stas)\n",
    "#         dy_sharp = len_r*np.sin(direction_sharp)\n",
    "#         dx_sharp = len_r*np.cos(direction_sharp)    \n",
    "\n",
    "#         title = 'SNR'\n",
    "#         v_snr,direction_snr,d_snr = weight_data(x_data,y_data,SNR_weight,test_func,v_s,stas)  \n",
    "#         dy_snr = len_r*np.sin(direction_snr)\n",
    "#         dx_snr = len_r*np.cos(direction_snr) \n",
    "\n",
    "#         fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "#         fig.suptitle('Fitted Cosine Curves')       \n",
    "#         ax.set_ylabel('characteristic frequency(Hz)', fontsize = 10)\n",
    "#         ax.set_xlabel('azimuth(degrees)', fontsize = 10)\n",
    "#         for i in range (0,len(Sta2)):\n",
    "#             ax.scatter(x_data[i], y_data[i], s = (SNR_weight[i]**2),label=Sta2[i], color = colors2[i])\n",
    "#         ax.plot(x_data,y_data, '--', label='rawdata')\n",
    "#         ax.plot(x_points, d, label = 'original')\n",
    "#         ax.plot(x_points, d_sharp, label = 'sharpness')\n",
    "#         ax.plot(x_points, d_snr, label = 'snr')\n",
    "#         ax.legend(loc='upper right', fontsize = 10)\n",
    "#         plt.grid(True)\n",
    "#         #plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'curves_freq_data'+'.png')\n",
    "\n",
    "#         #making plots of directivity and location\n",
    "# #             crs = r_dem_data_dict[associated_volcano]['crs']\n",
    "#         data = r_dem_data_dict[associated_volcano]['data']\n",
    "\n",
    "#         # convert loc data onto the DEM data\n",
    "#         contour_x,contour_y = np.meshgrid(left_x+x_vect,bottom_y+y_vect)\n",
    "#         center_x, center_y = transform(p1,p2,info[1],info[0])\n",
    "#         loc_x,loc_y=transform(p1,p2,loc_lon_slope,loc_lat_slope)\n",
    "#         duration=avg_duration\n",
    "#         length_factor = duration/100\n",
    "#         length_factor = v_snr/(np.max(v_snr)*4)\n",
    "\n",
    "#         fig,ax = plt.subplots(1,1,figsize=(8,11), dpi = 200)\n",
    "\n",
    "#         dem = ax.imshow(data,extent=[left, right, bottom, top],cmap='gist_earth', alpha = 0.8)\n",
    "#         contours = ax.contour(contour_x,contour_y,np.log10(rss_mat_slope[int(loc_idx[0]),:,:].T),cmap='plasma', linewidths = 0.5)\n",
    "#         topo_contours = ax.contour(data, levels = [1000,2000,3000,4000], extent=[left, right, bottom, top],origin=\"upper\", colors = 'k',linewidths = 0.3, alpha = 0.6)\n",
    "#         ax.scatter(loc_x, loc_y, s=150,marker='*',c='aqua', zorder = 5)\n",
    "# #             plt.arrow(loc_x,loc_y,dy*length_factor,dx*length_factor, color='m', width=170, label='no weight')\n",
    "# #             plt.arrow(loc_x,loc_y,dy_sharp*length_factor,dx_sharp*length_factor, color='k', width=170, label='sharpness')\n",
    "#         plt.arrow(loc_x,loc_y,dy_snr*length_factor,dx_snr*length_factor, color='w', width=100, zorder = 4)\n",
    "\n",
    "#         #plotting the stations on top of this as triangles\n",
    "#         for i, ii in enumerate(stas):\n",
    "#             sta_x,sta_y = transform(p1,p2,lons[i],lats[i])\n",
    "#             if left+info[3]<sta_x<right-info[4] and bottom+info[5]<sta_y<top-info[6]:\n",
    "#                 ax.plot(sta_x,sta_y, c='k', marker=\"^\")\n",
    "#                 ax.text(sta_x,sta_y,ii, c='k', fontsize = 15)\n",
    "\n",
    "# #             #getting lat and lon tick marks on the axis\n",
    "# #             tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "# #             tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "# #             ticks_x = []\n",
    "# #             ticks_y = []\n",
    "# #             for i in range(len(tick_lons)):\n",
    "# #                 tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "# #                 ticks_x.append(tick_x)\n",
    "# #                 ticks_y.append(tick_y)\n",
    "# #                 tick_lons[i]=str(tick_lons[i])\n",
    "# #                 tick_lats[i]=str(tick_lats[i])\n",
    "#         divider = make_axes_locatable(ax)\n",
    "#         cax1 = divider.append_axes('right', size='2%', pad=0.1)\n",
    "#         ax.set_title('Location and Directivity', fontsize = 20)\n",
    "#         ax.set_xlabel('longitudes(DD)', fontsize = 10)\n",
    "#         ax.set_ylabel('latitudes(DD)', fontsize = 10)\n",
    "#         ax.set_xticks(ticks_x)\n",
    "#         ax.set_xticklabels(tick_lons, fontsize = 10)\n",
    "#         ax.set_yticks(ticks_y)\n",
    "#         ax.set_yticklabels(tick_lats, fontsize = 10)\n",
    "#         ax.clabel(contours, contours.levels, fontsize = 15, inline = True, inline_spacing = 0.5)\n",
    "#         cbar = plt.colorbar(dem, cax=cax1)\n",
    "#         cbar.ax.tick_params(labelsize=10)\n",
    "#         cbar.set_label('Elevation(m)\\n', rotation=270, labelpad = 13, fontsize = 10)\n",
    "#         ax.set_xlim(left+info[3],right-info[4])\n",
    "#         ax.set_ylim(bottom+info[5],top-info[6])\n",
    "#         plt.tight_layout()\n",
    "#         #plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'loc_direction.png',bbox_inches=\"tight\")\n",
    "\n",
    "#         # make a dataframe of the data\n",
    "# #             evt_data = evt_data.append({'event_ID':event_ID, \n",
    "# #                         'location_latitude': loc_lat_slope,\n",
    "# #                         'location_longitude': loc_lon_slope,\n",
    "# #                         'location_uncertainty(m)':diameter/10,\n",
    "# #                         'origin_time': min(offsets)-int(loc_idx[0]),\n",
    "# #                         'direction(degrees)':np.rad2deg(direction),\n",
    "# #                         'direction_sharpness(degrees)':np.rad2deg(direction_sharp),\n",
    "# #                         'direction_snr(degrees)':np.rad2deg(direction_snr),\n",
    "# #                         'duration(sec)':avg_duration,\n",
    "# #                         'params_std_deviation':std_deviation, \n",
    "# #                         'velocity(m/s)':v, \n",
    "# #                         'number_of_stations':len(stas)}, ignore_index = True)\n",
    "\n",
    "# #             dict_temp = {}\n",
    "# #             for i in range(len(stas)):\n",
    "# #                 dict_temp[stas[i]] = char_freq[i]    \n",
    "# #             sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n",
    "\n",
    "# #             evt_data.to_csv('~/surface_events/Analysis_Data/Event_Data_Rainier.csv', index=False)\n",
    "# #             sta_freq.to_csv('~/surface_events/Analysis_Data/Station_frequency_data_Rainier.csv', index=False)\n",
    "# #     except:\n",
    "# #         reject_evts = reject_evts.append({'event_ID':[event_ID]}, ignore_index = True)\n",
    "# #         reject_evts.to_csv('~/surface_events/Rejects5.csv', index=False)\n",
    "# #         continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d7fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
