{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0f6cb81",
   "metadata": {},
   "source": [
    "# Surface Event Pick Time\n",
    "\n",
    "This is a modified version of the surface-event location+directivity analysis that Francesca Skene ( fskene@uw.edu), originally created by her in 7/22/22, who started as an undergraduate student at UW. This is marine denolle's version. It includes:\n",
    "* Waveform download for each event on each volcano given the PNSN pick times of \"su\" events.\n",
    "* Data pre-processing to trim the data within 2-12 Hz and remove outliers.\n",
    "* phase picking using transfer-learned model (Ni et al, 2023)\n",
    "* event location using 1D grid search\n",
    "* directivity measurements (velocity and direction) using Doppler effects.\n",
    "* gathering of the data into a CSV data frame.\n",
    "\n",
    "Updated 04/25/2024\n",
    "Marine Denolle\n",
    "(mdenolle@uw.edu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7021dc6f",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf759fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/data/wsd01/pnwstore/')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from mbf_elep_func import *\n",
    "import torch\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "\n",
    "import seisbench.models as sbm\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "# from ELEP.elep.ensemble_learners import ensemble_regressor_cnn\n",
    "from ELEP.elep import mbf, mbf_utils\n",
    "from ELEP.elep import trigger_func\n",
    "\n",
    "from ELEP.elep.mbf_utils import make_LogFq, make_LinFq, rec_filter_coeff, create_obspy_trace\n",
    "from ELEP.elep.mbf import MB_filter as MBF\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130a4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Calculate the azimuth between the stations and the source location\n",
    "def calculate_azimuth(source_lat,source_lon,station_lat,station_lon):\n",
    "    station_location = math.radians(station_lat), math.radians(station_lon)\n",
    "    source_location = math.radians(source_lat), math.radians(source_lon)\n",
    "    source_location_rad = (math.radians(source_location[0]), math.radians(source_location[1]))\n",
    "    \n",
    "    delta_lon = station_location[1] - source_location_rad[1]\n",
    "    \n",
    "    numerator = math.sin(delta_lon) * math.cos(station_location[0])\n",
    "    denominator = math.cos(source_location_rad[0]) * math.sin(station_location[0]) - math.sin(source_location_rad[0]) * math.cos(station_location[0]) * math.cos(delta_lon)\n",
    "    \n",
    "    azimuth = math.atan2(numerator, denominator)\n",
    "    azimuth = math.degrees(azimuth)\n",
    "     \n",
    "    # Convert azimuth to range [0, 360]\n",
    "    # if azimuth < 0:\n",
    "    #     azimuth += 360\n",
    "    \n",
    "    return azimuth# Assume source_location is a tuple (latitude, longitude)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a80c8a1b",
   "metadata": {},
   "source": [
    "What avalanche are we studying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e25b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ava = 'Avalanche_05282023'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd30de3d",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clients to download the station data\n",
    "# client = WaveformClient() # we ignore PNWdatastore for now\n",
    "client2 = Client('IRIS') # IRIS client\n",
    "\n",
    "t_before = 15 #number of seconds before pick time\n",
    "# t_after = 15 #number of seconds after pick time\n",
    "t_before_raw = 1200 #number of seconds before pick time before removing instrumental response\n",
    "# t_after_raw = 1200 #number of seconds after pick time before removing instrumental response\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 150 #window length of the signal (this will help with phase picking with EqT next). \n",
    "# Use 150 seconds @ 40 Hz gives 6001 points. \n",
    "pr = 98 #percentile\n",
    "thr = 7 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "vs = 2800 #shear wave velocity at the surface\n",
    "\n",
    "# range of dates that we are looking at\n",
    "t_beginning = UTCDateTime(2001,1,1,0,0,0) \n",
    "t_end = UTCDateTime(2024,1,1,23,59)\n",
    "\n",
    "smooth_length = 20 # constant for smoothing the waveform envelopes\n",
    "low_cut = 1 #low frequency threshold\n",
    "high_cut = 15 #high frequency threshold\n",
    "az_thr = 1000 #threshold of distance in meters from source location\n",
    "step = 100 #step every 100 m\n",
    "t_step = 1 #step every second\n",
    "ratio = 5.6915196 #used to define the grid \n",
    "# colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3 # radius of the earth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4db39e",
   "metadata": {},
   "source": [
    "## Volcano - Station Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ede0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('../data/station/Volcano_Metadata_50km.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27713bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rasterio\n",
    "from matplotlib.colors import LightSource\n",
    "from rasterio.warp import transform_bounds,  reproject, Resampling , calculate_default_transform\n",
    "from rasterio.transform import array_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_volcano = 'Mt_Rainier'\n",
    "        \n",
    "#get info for stations within 50km of volcano that event ocurred at\n",
    "stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af3fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center latitude, center longitude, elevation(m), left_trim, right_trim, bottom_trim, top_trim \n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5]\n",
    "#Find the lower left corner and grid size based on volcano elevation\n",
    "# define grid origin in lat,lon and grid dimensions in m\n",
    "lon_start = -121.9 #volc_lat_lon[associated_volcano][0]\n",
    "lon_end = -121.65 #volc_lat_lon[associated_volcano][0]\n",
    "lat_start = 46.6 #volc_lat_lon[associated_volcano][1]\n",
    "lat_end = 47 #volc_lat_lon[associated_volcano][1]\n",
    "\n",
    "\n",
    "# Create a light source\n",
    "ls = LightSource(azdeg=315, altdeg=45)\n",
    "\n",
    "\n",
    "# Load the DEM with rasterio\n",
    "with rasterio.open('../data/geospatial/Mt_Rainier/Mt_Rainier.tif') as src:\n",
    "    dem = src.read(1)  # read the first band\n",
    "    transform = src.transform\n",
    "    bounds = src.bounds\n",
    "    crs=src.crs\n",
    "    # dem = dem.astype('float64')\n",
    "    dem[dem == -32767] = np.nan #gets rid of edge effects\n",
    "    # dem = np.nan_to_num(dem,nan=1000)\n",
    "\n",
    "\n",
    "# Define the target CRS\n",
    "epsg_code = 32600 + 10\n",
    "# dst_crs = 'EPSG:4326'  # EPSG:4326 is the code for WGS84 lat/lon\n",
    "dst_crs = 'epsg:{}'.format(epsg_code) #pyproj.Proj(init=)\n",
    "# Calculate the transform and dimensions for the reprojected DEM\n",
    "transform_latlon, width, height = calculate_default_transform(crs, dst_crs, dem.shape[1], dem.shape[0], *bounds)\n",
    "\n",
    "\n",
    "# Create an empty array for the reprojected DEM\n",
    "dem_latlon = np.empty(shape=(height, width))\n",
    "\n",
    "# Reproject the DEM\n",
    "reproject(\n",
    "    source=dem,\n",
    "    destination=dem_latlon,\n",
    "    src_transform=transform,\n",
    "    src_crs=crs,\n",
    "    dst_transform=transform_latlon,\n",
    "    dst_crs=dst_crs,\n",
    "    resampling=Resampling.nearest)\n",
    "\n",
    "# Transform the bounds to the target CRS\n",
    "left, bottom, right, top = transform_bounds(crs, dst_crs, *bounds)\n",
    "\n",
    "# Calculate the illumination intensity\n",
    "illumination = ls.hillshade(dem_latlon)\n",
    "# new bounds\n",
    "left, bottom, right, top = array_bounds(height, width, transform_latlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center latitude, center longitude, elevation(m), left_trim, right_trim, bottom_trim, top_trim \n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5]\n",
    "#Find the lower left corner and grid size based on volcano elevation\n",
    "# define grid origin in lat,lon and grid dimensions in m\n",
    "lon_start = -122 #volc_lat_lon[associated_volcano][0]\n",
    "lon_end = -121.5 #volc_lat_lon[associated_volcano][0]\n",
    "lat_start = 46.6 #volc_lat_lon[associated_volcano][1]\n",
    "lat_end = 47 #volc_lat_lon[associated_volcano][1]\n",
    "\n",
    "\n",
    "proj = pyproj.Proj(proj='utm', zone=10, ellps='WGS84')\n",
    "if lon_start<0: \n",
    "    lon_start1 = lon_start+360\n",
    "    lon_end1 = lon_end + 360\n",
    "# Convert lat/long to Cartesian in meters\n",
    "x_step=100\n",
    "x1,y1=proj(lon_start,lat_start)\n",
    "x2,y2=proj(lon_end,lat_end)\n",
    "# Generate the x and y coordinates for the grid\n",
    "x_coords = np.arange(x1, x2, x_step)\n",
    "y_coords = np.arange(y1, y2, x_step)\n",
    "\n",
    "\n",
    "cmap1 = plt.get_cmap('hot_r')\n",
    "cmap2 = plt.get_cmap('hsv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b021322",
   "metadata": {},
   "source": [
    "## PNSN SU Pick information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.read_csv(\"../data/events/su_picks.txt\",sep=\"|\")\n",
    "f1.head()\n",
    "print(f1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the spaces in the file\n",
    "format='%Y/%m/%d %H:%M:%S'\n",
    "test=f1[\"date\"].values.tolist()\n",
    "start_time_temp = [  datetime.strptime(x.strip(),'%Y/%m/%d %H:%M:%S') for x in f1[\"date\"].values.tolist()]\n",
    "# # Ignore events prior to t_beginning\n",
    "ik=np.where(np.array(start_time_temp)>datetime(2001,1,1))[0][0]\n",
    "\n",
    "# select only net, sta, evid, startime for event past the start date.\n",
    "\n",
    "start_time = start_time_temp[ik:]\n",
    "net=[ x.strip() for x in f1[\"net\"].values.tolist()][ik:]\n",
    "sta=[ x.strip() for x in f1[\"sta\"].values.tolist()][ik:]\n",
    "evt_id=[ x for x in f1[\"orid\"].values.tolist()][ik:]\n",
    "all_stas=set(sta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20c89405",
   "metadata": {},
   "source": [
    "## ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.makedirs(\"/Users/marinedenolle/.seisbench/models/v3/eqtransformer\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/congcy/ELEP/raw/main/docs/tutorials/data/pnw.pt.v1 -O ~/.seisbench/models/v3/eqtransformer/pnw.pt.v1\n",
    "# !wget https://github.com/congcy/ELEP/raw/main/docs/tutorials/data/pnw.json.v1 -O ~/.seisbench/models/v3/eqtransformer/pnw.json.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download models\n",
    "list_models_name = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "# pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "\n",
    "list_models = [pn_pnw_model, pn_ethz_model, pn_instance_model, pn_scedc_model, pn_stead_model, pn_geofon_model]\n",
    "\n",
    "pn_pnw_model.to(device);\n",
    "pn_ethz_model.to(device);\n",
    "pn_scedc_model.to(device);\n",
    "# pn_neic_model.to(device);\n",
    "pn_geofon_model.to(device);\n",
    "pn_stead_model.to(device);\n",
    "pn_instance_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_semblance = {'dt':0.025, 'semblance_order':4, 'window_flag':True, \n",
    "                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "p_thrd, s_thrd = 0.01, 0.05\n",
    "\n",
    "fqmin = low_cut\n",
    "fqmax = high_cut\n",
    "dt = 0.025; fs = 40\n",
    "nfqs = 10\n",
    "nt = 6000; nc = 3\n",
    "fq_list = make_LogFq(fqmin, fqmax, dt, nfqs)\n",
    "coeff_HP, coeff_LP = rec_filter_coeff(fq_list, dt)\n",
    "MBF_paras = {'f_min':fqmin, 'f_max':fqmax, 'nfqs':nfqs, 'frequencies':fq_list, 'CN_HP':coeff_HP, 'CN_LP':coeff_LP, \\\n",
    "    'dt':dt, 'fs':fs, 'nt':nt, 'nc':nc, 'npoles': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605be24",
   "metadata": {},
   "source": [
    "# Full stack:\n",
    "\n",
    "* download waveforms\n",
    "* phase pick onset\n",
    "* estimate SNR\n",
    "* measure centroid, max envelope, duration\n",
    "* measure Fmax for doppler analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075da112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pdf = PdfPages('../plots/all_rainier.pdf')\n",
    "# associated_volcano = \"Mt_Rainier\"\n",
    "dff=[] \n",
    "for nn in range(len(evt_id)):\n",
    "    n = len(evt_id)-nn\n",
    "    associated_volcano = df[df['Station']== sta[n]]['Volcano_Name'].values[0]\n",
    "    if associated_volcano!=\"Mt_Rainier\":continue\n",
    "    event_ID = str(evt_id[n])\n",
    "    otime = UTCDateTime(start_time[n])\n",
    "    associated_volcano=\"Mt_Rainier\"\n",
    "\n",
    "\n",
    "    #get info for stations within 50km of volcano that event ocurred at\n",
    "    stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "    networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "    latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "    longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "    elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "\n",
    "    #################### WAVEFORM DOWNLOAD #######################\n",
    "    #Download all waveforms for that event based on stations and times\n",
    "    bulk = [] \n",
    "    for m in range(0, len(networks)):\n",
    "        bulk.append([networks[m], stations[m], '*', '*Z', otime-t_before_raw, otime+t_before_raw])\n",
    "    # try:\n",
    "    st = client2.get_waveforms_bulk(bulk)\n",
    "    st = resample(st,fs)  #resampling the data to 40Hz for each trace\n",
    "    evt_data = obspy.Stream()\n",
    "    snr=[]\n",
    "    stas=[]\n",
    "    nets=[]\n",
    "    lats=[]\n",
    "    lons=[]\n",
    "    els=[]\n",
    "    centroid_time = []\n",
    "    data_env_dict = {}\n",
    "    duration = []\n",
    "    max_time = []\n",
    "\n",
    "    # #Keeping all traces for one event with channel z, SNR>10, and bandpassed between 2-12Hz\n",
    "    # ,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],[],{},{}\n",
    "    for i,ii in enumerate(st):\n",
    "        ii.detrend(type = 'demean')\n",
    "        ii.filter('bandpass',freqmin=low_cut,freqmax=high_cut,corners=2,zerophase=True)\n",
    "        # trim the data and noise window to exactly 6000 points\n",
    "        signal_window = ii.copy()\n",
    "        noise_window = ii.copy()\n",
    "        signal_window.trim(otime - t_before, otime - t_before + window) # trim the signal at the first pick time of the PNSN data, with loose 40s before\n",
    "        noise_window.trim(otime - window -t_before, otime - t_before) # noise window of the same length\n",
    "        if  len(signal_window.data)<=10 or  len(noise_window.data)<=10: continue # skip if no data\n",
    "        # if not np.percentile(np.abs(signal_window.data),pr):continue # skip if max amplitude is zero\n",
    "        snr1 = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                        / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "        if snr1<thr: # and 100<max_amp_time<200:\n",
    "            st.remove(ii)\n",
    "            continue\n",
    "\n",
    "    ################# ENVELOPE, CENTROID, DURATION #######################\n",
    "        # enveloping the data \n",
    "        data_envelope = obspy.signal.filter.envelope(signal_window.data)\n",
    "        data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "\n",
    "        data_env_dict[ii.stats.network+'.'+ii.stats.station]= data_envelope/max(np.abs(data_envelope))\n",
    "\n",
    "\n",
    "        # max time\n",
    "        # finding the time of max amplitude of each event\n",
    "        # signal_window is windowed at otime-t_v before the PNSN pick time\n",
    "        crap = np.argmax(np.abs(data_envelope[:(t_before+40)*fs])) # time of max amplitude relative to otime\n",
    "        max_time.append(crap/fs)\n",
    "\n",
    "        # centroid time\n",
    "        tcrap = signal_window.times()\n",
    "        it = np.where(tcrap>0)[0]\n",
    "        centroid_time.append(np.sum(data_envelope*tcrap)/np.sum(data_envelope))\n",
    "\n",
    "        # find duration as data starting with the \"origin time\" and ending when the envelope falls below the mean noise\n",
    "        noise_envelope = obspy.signal.filter.envelope(noise_window.data)\n",
    "        data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "        mean_noise = np.mean(noise_envelope)\n",
    "        \n",
    "        mmax = np.max(np.cumsum(data_envelope**4))\n",
    "        crap = np.where( np.cumsum(data_envelope**4) <= 0.999*mmax)[0][-1]\n",
    "        duration.append(crap/fs)\n",
    "\n",
    "\n",
    "        stas.append(ii.stats.station)\n",
    "        nets.append(ii.stats.network)\n",
    "        ista=stations.index(ii.stats.station)\n",
    "        lats.append(latitudes[ista])\n",
    "        lons.append(longitudes[ista])\n",
    "        els.append(elevations[ista])\n",
    "        snr.append(snr1)\n",
    "        evt_data.append(signal_window)\n",
    "\n",
    "        t = evt_data.select(station=stas[-1])[0].times()\n",
    "        \n",
    "\n",
    "    centroid_time = np.asarray(centroid_time)\n",
    "    centroid_time -= t_before\n",
    "    max_time = np.asarray(max_time)\n",
    "    max_time -= t_before\n",
    "    duration = np.asarray(duration)\n",
    "    duration -= t_before\n",
    "\n",
    "    ################### ELEP #######################\n",
    "\n",
    "        # test the new function\n",
    "    smb_peak= apply_elep(evt_data, stas, \\\n",
    "            list_models, MBF_paras, paras_semblance, t_before)\n",
    "    smb_peak -= t_before\n",
    "\n",
    "\n",
    "    ############## PEAK FREQUENCY MEASUREMENTS ############\n",
    "    # Given the approximate measurement of duration, window the signal windows around that\n",
    "    # then measure peak frequency so that there is less noise in it.\n",
    "    # perform this on the Z component only.\n",
    "\n",
    "    char_freq, sharp_weight= [],[]\n",
    "    fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "    for ii,i in enumerate(evt_data):\n",
    "        data = np.zeros(200*fs)\n",
    "        crap=i.copy()\n",
    "        otime1 = crap.stats.starttime + smb_peak[ii] # pick time\n",
    "        crap.trim(otime1  - 10, otime1 + 2*duration[ii] + 10) # window the data around the pick time\n",
    "        crap.taper(max_percentage=0.01,max_length=20)\n",
    "\n",
    "        data[:len(crap.data)] = crap.data #*100\n",
    "        f,psd=scipy.signal.welch(data,fs=fs,nperseg=81,noverlap=4)\n",
    "        #just get the frequencies within the filter band\n",
    "        above_low_cut = [f>low_cut]\n",
    "        below_high_cut = [f<high_cut]\n",
    "        in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "        f = f[in_band]\n",
    "        psd = psd[in_band]\n",
    "\n",
    "        # calculate characteristic frequency and report\n",
    "        char_freq_max = f[np.argmax(psd)]\n",
    "        char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "        psd_cumsum = np.cumsum(psd)\n",
    "        psd_sum = np.sum(psd)\n",
    "        char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "        char_freq.append(char_freq_mean)\n",
    "\n",
    "        plt.rcParams.update({'font.size': 20})\n",
    "        p=ax.plot(f,psd,label=stas[ii],linewidth=2)\n",
    "        cc = p[0].get_color()\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.grid('True')\n",
    "        ax.set_xlabel('Frequency [Hz]')\n",
    "        ax.set_ylabel('PSD [$(mm/s)^2$/Hz]')\n",
    "        ax.vlines(char_freq_mean,ymin=np.min(psd)/10,ymax=np.max(psd)*10,linestyle=\"--\",colors=cc)\n",
    "        ax.grid(True)\n",
    "\n",
    "    #             # weighting the data by the spikiness of the PSD vs frequency graphs\n",
    "        ratio = (np.mean(psd)/np.max(psd))\n",
    "        sharp_weight.append(int(1/(ratio**2)*20))\n",
    "\n",
    "\n",
    "\n",
    "    ##################### get amplitude and energy ###########\n",
    "    MaxD=[];MaxEnv=[];Er=[]\n",
    "    for ii,i in enumerate(evt_data):\n",
    "        data = np.zeros(200*fs)\n",
    "        crap=i.copy()\n",
    "\n",
    "        # get response\n",
    "        inv = client2.get_stations(network=crap.stats.network, station=crap.stats.station,\n",
    "                            location=crap.stats.location, channel=crap.stats.channel,\n",
    "                            level=\"response\", starttime=crap.stats.starttime, endtime=crap.stats.endtime)\n",
    "        \n",
    "        # The instrument sensitivity (gain) is stored in the response object\n",
    "        channel = inv[0][0][0]\n",
    "        response = channel.response\n",
    "        gain = response.instrument_sensitivity.value \n",
    "        # remove response\n",
    "        crap.remove_response(inventory=inv,output=\"DISP\")\n",
    "        otime1 = crap.stats.starttime + smb_peak[ii] # pick time\n",
    "        crap.trim(otime1  - 10, otime1 + 1.5*duration[ii] + 10) # window the data around the pick time\n",
    "        crap.taper(max_percentage=0.01,max_length=20)\n",
    "        crap.plot()\n",
    "\n",
    "\n",
    "        data_envelope = obspy.signal.filter.envelope(crap.data)\n",
    "        data_envelope = obspy.signal.util.smooth(crap, smooth_length)\n",
    "\n",
    "        MaxD.append(np.max(np.abs(crap.data)))\n",
    "        MaxEnv.append(np.max(np.abs(data_envelope)))\n",
    "\n",
    "\n",
    "        ## get energy\n",
    "        crap=i.copy()\n",
    "        crap.remove_response(inventory=inv,output=\"VEL\")\n",
    "        otime1 = crap.stats.starttime + smb_peak[ii] # pick time\n",
    "        crap.trim(otime1  - 10, otime1 + 1.5*duration[ii] + 10) # window the data around the pick time\n",
    "        crap.taper(max_percentage=0.01,max_length=20)\n",
    "        Er.append(np.sum(crap.data**2)*crap.stats.sampling_rate)\n",
    "\n",
    "\n",
    "        ############# KEEP DATA #######################\n",
    "\n",
    "    #         if not max(smb_peak.shape):continue\n",
    "    ddict = {'otime':otime, 'nets':nets, 'stas':stas,  'snr':snr, 'smb_peak': smb_peak, 'max_time':max_time, 'centroid_time': centroid_time , \\\n",
    "            'lats':lats, 'lons':lons, 'elevs':els, 'char_freq':char_freq, 'duration':duration, \\\n",
    "                'sharp_weight':sharp_weight, 'volcano':associated_volcano, 'event_ID':event_ID, \\\n",
    "                    'MaxD':MaxD,'MaxEnv':MaxEnv,'ER':Er}\n",
    "    if not np.any(dff):\n",
    "        dff = pd.DataFrame.from_dict(ddict)\n",
    "    else:\n",
    "        dff=pd.concat([dff,pd.DataFrame.from_dict(ddict)],ignore_index=True)\n",
    "    dff.tail()\n",
    "dff.to_csv(\"../data/events/catalog_all_rainier.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07af71",
   "metadata": {},
   "source": [
    "## Event location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3bc3a",
   "metadata": {},
   "source": [
    "### Volcano data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c702477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "associated_volcano == 'Mt_Rainier'\n",
    "        \n",
    "#get info for stations within 50km of volcano that event ocurred at\n",
    "stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "############ LOCATION ############################\n",
    "# input necessary data for grid search\n",
    "arrivals = dff['smb_peak'].values\n",
    "# arrivals = dff[dff['event_ID']==event_ID]['smb_peak'].values\n",
    "sta_lats = dff['lats'].values\n",
    "sta_lons = dff['lons'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "\n",
    "\n",
    "# Define the projection: UTM zone 11 for Washington state\n",
    "proj = pyproj.Proj(proj='utm', zone=10, ellps='WGS84')\n",
    "\n",
    "# Convert lat/long to Cartesian in meters\n",
    "xsta, ysta = proj(sta_lons, sta_lats)\n",
    "new_xsta =( xsta - np.min(xsta))/1E3\n",
    "new_ysta = (ysta - np.min(ysta))/1E3\n",
    "\n",
    "# cmap = plt.get_cmap('hot_r')\n",
    "# ista=np.where(arrivals>0)[0]\n",
    "# imin = np.argmin(arrivals[ista])\n",
    "# for i in ista:\n",
    "#     tt = arrivals[i]-np.min(arrivals[ista])\n",
    "#     nmax=np.max(arrivals[ista])-np.min(arrivals[ista])\n",
    "#     plt.plot(new_xsta[i],new_ysta[i],'o',color=cmap(tt/nmax),markersize=10,markeredgecolor='k')\n",
    "#     plt.text(new_xsta[i],new_ysta[i],stas[i]+\":\"+str(np.ceil(tt*10)/10)+\" s\")\n",
    "#     plt.axis('equal')\n",
    "# plt.title(\"Travel time Relative to \" + dff['stas'].values[ista[imin]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de953a80",
   "metadata": {},
   "source": [
    "Now we are confident that we can do the grid search. let's check the other fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd6766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65a2032f",
   "metadata": {},
   "source": [
    "Add the rows to the entire data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e2d58",
   "metadata": {},
   "source": [
    "Number of picks total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462034f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "npicks = dff.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['x_best']=np.zeros(npicks)\n",
    "dff['y_best']=np.zeros(npicks)\n",
    "dff['lat_best']=np.zeros(npicks)\n",
    "dff['lon_best']=np.zeros(npicks)\n",
    "dff['dist']=np.zeros(npicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d35686",
   "metadata": {},
   "source": [
    "## Locate all of the events!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xxbest=[]\n",
    "yybest=[]\n",
    "llatbest=[]\n",
    "llonbest=[]\n",
    "\n",
    "df=[]\n",
    "for nn in range(len(evt_id)):\n",
    "  n = len(evt_id)-nn\n",
    "  arrivals = dff[dff['event_ID']==evt_id[n]]['smb_peak'].values\n",
    "  ista =np.where(arrivals>-t_before+0.1)[0]\n",
    "\n",
    "  # location\n",
    "  rss_mat,t_best,lon_best,lat_best,x_best,y_best,idx_best = gridsearch(lat_start,lon_start,\\\n",
    "                                                lat_end,lon_end,\\\n",
    "                                                sta_lats[ista],sta_lons[ista],arrivals[ista],vs=vs)\n",
    "\n",
    "  \n",
    "  # distances\n",
    "  dist=[]\n",
    "  for ista in range(len(xsta)):\n",
    "      crap=np.sqrt( (xsta[ista] - x_best)**2 + (ysta[ista] - y_best)**2)\n",
    "      dist.append(crap[0])\n",
    "\n",
    "\n",
    "  # doppler effects\n",
    "\n",
    "## calculate azimuth\n",
    "\n",
    "  # Calculate the azimuth between the stations and the source location\n",
    "  dff['azimuth'] =  #dff.apply(calculate_azimuth, axis=1)\n",
    "\n",
    "\n",
    "  crap = np.zeros(len(xsta))\n",
    "  crap2 = np.zeros(len(xsta))\n",
    "  for ista in range(len(xsta)):\n",
    "      A = dff[dff['event_ID']==evt_id[n]]['MaxD'].values[ista]*1E6\n",
    "      A2 = dff[dff['event_ID']==evt_id[n]]['MaxEnv'].values[ista]*1E6\n",
    "      d = dff[dff['event_ID']==evt_id[n]]['dist'].values[ista]/1E3\n",
    "      crap[ista]=  np.log(A) + 0.55*np.log(d)+2.44\n",
    "      crap2[ista]=  np.log(A2) + 0.55*np.log(d)+2.44\n",
    "\n",
    "  dff[dff['event_ID']==evt_id[n]]['Lm_sta_D']=crap\n",
    "  dff[dff['event_ID']==evt_id[n]]['Lm_evt_D'] = np.median(crap)\n",
    "  dff[dff['event_ID']==evt_id[n]]['Lm_sta_Env']=crap2\n",
    "  dff[dff['event_ID']==evt_id[n]]['Lm_evt_Env'] = np.median(crap2)\n",
    "\n",
    "\n",
    "\n",
    "  #         if not max(smb_peak.shape):continue\n",
    "  dff[dff['event_ID']==evt_id[n]]['x_best']=x_best\n",
    "  dff[dff['event_ID']==evt_id[n]]['y_best']=y_best\n",
    "  dff[dff['event_ID']==evt_id[n]]['lat_best']=lat_best\n",
    "  dff[dff['event_ID']==evt_id[n]]['lon_best']=lon_best\n",
    "  dff[dff['event_ID']==evt_id[n]]['dist']=dist\n",
    "\n",
    "  dff.tail()\n",
    "\n",
    "\n",
    "\n",
    "dff.to_csv(\"../data/events/catalog_all_rainier.csv\",index=False)\n",
    "\n",
    "  # dff['xsta']=xsta\n",
    "  # dff['ysta']=ysta\n",
    "  # dff['xbest']=x_best*np.ones(len(xsta))\n",
    "  # dff['ybest']=y_best*np.ones(len(xsta))\n",
    "\n",
    "  # evlon,evlat = proj(x_best,y_best,inverse=True)\n",
    "  # dff['evlat']=evlat*np.ones(len(xsta))\n",
    "  # dff['evlon']=evlon*np.ones(len(xsta))\n",
    "\n",
    "  # dist=[]\n",
    "  # for ista in range(len(xsta)):\n",
    "  #     crap=np.sqrt( (xsta[ista] - x_best)**2 + (ysta[ista] - y_best)**2)\n",
    "  #     dist.append(crap[0])\n",
    "\n",
    "  # dff['dist']=dist\n",
    "print(t_best,lon_best,lat_best,x_best,y_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7284bd1",
   "metadata": {},
   "source": [
    "## Doppler effects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec33435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Fit a doppler function\n",
    "# Prepare data\n",
    "x_data = np.radians(dff['azimuth'].values[ista])\n",
    "y_data = dff['char_freq'].values[ista]\n",
    "\n",
    "# Initial guess for the parameters\n",
    "initial_guess = [0, 2*np.pi,  0]\n",
    "\n",
    "# Fit the function to the data\n",
    "params, params_covariance = curve_fit(doppler_func, x_data, y_data, p0=initial_guess)\n",
    "\n",
    "print(params)\n",
    "\n",
    "az=np.linspace(-np.pi,np.pi,100)\n",
    "dop = doppler_func(az,params[0],params[1],params[2])\n",
    "\n",
    "# Plot the characteristic frequency as a function of azimuth\n",
    "plt.figure(figsize=(10, 6))\n",
    "for station in dff['stas'].unique():\n",
    "    station_data = dff[dff['stas'] == station]\n",
    "    plt.plot(station_data['azimuth'], station_data['char_freq'],'o',markersize=14, label=station)\n",
    "plt.plot(np.degrees(az),dop,label='l2')\n",
    "plt.xlabel('Azimuth (degrees)')\n",
    "plt.ylabel('Characteristic Frequency (Hz)')\n",
    "plt.legend(loc='upper right',fontsize=10)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0367a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dff['snr'].values[ista])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defea303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "# Define the residuals function\n",
    "def residuals_func(params, x_data, y_data,weight):\n",
    "    return (y_data - doppler_func(x_data, *params))*weight\n",
    "\n",
    "# Initial guess for the parameters\n",
    "initial_guess = [0, 2*np.pi,  0]\n",
    "\n",
    "# Fit the function to the data using L1 norm minimization\n",
    "result = least_squares(residuals_func, initial_guess, args=(x_data, y_data,1/dff['snr'].values[ista]), loss='soft_l1')\n",
    "\n",
    "# The optimized parameters are stored in result.x\n",
    "params = result.x\n",
    "\n",
    "print(params)\n",
    "\n",
    "az=np.linspace(-np.pi,np.pi,100)\n",
    "dop = doppler_func(az,params[0],params[1],params[2])\n",
    "\n",
    "\n",
    "# Fit the function to the data\n",
    "params, params_covariance = curve_fit(doppler_func, x_data, y_data, p0=initial_guess,sigma=dff['snr'].values[ista])\n",
    "dop2 = doppler_func(az,params[0],params[1],params[2])\n",
    "\n",
    "# Plot the characteristic frequency as a function of azimuth\n",
    "plt.figure(figsize=(10, 6))\n",
    "for station in dff['stas'].unique():\n",
    "    station_data = dff[dff['stas'] == station]\n",
    "    plt.plot(station_data['azimuth'], station_data['char_freq'],'o',markersize=14, label=station)\n",
    "plt.plot(np.degrees(az),dop,label='l2')\n",
    "plt.plot(np.degrees(az),dop2,label='l1')\n",
    "plt.xlabel('Azimuth (degrees)')\n",
    "plt.ylabel('Characteristic Frequency (Hz)')\n",
    "plt.legend(loc='upper right',fontsize=10)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c43e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[0]<0:\n",
    "    direction = params[1]+np.pi \n",
    "else:\n",
    "    direction = params[1]   \n",
    "fmax = max(dop)\n",
    "fmin = min(dop)\n",
    "v = vs*((fmax-fmin)/(fmax+fmin))\n",
    "print(direction,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['flow_dir'] = direction\n",
    "dff['flow_speed'] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfd372",
   "metadata": {},
   "source": [
    "## Measure magnitude and radiated energy\n",
    "\n",
    "\n",
    "We may need to figure out the wave types using polarization.\n",
    "\n",
    "We will assume a body wave magnitude because the wavespeeds are quite high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4d486",
   "metadata": {},
   "source": [
    "## Get magnitude\n",
    "\n",
    "Follow the relation from Lin et al, 2015:\n",
    "$ Lm = \\log(A)+0.55 \\log(dist) + 2.44$\n",
    "\n",
    "where $A$ is in micrometers (1E-6 meters) and $dist$ is in km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "crap = np.zeros(len(xsta))\n",
    "crap2 = np.zeros(len(xsta))\n",
    "for ista in range(len(xsta)):\n",
    "    A = dff['MaxD'].values[ista]*1E6\n",
    "    A2 = dff['MaxEnv'].values[ista]*1E6\n",
    "    d = dff['dist'].values[ista]/1E3\n",
    "    crap[ista]=  np.log(A) + 0.55*np.log(d)+2.44\n",
    "    crap2[ista]=  np.log(A2) + 0.55*np.log(d)+2.44\n",
    "\n",
    "dff['Lm_sta_D']=crap\n",
    "dff['Lm_evt_D'] = np.median(crap)\n",
    "dff['Lm_sta_Env']=crap2\n",
    "dff['Lm_evt_Env'] = np.median(crap2)\n",
    "\n",
    "\n",
    "\n",
    "dff.to_csv(\"../data/events/new_catalog.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696334ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo_exo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
