{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0f6cb81",
   "metadata": {},
   "source": [
    "# Surface Event Phase Picking\n",
    "\n",
    "This is a modified version of the surface-event location+directivity analysis that Francesca Skene ( fskene@uw.edu), originally created by her in 7/22/22, who started as an undergraduate student at UW. This is marine denolle's version. It includes:\n",
    "* Waveform download for each event on each volcano given the PNSN pick times of \"su\" events.\n",
    "* Data pre-processing to trim the data within 1-20 Hz and remove outliers.\n",
    "* phase picking using transfer-learned model (Ni et al, 2023)\n",
    "* Centroid time picking using envelope measurements\n",
    "* Frequency measurements for doppler analysis\n",
    "* gathering of the data into a CSV data frame.\n",
    "\n",
    "Updated 03/21/2024\n",
    "Marine Denolle\n",
    "(mdenolle@uw.edu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7021dc6f",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf759fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/data/wsd01/pnwstore/')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from mbf_elep_func import *\n",
    "import torch\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "\n",
    "import seisbench.models as sbm\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "# from ELEP.elep.ensemble_learners import ensemble_regressor_cnn\n",
    "from ELEP.elep import mbf, mbf_utils\n",
    "from ELEP.elep import trigger_func\n",
    "\n",
    "from ELEP.elep.mbf_utils import make_LogFq, make_LinFq, rec_filter_coeff, create_obspy_trace\n",
    "from ELEP.elep.mbf import MB_filter as MBF\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import pyproj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd30de3d",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clients to download the station data\n",
    "# client = WaveformClient() # we ignore PNWdatastore for now\n",
    "client2 = Client('IRIS') # IRIS client\n",
    "\n",
    "t_before = 15 #number of seconds before pick time\n",
    "# t_after = 15 #number of seconds after pick time\n",
    "t_before_raw = 1200 #number of seconds before pick time before removing instrumental response\n",
    "# t_after_raw = 1200 #number of seconds after pick time before removing instrumental response\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 150 #window length of the signal (this will help with phase picking with EqT next). \n",
    "# Use 150 seconds @ 40 Hz gives 6001 points. \n",
    "pr = 98 #percentile\n",
    "thr = 7 #SNR threshold\n",
    "station_distance_threshold = 25 #distance threshold in km\n",
    "pi = np.pi\n",
    "v_s = 1000 #shear wave velocity at the surface\n",
    "\n",
    "# range of dates that we are looking at\n",
    "t_beginning = UTCDateTime(2001,1,1,0,0,0) \n",
    "t_end = UTCDateTime(2024,1,1,23,59)\n",
    "\n",
    "smooth_length = 20 # constant for smoothing the waveform envelopes\n",
    "low_cut = 1 #low frequency threshold\n",
    "high_cut = 15 #high frequency threshold\n",
    "az_thr = 1000 #threshold of distance in meters from source location\n",
    "step = 100 #step every 100 m\n",
    "t_step = 1 #step every second\n",
    "ratio = 5.6915196 #used to define the grid \n",
    "# colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3 # radius of the earth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4db39e",
   "metadata": {},
   "source": [
    "## Volcano - Station Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ede0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('../data/station/Volcano_Metadata_50km.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b021322",
   "metadata": {},
   "source": [
    "## PNSN SU Pick information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.read_csv(\"../data/events/su_picks.txt\",sep=\"|\") \n",
    "f1.head()\n",
    "print(f1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the spaces in the file\n",
    "format='%Y/%m/%d %H:%M:%S'\n",
    "test=f1[\"date\"].values.tolist()\n",
    "start_time_temp = [  datetime.strptime(x.strip(),'%Y/%m/%d %H:%M:%S') for x in f1[\"date\"].values.tolist()]\n",
    "# # Ignore events prior to t_beginning\n",
    "ik=np.where(np.array(start_time_temp)>datetime(2001,1,1))[0][0]\n",
    "\n",
    "# select only net, sta, evid, startime for event past the start date.\n",
    "\n",
    "start_time = start_time_temp[ik:]\n",
    "net=[ x.strip() for x in f1[\"net\"].values.tolist()][ik:]\n",
    "sta=[ x.strip() for x in f1[\"sta\"].values.tolist()][ik:]\n",
    "evt_id=[ x for x in f1[\"orid\"].values.tolist()][ik:]\n",
    "all_stas=set(sta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20c89405",
   "metadata": {},
   "source": [
    "## ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.makedirs(\"/Users/marinedenolle/.seisbench/models/v3/eqtransformer\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/congcy/ELEP/raw/main/docs/tutorials/data/pnw.pt.v1 -O ~/.seisbench/models/v3/eqtransformer/pnw.pt.v1\n",
    "# !wget https://github.com/congcy/ELEP/raw/main/docs/tutorials/data/pnw.json.v1 -O ~/.seisbench/models/v3/eqtransformer/pnw.json.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download models\n",
    "list_models_name = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "# pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "\n",
    "list_models = [pn_pnw_model, pn_ethz_model, pn_instance_model, pn_scedc_model, pn_stead_model, pn_geofon_model]\n",
    "\n",
    "pn_pnw_model.to(device);\n",
    "pn_ethz_model.to(device);\n",
    "pn_scedc_model.to(device);\n",
    "# pn_neic_model.to(device);\n",
    "pn_geofon_model.to(device);\n",
    "pn_stead_model.to(device);\n",
    "pn_instance_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_semblance = {'dt':0.025, 'semblance_order':4, 'window_flag':True, \n",
    "                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "p_thrd, s_thrd = 0.01, 0.05\n",
    "\n",
    "fqmin = low_cut\n",
    "fqmax = high_cut\n",
    "dt = 0.025; fs = 40\n",
    "nfqs = 10\n",
    "nt = 6000; nc = 3\n",
    "fq_list = make_LogFq(fqmin, fqmax, dt, nfqs)\n",
    "coeff_HP, coeff_LP = rec_filter_coeff(fq_list, dt)\n",
    "MBF_paras = {'f_min':fqmin, 'f_max':fqmax, 'nfqs':nfqs, 'frequencies':fq_list, 'CN_HP':coeff_HP, 'CN_LP':coeff_LP, \\\n",
    "    'dt':dt, 'fs':fs, 'nt':nt, 'nc':nc, 'npoles': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605be24",
   "metadata": {},
   "source": [
    "# Measurements\n",
    "\n",
    "* download waveforms\n",
    "* phase pick onset\n",
    "* estimate SNR\n",
    "* measure centroid, max envelope, duration\n",
    "* measure Fmax for doppler analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075da112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf = PdfPages('../plots/Mt_RAinier_plot.pdf')\n",
    "associated_volcano = \"Mt_Rainier\"\n",
    "dff=[] \n",
    "# event_ID = '0000' #str(evt_id[n])\n",
    "nplot=0\n",
    "for n in range(len(evt_id)):\n",
    "    if start_time[n]<datetime(2022,1,1):continue   \n",
    "    event_ID = str(evt_id[n])\n",
    "    if (n>1) & (event_ID==str(evt_id[n-1])):continue\n",
    "    otime = UTCDateTime(start_time[n])  \n",
    "    associated_volcano=\"Mt_Rainier\"\n",
    "\n",
    "\n",
    "    #get info for stations within 50km of volcano that event ocurred at\n",
    "    stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "    networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "    latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "    longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "    elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "\n",
    "    #################### WAVEFORM DOWNLOAD #######################\n",
    "    #Download all waveforms for that event based on stations and times\n",
    "    bulk = [] \n",
    "    for m in range(0, len(networks)):\n",
    "        bulk.append([networks[m], stations[m], '*', '*Z', otime-t_before_raw, otime+t_before_raw])\n",
    "    try:\n",
    "        st = client2.get_waveforms_bulk(bulk)\n",
    "        st = resample(st,fs)  #resampling the data to 40Hz for each trace\n",
    "        evt_data = obspy.Stream()\n",
    "        snr=[]\n",
    "        stas=[]\n",
    "        nets=[]\n",
    "        lats=[]\n",
    "        lons=[]\n",
    "        els=[]\n",
    "        centroid_time = []\n",
    "        data_env_dict = {}\n",
    "        duration = []\n",
    "        max_time = []\n",
    "\n",
    "        # #Keeping all traces for one event with channel z, SNR>10, and bandpassed between 2-12Hz\n",
    "        # ,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],[],{},{}\n",
    "        for i,ii in enumerate(st):\n",
    "            ii.detrend(type = 'demean')\n",
    "            ii.filter('bandpass',freqmin=low_cut,freqmax=high_cut,corners=2,zerophase=True)\n",
    "            # trim the data and noise window to exactly 6000 points\n",
    "            signal_window = ii.copy()\n",
    "            noise_window = ii.copy()\n",
    "            signal_window.trim(otime - t_before, otime - t_before + window) # trim the signal at the first pick time of the PNSN data, with loose 40s before\n",
    "            noise_window.trim(otime - window -t_before, otime - t_before) # noise window of the same length\n",
    "            if  len(signal_window.data)<=10 or  len(noise_window.data)<=10: continue # skip if no data\n",
    "\n",
    "            snr2 = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                            / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "            # if not np.percentile(np.abs(signal_window.data),pr):continue # skip if max amplitude is zero\n",
    "            snr1 = (20 * np.log(np.percentile(np.abs(signal_window.data[:signal_window.stats.npts//2]),pr) \n",
    "                            / np.percentile(np.abs(noise_window.data[:noise_window.stats.npts//2]),pr))/np.log(10))\n",
    "            \n",
    "            # snr1 = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                            # / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "\n",
    "            if snr1<thr: # and 100<max_amp_time<200:\n",
    "                st.remove(ii)\n",
    "                continue\n",
    "\n",
    "        ################# ENVELOPE, CENTROID, DURATION #######################\n",
    "            # enveloping the data \n",
    "            data_envelope = obspy.signal.filter.envelope(signal_window.data)\n",
    "            data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "\n",
    "            data_env_dict[ii.stats.network+'.'+ii.stats.station]= data_envelope/max(np.abs(data_envelope))\n",
    "            print(\"snr\",snr1,snr2)\n",
    "            print(ii.stats.network+'.'+ii.stats.station)\n",
    "            # max time\n",
    "            # finding the time of max amplitude of each event\n",
    "            # signal_window is windowed at otime-t_v before the PNSN pick time\n",
    "            # crap = np.argmax(np.abs(data_envelope[:(t_before+40)*fs])) # time of max amplitude relative to otime\n",
    "\n",
    "            # centroid time\n",
    "            ss = signal_window.copy()\n",
    "            ss.trim()\n",
    "            tcrap = signal_window.times()-t_before\n",
    "            it = np.where(tcrap>0)[0] # origin tim\n",
    "            # print(ii.stats.station,max_time[-1],centroid_time[-1])\n",
    "\n",
    "            # find duration as data starting with the \"origin time\" and ending when the envelope falls below the mean noise\n",
    "            data_envelope -= np.mean(data_envelope[:t_before*fs])\n",
    "            \n",
    "            mmax = np.max(np.cumsum(data_envelope[it]**4))\n",
    "            crap = np.where( np.cumsum(data_envelope[it]**4) <= 0.999*mmax)[0]#[-1]\n",
    "            duration.append(len(crap)/fs)\n",
    "\n",
    "            it = np.where((tcrap>0) & (tcrap<duration[-1]))[0] # select the time window after origin and before the end\n",
    "            centroid_time.append(np.sum(data_envelope[it]*tcrap[it])/np.sum(data_envelope[it]))\n",
    "\n",
    "            max_time.append(tcrap[it[np.argmax(data_envelope[it])]])\n",
    "\n",
    "            stas.append(ii.stats.station)\n",
    "            nets.append(ii.stats.network)\n",
    "            ista=stations.index(ii.stats.station)\n",
    "            lats.append(latitudes[ista])\n",
    "            lons.append(longitudes[ista])\n",
    "            els.append(elevations[ista])\n",
    "            snr.append(snr1)\n",
    "            evt_data.append(signal_window)\n",
    "\n",
    "            t = evt_data.select(station=stas[-1])[0].times()\n",
    "            \n",
    "        if len(stas)<3:continue\n",
    "        centroid_time = np.asarray(centroid_time)\n",
    "        # centroid_time -= t_before\n",
    "        max_time = np.asarray(max_time)\n",
    "        # max_time -= t_before\n",
    "        duration = np.asarray(duration)\n",
    "        # duration -= t_before\n",
    "\n",
    "        ################### ELEP #######################\n",
    "\n",
    "            # test the new function\n",
    "        smb_peak= apply_elep(evt_data, stas, \\\n",
    "                list_models, MBF_paras, paras_semblance, t_before)\n",
    "        smb_peak -= t_before\n",
    "\n",
    "\n",
    "        ############### RECALCULATE CENTROID & DURATION ################\n",
    "        new_centroid_time = np.zeros(len(stas))\n",
    "        new_duration = np.zeros(len(stas))\n",
    "        for ista in range(len(stas)):\n",
    "            tt_data = evt_data.select(station=stas[ista])[0].data\n",
    "            data_envelope = obspy.signal.filter.envelope(tt_data)\n",
    "            data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "            t = evt_data.select(station=stas[ista])[0].times()\n",
    "            data_envelope = data_envelope[:len(t)]\n",
    "            data_envelope = data_envelope - np.mean(data_envelope[0:int(t_before*fs)]) # remove the mean of the noise\n",
    "            data_envelope = data_envelope/np.max(np.abs(data_envelope)) # normalize the envelope\n",
    "            t = t - t_before - smb_peak[ista] # shift the time to the pick time\n",
    "            ikk=np.where(t>0)[0]#[0] # find the first positive time\n",
    "            # data_envelope = data_envelope[ikk]\n",
    "            mmax = np.max(np.cumsum(data_envelope[ikk]**4))\n",
    "            crap = np.where( np.cumsum(data_envelope[ikk]**4) <= 0.999*mmax)[0]#[-1]\n",
    "            new_duration[ista]= len(crap)/fs\n",
    "\n",
    "            it = np.where((t>0) & (t<new_duration[ista]))[0]\n",
    "            new_centroid_time[ista] = np.sum(data_envelope[it]*t[it])/np.sum(data_envelope[it])+smb_peak[ista]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(stas[ista],\"old centroid time\", centroid_time[ista], \"new centroid time\",new_centroid_time[ista])\n",
    "            print(stas[ista],\"old duration\", duration[ista], \"new duration time\",new_duration[ista])\n",
    "\n",
    "\n",
    "\n",
    "        ############## PEAK FREQUENCY MEASUREMENTS ############\n",
    "        # Given the approximate measurement of duration, window the signal windows around that\n",
    "        # then measure peak frequency so that there is less noise in it.\n",
    "        # perform this on the Z component only.\n",
    "\n",
    "        char_freq, sharp_weight= [],[]\n",
    "        # fig1,ax1 = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "        for ii,i in enumerate(evt_data):\n",
    "            data = np.zeros(200*fs)\n",
    "            crap=i.copy()\n",
    "            otime1 = crap.stats.starttime + smb_peak[ii] # pick time\n",
    "            crap.trim(otime1  - 10, otime1 + 2*new_duration[ii] + 10) # window the data around the pick time\n",
    "            crap.taper(max_percentage=0.01,max_length=20)\n",
    "\n",
    "            data[:len(crap.data)] = crap.data #*100\n",
    "            f,psd=scipy.signal.welch(data,fs=fs,nperseg=81,noverlap=4)\n",
    "            #just get the frequencies within the filter band\n",
    "            above_low_cut = [f>low_cut]\n",
    "            below_high_cut = [f<high_cut]\n",
    "            in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "            f = f[in_band]\n",
    "            psd = psd[in_band]\n",
    "\n",
    "            # calculate characteristic frequency and report\n",
    "            char_freq_max = f[np.argmax(psd)]\n",
    "            char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "            psd_cumsum = np.cumsum(psd)\n",
    "            psd_sum = np.sum(psd)\n",
    "            char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "            char_freq.append(char_freq_mean)\n",
    "\n",
    "            # plt.rcParams.update({'font.size': 20})\n",
    "            # p=ax1.plot(f,psd,label=stas[ii],linewidth=2)\n",
    "            # cc = p[0].get_color()\n",
    "            # ax1.set_xscale('log')\n",
    "            # ax1.set_yscale('log')\n",
    "            # ax1.grid('True')\n",
    "            # ax1.set_xlabel('Frequency [Hz]')\n",
    "            # ax1.set_ylabel('PSD [$(mm/s)^2$/Hz]')\n",
    "            # ax1.vlines(char_freq_mean,ymin=np.min(psd)/10,ymax=np.max(psd)*10,linestyle=\"--\",colors=cc)\n",
    "\n",
    "        #             # weighting the data by the spikiness of the PSD vs frequency graphs\n",
    "            ratio = (np.mean(psd)/np.max(psd))\n",
    "            sharp_weight.append(int(1/(ratio**2)*20))\n",
    "            # del fig1,ax1\n",
    "\n",
    "\n",
    "            ############# KEEP DATA #######################\n",
    "\n",
    "        #         if not max(smb_peak.shape):continue\n",
    "        ddict = {'otime':otime, 'nets':nets, 'stas':stas,  'snr':snr, 'smb_peak': smb_peak, 'max_time':max_time, 'centroid_time': centroid_time , \\\n",
    "                'lats':lats, 'lons':lons, 'elevs':els, 'char_freq':char_freq, 'duration':duration,'new_duration':new_duration, \\\n",
    "                    'new_centroid':new_centroid_time,'sharp_weight':sharp_weight, 'volcano':associated_volcano, 'event_ID':event_ID}\n",
    "        if not np.any(dff):\n",
    "            dff = pd.DataFrame.from_dict(ddict)\n",
    "            dff.to_csv(\"../data/events/MLPicks_MtRainier.csv\")\n",
    "        else:\n",
    "            dff=pd.concat([dff,pd.DataFrame.from_dict(ddict)],ignore_index=True)\n",
    "            dff.to_csv(\"../data/events/MLPicks_MtRainier.csv\")\n",
    "\n",
    "        print(dff)\n",
    "        if nplot<100:\n",
    "            nplot+=1\n",
    "            fig = plt.figure(figsize = (11,8), dpi=400)\n",
    "            fig.suptitle(str(otime)+\" \"+associated_volcano)\n",
    "            ax = plt.subplot(1,1,1)\n",
    "            iplot = 0\n",
    "            for i in range(len(stas)):\n",
    "                data = evt_data.select(station=stas[i])[0].data\n",
    "                max1 = np.max(np.abs(data))\n",
    "                t = evt_data.select(station=stas[i])[0].times()\n",
    "                ax.plot(t-t_before,data/max1+iplot*1.5,linewidth=0.5) # plot the data\n",
    "                if np.any(data_env_dict[nets[i]+'.'+stas[i]]): # plot the envelope\n",
    "                    ax.plot(t-t_before,data_env_dict[nets[i]+'.'+stas[i]]+iplot*1.5,'k',linewidth=1)\n",
    "                # ax.plot(smb_peak[i],iplot*1.5,'r*',markersize=5)   #  the pick time\n",
    "                ax.plot(centroid_time[i],iplot*1.5,'rp',markersize=5)   #  old centroid time\n",
    "                ax.plot(new_centroid_time[i],iplot*1.5,'kp',markersize=5)   #  new centroid time\n",
    "                ax.plot(max_time[i],iplot*1.5,'r*',markersize=5)  # maimum t\n",
    "                ax.set_yticks([])\n",
    "                ax.text(-15, iplot*1.5+0.5, stas[i])\n",
    "                ax.vlines(smb_peak[i],iplot*1.5-1.,iplot*1.5+1.,'r') # pick time\n",
    "                ax.vlines(smb_peak[i]+duration[i],iplot*1.5-1.,iplot*1.5+1.,'k') # old duration\n",
    "                ax.vlines(smb_peak[i]+new_duration[i],iplot*1.5-1.,iplot*1.5+1.,'b') # new duration\n",
    "                print(sta[i],duration[i],char_freq[i])\n",
    "\n",
    "                iplot+=1\n",
    "            ax.grid(True)\n",
    "            ax.set_xlim([-t_before,90])\n",
    "            ax.set_xlabel('time (seconds) relative to PNSN picks')\n",
    "            plt.show()\n",
    "            pdf.savefig(fig)\n",
    "            plt.clf()\n",
    "            del fig\n",
    "        dff.describe()\n",
    "    except:\n",
    "        print(\"No data for event\",event_ID)\n",
    "dff.to_csv(\"../data/events/MLPicks_MtRainier.csv\")\n",
    "\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_window.stats.npts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo_exo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
