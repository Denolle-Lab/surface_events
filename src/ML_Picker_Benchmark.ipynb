{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark ELEP for phase picking surface events\n",
    "\n",
    "We will test the ELEP workflow to phase picks the SU events picked by the PNSN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/data/wsd01/pnwstore/')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from geopy import distance\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from mbf_elep_func import *\n",
    "import torch\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "\n",
    "import seisbench.models as sbm\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "# from ELEP.elep.ensemble_learners import ensemble_regressor_cnn\n",
    "from ELEP.elep import mbf, mbf_utils\n",
    "from ELEP.elep import trigger_func\n",
    "\n",
    "from ELEP.elep.mbf_utils import make_LogFq, make_LinFq, rec_filter_coeff, create_obspy_trace\n",
    "from ELEP.elep.mbf import MB_filter as MBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clients to download the station data\n",
    "# client = WaveformClient() # we ignore PNWdatastore for now\n",
    "client2 = Client('IRIS')\n",
    "\n",
    "t_before = 15 #number of seconds before pick time\n",
    "# t_after = 15 #number of seconds after pick time\n",
    "t_before_raw = 1200 #number of seconds before pick time before removing instrumental response\n",
    "# t_after_raw = 1200 #number of seconds after pick time before removing instrumental response\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 150 #window length of the signal (this will help with phase picking with EqT next). \n",
    "# Use 150 seconds @ 40 Hz gives 6001 points. \n",
    "pr = 98 #percentile\n",
    "thr = 5 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "v_s = 1000 #shear wave velocity at the surface\n",
    "\n",
    "# range of dates that we are looking at\n",
    "t_beginning = UTCDateTime(2001,1,1,0,0,0) \n",
    "t_end = UTCDateTime(2023,8,2,23,59)\n",
    "\n",
    "# smooth_length = 5 # constant for smoothing the waveform envelopes\n",
    "low_cut = 1 #low frequency threshold\n",
    "high_cut = 12 #high frequency threshold\n",
    "# az_thr = 1000 #threshold of distance in meters from source location\n",
    "# step = 100 #step every 100 m\n",
    "# t_step = 1 #step every second\n",
    "# ratio = 5.6915196 #used to define the grid \n",
    "# colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "# radius = 6371e3 # radius of the earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read ML models\n",
    "\n",
    "Make sure that model weights are downloaded for seisbench\n",
    "```\n",
    "import os\n",
    "os.makedirs(\"/Users/marinedenolle/.seisbench/models/v3/eqtransformer\",exist_ok=True)\n",
    "!wget https://github.com/congcy/ELEP/raw/main/docs/tutorials/data/pnw.pt.v1 -O ~/.seisbench/models/v3/eqtransformer/pnw.pt.v1\n",
    "!wget https://github.com/congcy/ELEP/raw/main/docs/tutorials/data/pnw.json.v1 -O ~/.seisbench/models/v3/eqtransformer/pnw.json.v1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ML picker parameters\n",
    "paras_semblance = {'dt':0.025, 'semblance_order':4, 'window_flag':True, \n",
    "                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "p_thrd, s_thrd = 0.01, 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download models\n",
    "list_models_name = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "# pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "\n",
    "list_models = [pn_pnw_model, pn_ethz_model, pn_instance_model, pn_scedc_model, pn_stead_model, pn_geofon_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_pnw_model.to(device);\n",
    "pn_ethz_model.to(device);\n",
    "pn_scedc_model.to(device);\n",
    "# pn_neic_model.to(device);\n",
    "pn_geofon_model.to(device);\n",
    "pn_stead_model.to(device);\n",
    "pn_instance_model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volcano info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('../data/station/Volcano_Metadata_50km.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Surface Event Data from PNSN\n",
    "\n",
    "Read files straight from PNSN data base. Extract station and network code, phase pick time (seen as start_time). this should be converted to pick_time0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'net', 'sta', 'location', 'seedchan', 'iphase', 'quality',\n",
      "       'orid', 'etype', 'evid'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "f1 = pd.read_csv(\"../data/events/su_picks.txt\",sep=\"|\")\n",
    "f1.head()\n",
    "print(f1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the spaces in the file\n",
    "format='%Y/%m/%d %H:%M:%S'\n",
    "test=f1[\"date\"].values.tolist()\n",
    "start_time_temp = [  datetime.strptime(x.strip(),'%Y/%m/%d %H:%M:%S') for x in f1[\"date\"].values.tolist()]\n",
    "# # Ignore events prior to t_beginning\n",
    "ik=np.where(np.array(start_time_temp)>datetime(2001,1,1))[0][0]\n",
    "\n",
    "# select only net, sta, evid, startime for event past the start date.\n",
    "\n",
    "start_time = start_time_temp[ik:]\n",
    "net=[ x.strip() for x in f1[\"net\"].values.tolist()][ik:]\n",
    "sta=[ x.strip() for x in f1[\"sta\"].values.tolist()][ik:]\n",
    "evt_id=[ x for x in f1[\"orid\"].values.tolist()][ik:]\n",
    "all_stas=set(sta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_semblance = {'dt':0.025, 'semblance_order':4, 'window_flag':True, \n",
    "                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "p_thrd, s_thrd = 0.01, 0.05\n",
    "\n",
    "fqmin = low_cut\n",
    "fqmax = high_cut\n",
    "dt = 0.025; fs = 40\n",
    "nfqs = 10\n",
    "nt = 6000; nc = 3\n",
    "fq_list = make_LogFq(fqmin, fqmax, dt, nfqs)\n",
    "coeff_HP, coeff_LP = rec_filter_coeff(fq_list, dt)\n",
    "MBF_paras = {'f_min':fqmin, 'f_max':fqmax, 'nfqs':nfqs, 'frequencies':fq_list, 'CN_HP':coeff_HP, 'CN_LP':coeff_LP, \\\n",
    "    'dt':dt, 'fs':fs, 'nt':nt, 'nc':nc, 'npoles': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download data and phase pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "829903 2002-01-01T21:34:50.000000Z\n",
      "1\n",
      "0.25 0.25\n",
      "829903 2002-01-01T21:34:52.000000Z\n",
      "1\n",
      "0.22499999999999964 0.22499999999999964\n",
      "830593 2002-01-02T17:38:19.000000Z\n",
      "1\n",
      "0.6999999999999993 0.6999999999999993\n",
      "830593 2002-01-02T17:38:20.000000Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinedenolle/opt/miniconda3/envs/seismo_exo/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/marinedenolle/opt/miniconda3/envs/seismo_exo/lib/python3.10/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/marinedenolle/opt/miniconda3/envs/seismo_exo/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "830593 2002-01-02T17:38:22.000000Z\n",
      "0\n",
      "830623 2002-01-02T20:16:48.000000Z\n",
      "1\n",
      "1.4499999999999993 1.4499999999999993\n",
      "831613 2002-01-06T20:50:09.000000Z\n",
      "1\n",
      "0.7750000000000004 0.7750000000000004\n",
      "831613 2002-01-06T20:50:09.000000Z\n",
      "1\n",
      "0.4250000000000007 0.4250000000000007\n",
      "831613 2002-01-06T20:50:10.000000Z\n",
      "1\n",
      "0.22499999999999964 0.22499999999999964\n",
      "837748 2002-01-31T21:59:43.000000Z\n",
      "1\n",
      "1.6999999999999993 1.6999999999999993\n",
      "830748 2002-02-05T03:19:41.000000Z\n",
      "1\n",
      "0.4499999999999993 0.4499999999999993\n",
      "830748 2002-02-05T03:19:42.000000Z\n",
      "1\n",
      "0.3249999999999993 0.3249999999999993\n",
      "830748 2002-02-05T03:19:43.000000Z\n",
      "1\n",
      "0.125 0.125\n",
      "832653 2002-02-14T19:25:21.000000Z\n",
      "1\n",
      "0.5 0.5\n",
      "835223 2002-02-22T20:12:30.000000Z\n",
      "1\n",
      "0.3000000000000007 0.3000000000000007\n",
      "832268 2002-03-08T18:44:26.000000Z\n",
      "1\n",
      "1.4499999999999993 1.4499999999999993\n",
      "832278 2002-03-08T18:45:23.000000Z\n",
      "0\n",
      "832878 2002-03-10T22:52:25.000000Z\n",
      "1\n",
      "0.5749999999999993 0.5749999999999993\n",
      "833068 2002-03-11T14:13:12.000000Z\n",
      "1\n",
      "0.05000000000000071 0.05000000000000071\n",
      "833228 2002-03-12T21:37:43.000000Z\n",
      "1\n",
      "0.05000000000000071 0.05000000000000071\n",
      "835908 2002-03-22T13:17:08.000000Z\n",
      "1\n",
      "0.07499999999999929 0.07499999999999929\n",
      "836198 2002-03-23T02:30:15.000000Z\n",
      "0\n",
      "836198 2002-03-23T02:30:30.000000Z\n",
      "1\n",
      "0.5250000000000004 0.5250000000000004\n",
      "836218 2002-03-23T02:31:14.000000Z\n",
      "0\n",
      "837178 2002-03-25T22:58:39.000000Z\n",
      "1\n",
      "0.4250000000000007 0.4250000000000007\n",
      "838613 2002-03-28T20:33:17.000000Z\n",
      "1\n",
      "0.07499999999999929 0.07499999999999929\n",
      "833208 2002-04-07T04:23:44.000000Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/lzmy975n0l5bjbmr9db291m00000gn/T/ipykernel_29486/4065219544.py:92: RuntimeWarning: invalid value encountered in divide\n",
      "  data_std = crap2 / np.std(crap2) + 1e-10\n",
      "/Users/marinedenolle/GitHub/surface_events/src/mbf_elep_func.py:151: RuntimeWarning: invalid value encountered in divide\n",
      "  data_std = crap2 / np.std(crap2) + 1e-10\n",
      "/var/folders/js/lzmy975n0l5bjbmr9db291m00000gn/T/ipykernel_29486/4065219544.py:127: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  pick_mse[n] = np.sqrt(np.sum(smb_peak[:]-t_before)**2/len(stas))\n",
      "/var/folders/js/lzmy975n0l5bjbmr9db291m00000gn/T/ipykernel_29486/4065219544.py:128: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  pick_mae[n] = np.sum(np.abs(smb_peak[:]-t_before))/len(stas)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "inf inf\n",
      "833748 2002-04-08T17:10:33.000000Z\n",
      "1\n",
      "0.3000000000000007 0.3000000000000007\n",
      "833748 2002-04-08T17:10:33.000000Z\n",
      "1\n",
      "0.3000000000000007 0.3000000000000007\n",
      "833748 2002-04-08T17:10:38.000000Z\n",
      "1\n",
      "15.0 15.0\n",
      "833778 2002-04-08T20:37:48.000000Z\n",
      "1\n",
      "80.55 80.55\n",
      "834263 2002-04-10T17:24:29.000000Z\n",
      "1\n",
      "0.05000000000000071 0.05000000000000071\n",
      "834263 2002-04-10T17:24:29.000000Z\n",
      "1\n",
      "1.9499999999999993 1.9499999999999993\n",
      "834928 2002-04-12T07:20:13.000000Z\n",
      "1\n",
      "0.27500000000000036 0.27500000000000036\n",
      "834928 2002-04-12T07:20:14.000000Z\n",
      "1\n",
      "0.27500000000000036 0.27500000000000036\n",
      "834988 2002-04-12T18:34:53.000000Z\n",
      "1\n",
      "0.025000000000000355 0.025000000000000355\n",
      "834993 2002-04-12T18:49:08.000000Z\n",
      "0\n",
      "834993 2002-04-12T18:49:08.000000Z\n",
      "0\n",
      "834998 2002-04-12T19:05:10.000000Z\n",
      "0\n",
      "835003 2002-04-12T19:15:56.000000Z\n",
      "0\n",
      "835003 2002-04-12T19:15:56.000000Z\n"
     ]
    },
    {
     "ename": "FDSNNoDataException",
     "evalue": "No data available for request.\nHTTP Status code: 204\nDetailed response of server:\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFDSNNoDataException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m bulk\u001b[38;5;241m=\u001b[39m[[net[n], sta[n], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*H*\u001b[39m\u001b[38;5;124m'\u001b[39m, otime\u001b[38;5;241m-\u001b[39mt_before_raw, otime\u001b[38;5;241m+\u001b[39mt_before_raw]]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m st \u001b[38;5;241m=\u001b[39m \u001b[43mclient2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_waveforms_bulk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbulk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m st \u001b[38;5;241m=\u001b[39m resample(st,fs)  \u001b[38;5;66;03m#resampling the data to 40Hz for each trace\u001b[39;00m\n\u001b[1;32m     27\u001b[0m evt_data \u001b[38;5;241m=\u001b[39m obspy\u001b[38;5;241m.\u001b[39mStream()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/seismo_exo/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:1051\u001b[0m, in \u001b[0;36mClient.get_waveforms_bulk\u001b[0;34m(self, bulk, quality, minimumlength, longestonly, filename, attach_response, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m bulk \u001b[38;5;241m=\u001b[39m get_bulk_string(bulk, arguments)\n\u001b[1;32m   1049\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_url(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataselect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1051\u001b[0m data_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext/plain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m data_stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/seismo_exo/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:1486\u001b[0m, in \u001b[0;36mClient._download\u001b[0;34m(self, url, return_string, data, use_gzip, content_type)\u001b[0m\n\u001b[1;32m   1481\u001b[0m     headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m content_type\n\u001b[1;32m   1482\u001b[0m code, data \u001b[38;5;241m=\u001b[39m download_url(\n\u001b[1;32m   1483\u001b[0m     url, opener\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_opener, headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1484\u001b[0m     debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug, return_string\u001b[38;5;241m=\u001b[39mreturn_string, data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1485\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, use_gzip\u001b[38;5;241m=\u001b[39muse_gzip)\n\u001b[0;32m-> 1486\u001b[0m \u001b[43mraise_on_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/seismo_exo/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:1813\u001b[0m, in \u001b[0;36mraise_on_error\u001b[0;34m(code, data)\u001b[0m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;66;03m# No data.\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m204\u001b[39m:\n\u001b[0;32m-> 1813\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FDSNNoDataException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data available for request.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1814\u001b[0m                               server_info)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m   1816\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad request. If you think your request was valid \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1817\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease contact the developers.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFDSNNoDataException\u001b[0m: No data available for request.\nHTTP Status code: 204\nDetailed response of server:\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pick_mse = np.zeros(len(evt_id))\n",
    "pick_mae = np.zeros(len(evt_id))\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "# pdf = PdfPages('../plots/MLPicks_test.pdf')\n",
    "for n in range(len(evt_id)): \n",
    "    if start_time[n]<datetime(2002,1,1):continue   \n",
    "    event_ID = str(evt_id[n])\n",
    "    otime = UTCDateTime(start_time[n])\n",
    "    networks=net[n]\n",
    "    stations=sta[n]\n",
    "    if sta[n]==\"LON\" or sta[n]==\"LO2\":continue\n",
    "    print(event_ID,otime)\n",
    "    # print(net[n],sta[n])\n",
    "    try:\n",
    "        associated_volcano = df[df['Station']== sta[n]]['Volcano_Name'].values[0]\n",
    "    except: \n",
    "        pass\n",
    "    # if associated_volcano!=\"Mt_Rainier\":continue\n",
    "            \n",
    "#################### WAVEFORM DOWNLOAD #######################\n",
    "    #Download all waveforms for that event based on stations and times\n",
    "    bulk=[[net[n], sta[n], '*', '*H*', otime-t_before_raw, otime+t_before_raw]]\n",
    "    try:\n",
    "        st = client2.get_waveforms_bulk(bulk)\n",
    "        st = resample(st,fs)  #resampling the data to 40Hz for each trace\n",
    "        \n",
    "        evt_data = obspy.Stream()\n",
    "\n",
    "        # #Plotting all traces for one event with channel z, SNR>10, and bandpasses between 2-12Hz\n",
    "        SNR,SNR_weight, no_weight,stas,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],[],{},{}\n",
    "        for i,ii in enumerate(st):\n",
    "            network = ii.stats.network\n",
    "            station = ii.stats.station\n",
    "            ii.detrend(type = 'demean')\n",
    "            ii.filter('bandpass',freqmin=low_cut,freqmax=high_cut,corners=2,zerophase=True)\n",
    "            cha = ii.stats.channel\n",
    "            starttime = ii.stats.starttime\n",
    "            signal_window = ii.copy()\n",
    "            noise_window = ii.copy()\n",
    "            # trim the data and noise window to exactly 6000 points\n",
    "            signal_window.trim(otime - t_before, otime - t_before + window) # trim the signal at the first pick time of the PNSN data, with loose 40s before\n",
    "            noise_window.trim(otime - window -t_before, otime - t_before) # noise window of the same length\n",
    "            if not len(signal_window.data) or not len(signal_window.data): continue\n",
    "            # print(len(signal_window),len(noise_window.data))\n",
    "            if not np.max(np.percentile(np.abs(signal_window.data),pr)):continue\n",
    "            if len(noise_window.data)==0:continue\n",
    "            snr = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                            / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "            max_amp_time = np.argmax(noise_window.data)/fs\n",
    "            # print(snr,max_amp_time)\n",
    "            # signal_window.plot()x\n",
    "            if snr<thr: # and 100<max_amp_time<200:\n",
    "                st.remove(ii)\n",
    "                continue\n",
    "            # start the time axis 15 seconds before the pick time of the first reference station\n",
    "            t = signal_window.times()[:-1] #\n",
    "            t_diff[network+'.'+station] = starttime-otime \n",
    "            stas.append(ii.stats.station)\n",
    "            nets.append(ii.stats.network)\n",
    "            SNR.append(snr)\n",
    "            SNR_weight.append(int(snr))\n",
    "            no_weight.append(1)\n",
    "            evt_data.append(signal_window)\n",
    "            \n",
    "        sta_available,ind = np.unique(np.array(stas),return_index=True)\n",
    "        sta_available=sta_available[np.argsort(ind)]\n",
    "        bigS = np.zeros(shape=(len(sta_available),3,6000))\n",
    "        stas=[]\n",
    "        for i in range(len(sta_available)):\n",
    "            stream = evt_data.select(station=sta_available[i])\n",
    "            if len(stream[0].data)<6000:continue\n",
    "            # print(\"original stream\")\n",
    "            # print(stream)\n",
    "            if len(stream)<3:\n",
    "                # copy stream to 2 components, zero the missing data.\n",
    "                tr3 = stream[0].copy() # assumed to be the vertical\n",
    "                tr2 = stream[0].copy();tr2.stats.channel=stream[0].stats.channel[0:2]+\"N\"\n",
    "                tr1 = stream[0].copy();tr1.stats.channel=stream[0].stats.channel[0:2]+\"E\"\n",
    "                tr1.data=np.zeros(len(stream[0].data))\n",
    "                tr2.data=np.zeros(len(stream[0].data))\n",
    "                stream=obspy.Stream(traces=[tr1,tr2,tr3])\n",
    "            # convert Stream into seisbench-friendly array    \n",
    "            # fill in big array and order data ZNE\n",
    "            bigS[i,0,:]=stream[2].data[:-1]\n",
    "            bigS[i,1,:]=stream[1].data[:-1]\n",
    "            bigS[i,2,:]=stream[0].data[:-1]\n",
    "            stas.append(sta_available[i])\n",
    "        # pre process the data\n",
    "        crap2  = bigS.copy()\n",
    "        crap2 -= np.mean(crap2,axis=-1,keepdims=True) # demean data\n",
    "        # original use std norm\n",
    "        data_std = crap2 / np.std(crap2) + 1e-10\n",
    "        # could use max data\n",
    "        mmax= np.max(np.abs(crap2), axis=-1, keepdims=True)\n",
    "        data_max = np.divide(crap2 ,mmax,out=np.zeros_like(crap2),where=mmax!=0)\n",
    "\n",
    "    #         # test the new function\n",
    "        smb_peak= apply_elep(evt_data, sta_available, \\\n",
    "                list_models, MBF_paras, paras_semblance, t_before)\n",
    "        print( max(smb_peak.shape))\n",
    "        if not max(smb_peak.shape):continue\n",
    "            ## plot figure\n",
    "\n",
    "            # fig = plt.figure()#figsize = (11,8), dpi=200)\n",
    "            # fig.suptitle(str(otime)+\" \"+associated_volcano)\n",
    "            # ax = plt.subplot(1,1,1)\n",
    "            # iplot = 0\n",
    "            # for i in range(len(stas)):\n",
    "            #     ax.plot(t-15,data_max[i,0,:]+iplot*1.5,linewidth=0.5)\n",
    "            #     ax.plot(t-15,smb_pred[ i, :]/np.max(np.abs(smb_pred[ i, :]))+iplot*1.5,'k',linewidth=0.5)\n",
    "            #     ax.set_yticks([])\n",
    "            #     plt.text(-15, iplot*1.5+0.5, stas[i])\n",
    "            #     # if i==ista:\n",
    "            #     err_title=(\"%s %2.2f (s) error in picks\"%(stas[i],smb_peak[i]-t_before))\n",
    "            #     plt.text(60, iplot*1.5+0.5,err_title,color='r')\n",
    "            #     plt.vlines(smb_peak[i]-t_before,iplot*1.5-1.,iplot*1.5+1.,'r')\n",
    "            #     print(stas[i],smb_peak[i]-t_before)\n",
    "            #     iplot+=1\n",
    "            # # plt.grid(True)\n",
    "            # ax.set_xlim([-t_before,80])\n",
    "            # ax.set_xlabel('time (seconds) relative to PNSN picks')\n",
    "            # plt.show()\n",
    "            # pdf.savefig(fig)\n",
    "            # plt.clf()\n",
    "            # del fig\n",
    "\n",
    "        pick_mse[n] = np.sqrt(np.sum(smb_peak[:]-t_before)**2/len(stas))\n",
    "        pick_mae[n] = np.sum(np.abs(smb_peak[:]-t_before))/len(stas)\n",
    "        print(pick_mse[n],pick_mae[n] )\n",
    "        if n > 10000: break\n",
    "    except:\n",
    "        pass\n",
    "# pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noise_window.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ik=np.where(np.abs(pick_error)>0)\n",
    "plt.hist(pick_error[ik],100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n)\n",
    "print(pick_error.shape)\n",
    "print(len(evt_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pick_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-band Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqmin = 0.1\n",
    "fqmax = 20\n",
    "dt = 0.025; fs = 40;\n",
    "nfqs = 10\n",
    "nt = 6000; nc = 3;\n",
    "fq_list = make_LogFq(fqmin, fqmax, dt, nfqs)\n",
    "coeff_HP, coeff_LP = rec_filter_coeff(fq_list, dt)\n",
    "MBF_paras = {'f_min':fqmin, 'f_max':fqmax, 'nfqs':nfqs, 'frequencies':fq_list, 'CN_HP':coeff_HP, 'CN_LP':coeff_LP, \\\n",
    "    'dt':dt, 'fs':fs, 'nt':nt, 'nc':nc, 'npoles': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bandpass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_error = np.zeros(len(evt_id))\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "# pdf = PdfPages('../plots/MLPicks_test.pdf')\n",
    "for n in range(len(evt_id)): \n",
    "    if start_time[n]<datetime(2002,1,1):continue   \n",
    "    event_ID = str(evt_id[n])\n",
    "    otime = UTCDateTime(start_time[n])\n",
    "    networks=net[n]\n",
    "    stations=sta[n]\n",
    "    if sta[n]==\"LON\" or sta[n]==\"LO2\":continue\n",
    "    print(event_ID,otime)\n",
    "    # print(net[n],sta[n])\n",
    "    try:\n",
    "        associated_volcano = df[df['Station']== sta[n]]['Volcano_Name'].values[0]\n",
    "    except: \n",
    "        pass\n",
    "    print(associated_volcano)\n",
    "    # if associated_volcano!=\"Mt_Rainier\":continue\n",
    "            \n",
    "#################### WAVEFORM DOWNLOAD #######################\n",
    "    #Download all waveforms for that event based on stations and times\n",
    "    bulk=[[net[n], sta[n], '*', '*H*', otime-t_before_raw, otime+t_before_raw]]\n",
    "    print(bulk)\n",
    "    try:\n",
    "        st = client2.get_waveforms_bulk(bulk)\n",
    "        st = resample(st,fs)  #resampling the data to 40Hz for each trace\n",
    "        \n",
    "        evt_data = obspy.Stream()\n",
    "\n",
    "        # #Plotting all traces for one event with channel z, SNR>10, and bandpasses between 2-12Hz\n",
    "        SNR,SNR_weight, no_weight,stas,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],[],{},{}\n",
    "        for i,ii in enumerate(st):\n",
    "            network = ii.stats.network\n",
    "            station = ii.stats.station\n",
    "            ii.detrend(type = 'demean')\n",
    "            ii.filter('bandpass',freqmin=low_cut,freqmax=high_cut,corners=2,zerophase=True)\n",
    "            cha = ii.stats.channel\n",
    "            starttime = ii.stats.starttime\n",
    "            signal_window = ii.copy()\n",
    "            noise_window = ii.copy()\n",
    "            # trim the data and noise window to exactly 6000 points\n",
    "            signal_window.trim(otime - t_before, otime - t_before + window) # trim the signal at the first pick time of the PNSN data, with loose 40s before\n",
    "            noise_window.trim(otime - window -t_before, otime - t_before) # noise window of the same length\n",
    "            if not len(signal_window.data) or not len(signal_window.data): continue\n",
    "            # print(len(signal_window),len(noise_window.data))\n",
    "            if not np.max(np.percentile(np.abs(signal_window.data),pr)):continue\n",
    "            if len(noise_window.data)==0:continue\n",
    "            snr = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                            / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "            max_amp_time = np.argmax(noise_window.data)/fs\n",
    "            # print(snr,max_amp_time)\n",
    "            # signal_window.plot()x\n",
    "            if snr<thr: # and 100<max_amp_time<200:\n",
    "                st.remove(ii)\n",
    "                continue\n",
    "            # start the time axis 15 seconds before the pick time of the first reference station\n",
    "            t = signal_window.times()[:-1] #\n",
    "            t_diff[network+'.'+station] = starttime-otime \n",
    "            stas.append(ii.stats.station)\n",
    "            nets.append(ii.stats.network)\n",
    "            SNR.append(snr)\n",
    "            SNR_weight.append(int(snr))\n",
    "            no_weight.append(1)\n",
    "            evt_data.append(signal_window)\n",
    "            \n",
    "        sta_available,ind = np.unique(np.array(stas),return_index=True)\n",
    "        sta_available=sta_available[np.argsort(ind)]\n",
    "        print(sta_available)\n",
    "        bigS = np.zeros(shape=(len(sta_available),3,6000))\n",
    "        stas=[]\n",
    "        for i in range(len(sta_available)):\n",
    "            stream = evt_data.select(station=sta_available[i])\n",
    "            if len(stream[0].data)<6000:continue\n",
    "            # print(\"original stream\")\n",
    "            # print(stream)\n",
    "            if len(stream)<3:\n",
    "                # copy stream to 2 components, zero the missing data.\n",
    "                tr3 = stream[0].copy() # assumed to be the vertical\n",
    "                tr2 = stream[0].copy();tr2.stats.channel=stream[0].stats.channel[0:2]+\"N\"\n",
    "                tr1 = stream[0].copy();tr1.stats.channel=stream[0].stats.channel[0:2]+\"E\"\n",
    "                tr1.data=np.zeros(len(stream[0].data))\n",
    "                tr2.data=np.zeros(len(stream[0].data))\n",
    "                stream=obspy.Stream(traces=[tr1,tr2,tr3])\n",
    "            # convert Stream into seisbench-friendly array    \n",
    "            # fill in big array and order data ZNE\n",
    "            bigS[i,0,:]=stream[2].data[:-1]\n",
    "            bigS[i,1,:]=stream[1].data[:-1]\n",
    "            bigS[i,2,:]=stream[0].data[:-1]\n",
    "            stas.append(sta_available[i])\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"put data in numpy array\")\n",
    "\n",
    "        nseg = len(sta_available)\n",
    "\n",
    "        windows_std = np.zeros(shape=(nseg, nfqs, 3, twin), dtype= np.float32)\n",
    "        windows_max = np.zeros(shape=( nseg, nfqs, 3, twin), dtype= np.float32)\n",
    "        _windows = np.zeros(shape=( 3, twin), dtype= np.float32)\n",
    "        _windows_mb = np.zeros(shape=(nseg, 3, nfqs, twin), dtype= np.float32)\n",
    "        windows_idx = np.zeros(nseg, dtype=np.int32)\n",
    "        # pre process the data\n",
    "        crap2  = bigS.copy()\n",
    "        crap2 -= np.mean(crap2,axis=-1,keepdims=True) # demean data\n",
    "        # original use std norm\n",
    "        data_std = crap2 / np.std(crap2) + 1e-10\n",
    "        # could use max data\n",
    "        mmax= np.max(np.abs(crap2), axis=-1, keepdims=True)\n",
    "        data_max = np.divide(crap2 ,mmax,out=np.zeros_like(crap2),where=mmax!=0)\n",
    "\n",
    "        # print(f\"Window data shape: {data_std.shape}\")\n",
    "\n",
    "        # evaluate\n",
    "        pn_pnw_model.eval()\n",
    "        pn_ethz_model.eval()\n",
    "        pn_scedc_model.eval()\n",
    "        pn_neic_model.eval()\n",
    "        pn_geofon_model.eval()\n",
    "        pn_stead_model.eval()\n",
    "        pn_instance_model.eval()\n",
    "        # convert numpy array to torch tensor\n",
    "        data_tt = torch.Tensor(data_std)\n",
    "        # batch predict picks.\n",
    "        _torch_pred_1 = pn_pnw_model(data_tt.to(device))\n",
    "        _torch_pred_2 = pn_ethz_model(data_tt.to(device))\n",
    "        _torch_pred_3 = pn_scedc_model(data_tt.to(device))\n",
    "        _torch_pred_4 = pn_neic_model(data_tt.to(device))\n",
    "        _torch_pred_5 = pn_geofon_model(data_tt.to(device))\n",
    "        _torch_pred_6 = pn_stead_model(data_tt.to(device))\n",
    "        _torch_pred_7 = pn_instance_model(data_tt.to(device))\n",
    "        # extract P pdf\n",
    "        batch_pred =np.zeros(shape=(7,data_std.shape[0],6000))\n",
    "        batch_pred[0,:, :] = _torch_pred_1[1].detach().cpu().numpy()[:, :]\n",
    "        batch_pred[1,:, :] = _torch_pred_2[1].detach().cpu().numpy()[:, :]\n",
    "        batch_pred[2,:, :] = _torch_pred_3[1].detach().cpu().numpy()[:, :]\n",
    "        batch_pred[3,:, :] = _torch_pred_4[1].detach().cpu().numpy()[:, :]\n",
    "        batch_pred[4,:, :] = _torch_pred_5[1].detach().cpu().numpy()[:, :]\n",
    "        batch_pred[5,:, :] = _torch_pred_6[1].detach().cpu().numpy()[:, :]\n",
    "        batch_pred[6,:, :] = _torch_pred_7[1].detach().cpu().numpy()[:, :]\n",
    "                    \n",
    "        # ensemble semblance\n",
    "        nwin,twin=batch_pred.shape[1],batch_pred.shape[-1]\n",
    "        print(nwin)\n",
    "        if nwin==0:continue\n",
    "        smb_pred = np.zeros([ nwin, twin], dtype = np.float32)\n",
    "        smb_peak = np.zeros([ nwin], dtype = np.float32)\n",
    "        for iwin in range(len(stas)):\n",
    "            # 0 for P-wave\n",
    "            smb_pred[ iwin, :] = ensemble_semblance(batch_pred[:, iwin, :], paras_semblance)\n",
    "            imax = np.argmax(smb_pred[ iwin, :80*40]) # search for peak in the first 80 seconds\n",
    "            if smb_pred[ iwin,imax]>0:\n",
    "                smb_peak[iwin]=float((imax)/40)\n",
    "\n",
    "\n",
    "for iseg in range(nseg):\n",
    "    for icha in range(3):\n",
    "        _windows_mb[iseg, icha, :, :] = MB_filter(_windows[iseg, icha], MBF_paras)\n",
    "_windows_mb = _windows_mb.swapaxes(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iseg in tqdm(range(nseg)):\n",
    "    for ifre in range(nfqs):\n",
    "        # original use std norm\n",
    "        windows_std[iseg, ifre, :] = _windows_mb[iseg, ifre, :] / np.std(_windows_mb[iseg, ifre, :]) + 1e-10\n",
    "        # others use max norm\n",
    "        windows_max[iseg, ifre, :] = _windows_mb[iseg, ifre, :] / np.max(np.abs(_windows_mb[iseg, ifre, :]), axis=-1, keepdims=True) + 1e-10\n",
    "        windows_idx[iseg] = idx\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo_exo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
