{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering of RedPy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will use RedPy feature data and cluster it to try and find patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Read TSFEL features for Mt Hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is some underway data collected from a cruise in 2019\n",
    "mt_hood = pd.read_csv('../data/Hood_tsfel_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mt_hood.copy()\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(axis=1, inplace=True)\n",
    "df.drop(['template'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().any()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are some of the features log-normal? Log-normal features will be wrongly scaled for a euclidian distance.\n",
    "\n",
    "We will select the features of high skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Calculate skewness for each feature\n",
    "skewness = df.apply(lambda x: x.skew())\n",
    "\n",
    "# Step 2: Identify features with high positive skewness (right-skewed)\n",
    "log_normal_features = skewness[skewness > 1.0].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(log_normal_features))\n",
    "print(len(df.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features are log normal. OK transform all log-normal features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df.copy()\n",
    "df_log[log_normal_features] = np.log(df_log[log_normal_features]) # log transform the skewed features\n",
    "\n",
    "# drop the features with Nan, Inf, Zeros from the data frames.\n",
    "df_log.dropna(inplace=True,axis=1)\n",
    "df_log.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_log.dropna(inplace=True,axis=1)\n",
    "df_log.replace(0, np.nan)\n",
    "df_log.dropna(inplace=True,axis=1)\n",
    "df_log.isna().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we notice that the features have extremely different values. It seems difficult to continue with normal kmeans because the Euclidian distance will be inadequate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some standard scaling first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = preprocessing.StandardScaler().fit(X_pca)\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform the DataFrame using StandardScaler\n",
    "df_log_scaled = pd.DataFrame(scaler.fit_transform(df_log), columns=df_log.columns)\n",
    "X_scaled = df_log_scaled.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on original data with silhouette score\n",
    "X = df.to_numpy()\n",
    "ncluster=4\n",
    "kmeans_model = KMeans(n_clusters=ncluster, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "sc=silhouette_score(X, labels, metric='euclidean')\n",
    "print(f\"Silhouette score for {ncluster} clusters: {sc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example on log-transformed data with silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_log.to_numpy()\n",
    "ncluster=4\n",
    "kmeans_model = KMeans(n_clusters=ncluster, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "sc=silhouette_score(X, labels, metric='euclidean')\n",
    "print(f\"Silhouette score for {ncluster} clusters: {sc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncluster=4\n",
    "import matplotlib.cm as cm\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "fig.set_size_inches(7, 7)\n",
    "ax1.set_xlim([-0.1, 1])\n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "# plots of individual clusters, to demarcate them clearly.\n",
    "ax1.set_ylim([0, len(X) + (ncluster + 1) * 10])\n",
    "\n",
    "# Initialize the clusterer with n_clusters value and a random generator\n",
    "# seed of 10 for reproducibility.\n",
    "clusterer = KMeans(n_clusters=ncluster, random_state=10)\n",
    "cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "# The silhouette_score gives the average value for all the samples.\n",
    "# This gives a perspective into the density and separation of the formed\n",
    "# clusters\n",
    "silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "print(f\"For n_clusters = {ncluster}, the average silhouette_score is : {silhouette_avg:.3f}\")\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(ncluster):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / ncluster)\n",
    "    ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "plt.suptitle(\n",
    "    \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "    % ncluster,\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_hood['clusterID'] = clusterID\n",
    "mt_hood.to_csv('../data/Hood_tsfel_features_clustered_kmeans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Choice of number of clusters: The Elbow Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the value of E for different values of the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_log_scaled.to_numpy()\n",
    "# Elbow method  \n",
    "inertia = []\n",
    "silhouette_avg=[]\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    if k>=2:\n",
    "        silhouette_avg.append(silhouette_score(X, cluster_labels))\n",
    "        print(f\"For n_clusters = {k}, the average silhouette_score is : {silhouette_avg[-1]:.3f}\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(range(1, 11), inertia, marker='o')\n",
    "ax[0].set_title('Elbow Curve for KMeans - log-scaled data')\n",
    "ax[0].set_xlabel('Number of Clusters (k)')\n",
    "ax[0].set_ylabel('Inertia')\n",
    "ax[1].plot(range(2, 11), silhouette_avg, marker='o')\n",
    "ax[1].set_title('Silhouette Curve for KMeans - log-scaled data')\n",
    "ax[1].set_xlabel('Number of Clusters (k)')\n",
    "ax[1].set_ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_log.to_numpy()\n",
    "# Elbow method  \n",
    "inertia = []\n",
    "silhouette_avg=[]\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    if k>=2:\n",
    "        silhouette_avg.append(silhouette_score(X, cluster_labels))\n",
    "        print(f\"For n_clusters = {k}, the average silhouette_score is : {silhouette_avg[-1]:.3f}\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(range(1, 11), inertia, marker='o')\n",
    "ax[0].set_title('Elbow Curve for KMeans - log data')\n",
    "ax[0].set_xlabel('Number of Clusters (k)')\n",
    "ax[0].set_ylabel('Inertia')\n",
    "ax[1].plot(range(2, 11), silhouette_avg, marker='o')\n",
    "ax[1].set_title('Silhouette Curve for KMeans - log data')\n",
    "ax[1].set_xlabel('Number of Clusters (k)')\n",
    "ax[1].set_ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.to_numpy()\n",
    "# Elbow method  \n",
    "inertia = []\n",
    "silhouette_avg=[]\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    if k>=2:\n",
    "        silhouette_avg.append(silhouette_score(X, cluster_labels))\n",
    "        print(f\"For n_clusters = {k}, the average silhouette_score is : {silhouette_avg[-1]:.3f}\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(range(1, 11), inertia, marker='o')\n",
    "ax[0].set_title('Elbow Curve for KMeans - raw data')\n",
    "ax[0].set_xlabel('Number of Clusters (k)')\n",
    "ax[0].set_ylabel('Inertia')\n",
    "ax[1].plot(range(2, 11), silhouette_avg, marker='o')\n",
    "ax[1].set_title('Silhouette Curve for KMeans - raw data')\n",
    "ax[1].set_xlabel('Number of Clusters (k)')\n",
    "ax[1].set_ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Clustering\n",
    "\n",
    "In K-means, we use the euclidian distance and prescribe the number of clusters K.\n",
    "\n",
    "In hierarchical clustering, we choose difference distance metrics, visualize the data structure, and then decide on the number of clusters. There are two approaches to building the hierarchy of clusters:\n",
    "\n",
    "* **Agglomerative**: each point starts in each unique cluster. data is merged in pairs as on creates a hierarchy of clusters.\n",
    "* **Divisive**: initially, all data is into 1 cluster. The data is recursively split into smaller and smaller clusters.\n",
    "\n",
    "\n",
    "There are several types of *linkages*. sklearn has detailed [documentation](!https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering), mostly for agglomerative: The different linkages methods are:\n",
    "\n",
    "* **Ward** minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "* **Maximum** or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n",
    "* **Average** linkage minimizes the average of the distances between all observations of pairs of clusters.\n",
    "* **Single** linkage minimizes the distance between the closest observations of pairs of clusters.\n",
    "\n",
    "We first import relevant packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from scipy.cluster import hierarchy  #\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "rcParams.update({'font.size': 18})\n",
    "plt.rcParams['figure.figsize'] = [12, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we explore the dendograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dendrograms on raw data\n",
    "X = df.to_numpy()\n",
    "Y = pdist(X,metric='euclidean')\n",
    "Z = hierarchy.linkage(Y,method='ward')\n",
    "thresh = 0.85*np.max(Z[:,2])\n",
    "\n",
    "plt.figure()\n",
    "dn = hierarchy.dendrogram(Z,p=100,color_threshold=thresh)\n",
    "plt.xlabel('Data Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Dendrogram with Ward linkage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dendrograms on log data\n",
    "X = df_log.to_numpy()\n",
    "Y = pdist(X,metric='euclidean')\n",
    "Z = hierarchy.linkage(Y,method='average')\n",
    "thresh = 0.85*np.max(Z[:,2])\n",
    "\n",
    "plt.figure()\n",
    "dn = hierarchy.dendrogram(Z,p=100,color_threshold=thresh)\n",
    "plt.xlabel('Data Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Dendrogram with average linkage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cluster:\n",
    "\n",
    "* Ward linkage on raw feature\n",
    "* ward linkage on log features\n",
    "* ward linkage on log and scaled features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "X=df.to_numpy()\n",
    "# Let's first find a reasonable distance threshod by precalculating the linkage matrix\n",
    "Z = hierarchy.linkage(X,method='ward')\n",
    "thresh = 0.4*np.max(Z[:,2])    # choose a threshold distance\n",
    "# design model\n",
    "model = AgglomerativeClustering(distance_threshold=thresh,linkage=\"ward\", n_clusters=None)\n",
    "# fit model and predict clusters on the data samples\n",
    "clusterID=model.fit_predict(X)\n",
    "ncluster=len(np.unique(clusterID))\n",
    "silhouette_avg = silhouette_score(X, clusterID)\n",
    "print(f\"For n_clusters = {ncluster}, the average silhouette_score is : {silhouette_avg:.3f}\")\n",
    "\n",
    "\n",
    "mt_hood['clusterID'] = clusterID\n",
    "mt_hood.to_csv('../data/Hood_tsfel_features_clustered_agg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ward on log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "X=df_log.to_numpy()\n",
    "# Let's first find a reasonable distance threshod by precalculating the linkage matrix\n",
    "Z = hierarchy.linkage(X,method='ward')\n",
    "thresh = 0.4*np.max(Z[:,2])    # choose a threshold distance\n",
    "# design model\n",
    "model = AgglomerativeClustering(distance_threshold=thresh,linkage=\"ward\", n_clusters=None)\n",
    "# fit model and predict clusters on the data samples\n",
    "clusterID=model.fit_predict(X)\n",
    "ncluster=len(np.unique(clusterID))\n",
    "silhouette_avg = silhouette_score(X, clusterID)\n",
    "print(f\"For n_clusters = {ncluster}, the average silhouette_score is : {silhouette_avg:.3f}\")\n",
    "\n",
    "mt_hood['clusterID'] = clusterID\n",
    "mt_hood.to_csv('../data/Hood_tsfel_features_clustered_agg_log.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ward on log-transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "X=df_log_scaled.to_numpy()\n",
    "# Let's first find a reasonable distance threshod by precalculating the linkage matrix\n",
    "Z = hierarchy.linkage(X,method='ward')\n",
    "thresh = 0.4*np.max(Z[:,2])    # choose a threshold distance\n",
    "# design model\n",
    "model = AgglomerativeClustering(distance_threshold=thresh,linkage=\"ward\", n_clusters=None)\n",
    "# fit model and predict clusters on the data samples\n",
    "clusterID=model.fit_predict(X)\n",
    "ncluster=len(np.unique(clusterID))\n",
    "silhouette_avg = silhouette_score(X, clusterID)\n",
    "print(f\"For n_clusters = {ncluster}, the average silhouette_score is : {silhouette_avg:.3f}\")\n",
    "\n",
    "mt_hood['clusterID'] = clusterID\n",
    "mt_hood.to_csv('../data/Hood_tsfel_features_clustered_agg_log_scaled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try PCA+normalization before clustering\n",
    "\n",
    "What happens if we apply PCA + normalization before the clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.to_numpy()   \n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_pca)\n",
    "X_scaled = scaler.transform(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first find a reasonable distance threshod by precalculating the linkage matrix\n",
    "Z = hierarchy.linkage(X_scaled,method='ward')\n",
    "thresh = 0.3*np.max(Z[:,2])    # choose a threshold distance\n",
    "\n",
    "plt.figure()\n",
    "dn = hierarchy.dendrogram(Z,p=100,color_threshold=thresh)\n",
    "plt.xlabel('Data Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Dendrogram with average linkage')\n",
    "plt.show()\n",
    "# design model\n",
    "model = AgglomerativeClustering(distance_threshold=thresh,linkage=\"ward\", n_clusters=None)\n",
    "# fit model and predict clusters on the data samples\n",
    "clusterID=model.fit_predict(X_scaled)\n",
    "plt.hist(clusterID);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncluster=len(np.unique(clusterID))\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "fig.set_size_inches(18, 7)\n",
    "ax1.set_xlim([-0.1, 1])\n",
    "ax1.set_ylim([0, len(X) + (ncluster + 1) * 10])\n",
    "\n",
    "silhouette_avg = silhouette_score(X_scaled, clusterID)\n",
    "print(\n",
    "    \"For n_clusters =\",\n",
    "    ncluster,\n",
    "    \"The average silhouette_score is :\",\n",
    "    silhouette_avg,\n",
    ")\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(X_scaled, clusterID)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(ncluster):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[clusterID == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / ncluster)\n",
    "    ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "    % ncluster,\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf1c1ab31e530e60b58e3d6ad0457a0c579c03efa8f6c28b6cdd125835b5a825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
