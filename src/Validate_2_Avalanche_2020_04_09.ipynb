{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0f6cb81",
   "metadata": {},
   "source": [
    "# Surface Event Pick Time\n",
    "\n",
    "This is a modified version of the surface-event location+directivity analysis that Francesca Skene (fskene@uw.edu), originally created by her in 7/22/22, who started as an undergraduate student at UW. This is marine denolle's version. It includes:\n",
    "* Waveform download for each event on each volcano given the PNSN pick times of \"su\" events.\n",
    "* Data pre-processing to trim the data within 2-12 Hz and remove outliers.\n",
    "* phase picking using transfer-learned model (Ni et al, 2023)\n",
    "* event location using 1D grid search\n",
    "* directivity measurements (velocity and direction) using Doppler effects.\n",
    "* gathering of the data into a CSV data frame.\n",
    "\n",
    "Here we will try to locate the 04/09/2020 avalanche on Carbon glacier headwall for its 4th year anniversary!\n",
    "\n",
    "Updated 04/9/2024\n",
    "Marine Denolle\n",
    "(mdenolle@uw.edu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7021dc6f",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf759fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/data/wsd01/pnwstore/')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from mbf_elep_func import *\n",
    "import torch\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "\n",
    "import seisbench.models as sbm\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "# from ELEP.elep.ensemble_learners import ensemble_regressor_cnn\n",
    "from ELEP.elep import mbf, mbf_utils\n",
    "from ELEP.elep import trigger_func\n",
    "\n",
    "from ELEP.elep.mbf_utils import make_LogFq, make_LinFq, rec_filter_coeff, create_obspy_trace\n",
    "from ELEP.elep.mbf import MB_filter as MBF\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c976b0f",
   "metadata": {},
   "source": [
    "What avalanche are we studying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ca57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ava = 'Avalanche_04092020'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd30de3d",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clients to download the station data\n",
    "# client = WaveformClient() # we ignore PNWdatastore for now\n",
    "client2 = Client('IRIS') # IRIS client\n",
    "\n",
    "t_before = 15 #number of seconds before pick time\n",
    "# t_after = 15 #number of seconds after pick time\n",
    "t_before_raw = 1200 #number of seconds before pick time before removing instrumental response\n",
    "# t_after_raw = 1200 #number of seconds after pick time before removing instrumental response\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 150 #window length of the signal (this will help with phase picking with EqT next). \n",
    "# Use 150 seconds @ 40 Hz gives 6001 points. \n",
    "pr = 98 #percentile\n",
    "thr = 7 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "vs = 3000 #shear wave velocity at the surface\n",
    "\n",
    "# range of dates that we are looking at\n",
    "t_beginning = UTCDateTime(2001,1,1,0,0,0) \n",
    "t_end = UTCDateTime(2024,1,1,23,59)\n",
    "\n",
    "smooth_length = 20 # constant for smoothing the waveform envelopes\n",
    "low_cut = 1 #low frequency threshold\n",
    "high_cut = 15 #high frequency threshold\n",
    "az_thr = 1000 #threshold of distance in meters from source location\n",
    "step = 100 #step every 100 m\n",
    "t_step = 1 #step every second\n",
    "ratio = 5.6915196 #used to define the grid \n",
    "# colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3 # radius of the earth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4db39e",
   "metadata": {},
   "source": [
    "## Volcano - Station Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ede0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('../data/station/Volcano_Metadata_50km.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b021322",
   "metadata": {},
   "source": [
    "## PNSN SU Pick information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = pd.read_csv(\"../data/events/su_picks.txt\",sep=\"|\")\n",
    "# f1.head()\n",
    "# print(f1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean up the spaces in the file\n",
    "# format='%Y/%m/%d %H:%M:%S'\n",
    "# test=f1[\"date\"].values.tolist()\n",
    "# start_time_temp = [  datetime.strptime(x.strip(),'%Y/%m/%d %H:%M:%S') for x in f1[\"date\"].values.tolist()]\n",
    "# # # Ignore events prior to t_beginning\n",
    "# ik=np.where(np.array(start_time_temp)>datetime(2001,1,1))[0][0]\n",
    "\n",
    "# # select only net, sta, evid, startime for event past the start date.\n",
    "\n",
    "# start_time = start_time_temp[ik:]\n",
    "# net=[ x.strip() for x in f1[\"net\"].values.tolist()][ik:]\n",
    "# sta=[ x.strip() for x in f1[\"sta\"].values.tolist()][ik:]\n",
    "# evt_id=[ x for x in f1[\"orid\"].values.tolist()][ik:]\n",
    "# all_stas=set(sta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20c89405",
   "metadata": {},
   "source": [
    "## ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.makedirs(\"/Users/marinedenolle/.seisbench/models/v3/eqtransformer\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/congcy/ELEP/raw/main/docs/tutorials/data/pnw.pt.v1 -O ~/.seisbench/models/v3/eqtransformer/pnw.pt.v1\n",
    "# !wget https://github.com/congcy/ELEP/raw/main/docs/tutorials/data/pnw.json.v1 -O ~/.seisbench/models/v3/eqtransformer/pnw.json.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download models\n",
    "list_models_name = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "# pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "\n",
    "list_models = [pn_pnw_model, pn_ethz_model, pn_instance_model, pn_scedc_model, pn_stead_model, pn_geofon_model]\n",
    "\n",
    "pn_pnw_model.to(device);\n",
    "pn_ethz_model.to(device);\n",
    "pn_scedc_model.to(device);\n",
    "# pn_neic_model.to(device);\n",
    "pn_geofon_model.to(device);\n",
    "pn_stead_model.to(device);\n",
    "pn_instance_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_semblance = {'dt':0.025, 'semblance_order':4, 'window_flag':True, \n",
    "                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "p_thrd, s_thrd = 0.01, 0.05\n",
    "\n",
    "fqmin = low_cut\n",
    "fqmax = high_cut\n",
    "dt = 0.025; fs = 40\n",
    "nfqs = 10\n",
    "nt = 6000; nc = 3\n",
    "fq_list = make_LogFq(fqmin, fqmax, dt, nfqs)\n",
    "coeff_HP, coeff_LP = rec_filter_coeff(fq_list, dt)\n",
    "MBF_paras = {'f_min':fqmin, 'f_max':fqmax, 'nfqs':nfqs, 'frequencies':fq_list, 'CN_HP':coeff_HP, 'CN_LP':coeff_LP, \\\n",
    "    'dt':dt, 'fs':fs, 'nt':nt, 'nc':nc, 'npoles': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605be24",
   "metadata": {},
   "source": [
    "# Full stack:\n",
    "\n",
    "* download waveforms\n",
    "* phase pick onset\n",
    "* estimate SNR\n",
    "* measure centroid, max envelope, duration\n",
    "* measure Fmax for doppler analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075da112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf = PdfPages('../plots/'+ava+'.pdf')\n",
    "associated_volcano = \"Mt_Rainier\"\n",
    "dff=[] \n",
    "event_ID = '0000' #str(evt_id[n])\n",
    "otime = UTCDateTime(2020,4,9,13,28,30)\n",
    "associated_volcano=\"Mt_Rainier\"\n",
    "\n",
    "\n",
    "#get info for stations within 50km of volcano that event ocurred at\n",
    "stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "\n",
    "#################### WAVEFORM DOWNLOAD #######################\n",
    "#Download all waveforms for that event based on stations and times\n",
    "bulk = [] \n",
    "for m in range(0, len(networks)):\n",
    "    if stations[m]==\"PCEP\" or stations[m]==\"LON\" or stations[m]==\"LO2\":continue\n",
    "    bulk.append([networks[m], stations[m], '*', '*Z', otime-t_before_raw, otime+t_before_raw])\n",
    "# try:\n",
    "st = client2.get_waveforms_bulk(bulk)\n",
    "st = resample(st,fs)  #resampling the data to 40Hz for each trace\n",
    "evt_data = obspy.Stream()\n",
    "snr=[]\n",
    "stas=[]\n",
    "nets=[]\n",
    "lats=[]\n",
    "lons=[]\n",
    "els=[]\n",
    "centroid_time = []\n",
    "data_env_dict = {}\n",
    "duration = []\n",
    "max_time = []\n",
    "\n",
    "# #Keeping all traces for one event with channel z, SNR>10, and bandpassed between 2-12Hz\n",
    "# ,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],[],{},{}\n",
    "for i,ii in enumerate(st):\n",
    "    ii.detrend(type = 'demean')\n",
    "    ii.filter('bandpass',freqmin=low_cut,freqmax=high_cut,corners=2,zerophase=True)\n",
    "    # trim the data and noise window to exactly 6000 points\n",
    "    signal_window = ii.copy()\n",
    "    noise_window = ii.copy()\n",
    "    signal_window.trim(otime - t_before, otime - t_before + window) # trim the signal at the first pick time of the PNSN data, with loose 40s before\n",
    "    noise_window.trim(otime - window -t_before, otime - t_before) # noise window of the same length\n",
    "    if  len(signal_window.data)<=10 or  len(noise_window.data)<=10: continue # skip if no data\n",
    "    # if not np.percentile(np.abs(signal_window.data),pr):continue # skip if max amplitude is zero\n",
    "    snr1 = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                    / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "    print(\"snr\",snr1)\n",
    "    if snr1<thr: # and 100<max_amp_time<200:\n",
    "        st.remove(ii)\n",
    "        continue\n",
    "\n",
    "################# ENVELOPE, CENTROID, DURATION #######################\n",
    "    # enveloping the data \n",
    "    data_envelope = obspy.signal.filter.envelope(signal_window.data)\n",
    "    data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "\n",
    "    data_env_dict[ii.stats.network+'.'+ii.stats.station]= data_envelope/max(np.abs(data_envelope))\n",
    "\n",
    "\n",
    "    # max time\n",
    "    # finding the time of max amplitude of each event\n",
    "    # signal_window is windowed at otime-t_v before the PNSN pick time\n",
    "    crap = np.argmax(np.abs(data_envelope[:(t_before+40)*fs])) # time of max amplitude relative to otime\n",
    "    max_time.append(crap/fs)\n",
    "\n",
    "    # centroid time\n",
    "    tcrap = signal_window.times()\n",
    "    it = np.where(tcrap>0)[0]\n",
    "    centroid_time.append(np.sum(data_envelope*tcrap)/np.sum(data_envelope))\n",
    "    print(ii.stats.station,max_time[-1],centroid_time[-1])\n",
    "\n",
    "    # find duration as data starting with the \"origin time\" and ending when the envelope falls below the mean noise\n",
    "    noise_envelope = obspy.signal.filter.envelope(noise_window.data)\n",
    "    data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "    mean_noise = np.mean(noise_envelope)\n",
    "    \n",
    "    mmax = np.max(np.cumsum(data_envelope**4))\n",
    "    crap = np.where( np.cumsum(data_envelope**4) <= 0.999*mmax)[0][-1]\n",
    "    duration.append(crap/fs)\n",
    "\n",
    "\n",
    "    stas.append(ii.stats.station)\n",
    "    nets.append(ii.stats.network)\n",
    "    ista=stations.index(ii.stats.station)\n",
    "    lats.append(latitudes[ista])\n",
    "    lons.append(longitudes[ista])\n",
    "    els.append(elevations[ista])\n",
    "    snr.append(snr1)\n",
    "    evt_data.append(signal_window)\n",
    "\n",
    "    t = evt_data.select(station=stas[-1])[0].times()\n",
    "    \n",
    "\n",
    "centroid_time = np.asarray(centroid_time)\n",
    "centroid_time -= t_before\n",
    "max_time = np.asarray(max_time)\n",
    "max_time -= t_before\n",
    "duration = np.asarray(duration)\n",
    "duration -= t_before\n",
    "\n",
    "################### ELEP #######################\n",
    "\n",
    "    # test the new function\n",
    "smb_peak= apply_elep(evt_data, stas, \\\n",
    "        list_models, MBF_paras, paras_semblance, t_before)\n",
    "smb_peak -= t_before\n",
    "\n",
    "\n",
    "############## PEAK FREQUENCY MEASUREMENTS ############\n",
    "# Given the approximate measurement of duration, window the signal windows around that\n",
    "# then measure peak frequency so that there is less noise in it.\n",
    "# perform this on the Z component only.\n",
    "\n",
    "char_freq, sharp_weight= [],[]\n",
    "fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "for ii,i in enumerate(evt_data):\n",
    "    data = np.zeros(200*fs)\n",
    "    crap=i.copy()\n",
    "    otime1 = crap.stats.starttime + smb_peak[ii] # pick time\n",
    "    crap.trim(otime1  - 10, otime1 + 1.5*duration[ii] + 10) # window the data around the pick time\n",
    "    crap.taper(max_percentage=0.01,max_length=20)\n",
    "\n",
    "    data[:len(crap.data)] = crap.data #*100\n",
    "    f,psd=scipy.signal.welch(data,fs=fs,nperseg=81,noverlap=4)\n",
    "    #just get the frequencies within the filter band\n",
    "    above_low_cut = [f>low_cut]\n",
    "    below_high_cut = [f<high_cut]\n",
    "    in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "    f = f[in_band]\n",
    "    psd = psd[in_band]\n",
    "\n",
    "    # calculate characteristic frequency and report\n",
    "    char_freq_max = f[np.argmax(psd)]\n",
    "    char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "    psd_cumsum = np.cumsum(psd)\n",
    "    psd_sum = np.sum(psd)\n",
    "    char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "    char_freq.append(char_freq_mean)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    p=ax.plot(f,psd,label=stas[ii],linewidth=2)\n",
    "    cc = p[0].get_color()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid('True')\n",
    "    ax.set_xlabel('Frequency [Hz]')\n",
    "    ax.set_ylabel('PSD [$(mm/s)^2$/Hz]')\n",
    "    ax.vlines(char_freq_mean,ymin=np.min(psd)/10,ymax=np.max(psd)*10,linestyle=\"--\",colors=cc)\n",
    "    ax.grid(True)\n",
    "\n",
    "#             # weighting the data by the spikiness of the PSD vs frequency graphs\n",
    "    ratio = (np.mean(psd)/np.max(psd))\n",
    "    sharp_weight.append(int(1/(ratio**2)*20))\n",
    "\n",
    "\n",
    "    ############# KEEP DATA #######################\n",
    "\n",
    "#         if not max(smb_peak.shape):continue\n",
    "ddict = {'otime':otime, 'nets':nets, 'stas':stas,  'snr':snr, 'smb_peak': smb_peak, 'max_time':max_time, 'centroid_time': centroid_time , \\\n",
    "         'lats':lats, 'lons':lons, 'elevs':els, 'char_freq':char_freq, 'duration':duration, \\\n",
    "            'sharp_weight':sharp_weight, 'volcano':associated_volcano, 'event_ID':event_ID}\n",
    "if not np.any(dff):\n",
    "    dff = pd.DataFrame.from_dict(ddict)\n",
    "else:\n",
    "    dff=pd.concat([dff,pd.DataFrame.from_dict(ddict)],ignore_index=True)\n",
    "print(dff)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (11,8), dpi=400)\n",
    "fig.suptitle(str(otime)+\" \"+associated_volcano)\n",
    "ax = plt.subplot(1,1,1)\n",
    "iplot = 0\n",
    "for i in range(len(stas)):\n",
    "    data = evt_data.select(station=stas[i])[0].data\n",
    "    max1 = np.max(np.abs(data))\n",
    "    t = evt_data.select(station=stas[i])[0].times()\n",
    "    ax.plot(t-t_before,data/max1+iplot*1.5,linewidth=0.5)\n",
    "    if np.any(data_env_dict[nets[i]+'.'+stas[i]]):\n",
    "        ax.plot(t-t_before,data_env_dict[nets[i]+'.'+stas[i]]+iplot*1.5,'k',linewidth=1)\n",
    "    ax.plot(smb_peak[i],iplot*1.5,'r*',markersize=5)  \n",
    "    ax.plot(centroid_time[i],iplot*1.5,'k*',markersize=5)  \n",
    "    ax.plot(max_time[i],iplot*1.5,'r*',markersize=5)  \n",
    "    ax.set_yticks([])\n",
    "    plt.text(-15, iplot*1.5+0.5, stas[i])\n",
    "    # print(stas[i],snr[i])\n",
    "    # if i==ista:\n",
    "    # err_title=(\"%s %2.2f (s) error in picks\"%(stas[i],smb_peak[i]-t_before))\n",
    "    # plt.text(60, iplot*1.5+0.5,err_title,color='r')\n",
    "    plt.vlines(smb_peak[i],iplot*1.5-1.,iplot*1.5+1.,'r')\n",
    "    # print(duration[i],char_freq[i],sharp_weight[i])\n",
    "    # print(stas[i],smb_peak[i]-t_before)\n",
    "    iplot+=1\n",
    "# plt.grid(True)\n",
    "ax.set_xlim([-t_before,130])\n",
    "ax.set_xlabel('Time (s) since 13:28 ')\n",
    "plt.show()\n",
    "plt.savefig(\"../plots/\" + ava + \"_waveforms.png\")\n",
    "plt.clf()\n",
    "# del fig\n",
    "pdf.close()\n",
    "dff.to_csv(\"../data/events/MLPicks_\" + ava +\".csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61c4d6e8",
   "metadata": {},
   "source": [
    "Now locate the event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3bc3a",
   "metadata": {},
   "source": [
    "## Volcano data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c702477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "associated_volcano == 'Mt_Rainier'\n",
    "        \n",
    "#get info for stations within 50km of volcano that event ocurred at\n",
    "stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "############ LOCATION ############################\n",
    "# input necessary data for grid search\n",
    "arrivals = dff['smb_peak'].values\n",
    "# arrivals = dff[dff['event_ID']==event_ID]['smb_peak'].values\n",
    "sta_lats = dff['lats'].values\n",
    "sta_lons = dff['lons'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "\n",
    "\n",
    "# # Define the projection: UTM zone 11 for Washington state\n",
    "# proj = pyproj.Proj(proj='utm', zone=11, ellps='WGS84')\n",
    "\n",
    "# # Convert lat/long to Cartesian in meters\n",
    "# xsta, ysta = proj(sta_lons, sta_lats)\n",
    "\n",
    "# cmap = plt.get_cmap('hot_r')\n",
    "# ik=np.where(arrivals>0)[0]\n",
    "# for i in ik:\n",
    "#     tt = arrivals[i]-np.min(arrivals[ik])\n",
    "#     nmax=np.max(arrivals[ik])-np.min(arrivals[ik])\n",
    "#     plt.plot(xsta[i],ysta[i],'o',color=cmap(tt/nmax),markersize=10,markeredgecolor='k')\n",
    "#     plt.text(xsta[i],ysta[i],stas[i]+\":\"+str(np.ceil(tt))+\" s\")\n",
    "#     plt.axis('equal')\n",
    "# plt.title(\"Travel time Relative to RCM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de953a80",
   "metadata": {},
   "source": [
    "Now we are confident that we can do the grid search. let's check the other fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center latitude, center longitude, elevation(m), left_trim, right_trim, bottom_trim, top_trim \n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5]\n",
    "#Find the lower left corner and grid size based on volcano elevation\n",
    "# define grid origin in lat,lon and grid dimensions in m\n",
    "lon_start = -121.8 #volc_lat_lon[associated_volcano][0]\n",
    "lon_end = -121.5 #volc_lat_lon[associated_volcano][0]\n",
    "lat_start = 46.7 #volc_lat_lon[associated_volcano][1]\n",
    "lat_end = 47 #volc_lat_lon[associated_volcano][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ista =np.where(arrivals>-t_before+0.1)[0]\n",
    "t_best,lon_best,lat_best = gridsearch_parallel(lat_start,lon_start,\\\n",
    "                                              lat_end,lon_end,\\\n",
    "                                                sta_lats[ista],sta_lons[ista],arrivals[ista],vs=vs,\n",
    "                                                weight=dff['snr'].values[ista]**2)\n",
    "\n",
    "print(t_best,lon_best,lat_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f628aaf",
   "metadata": {},
   "source": [
    "Now let's test this simple grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ista =np.where(arrivals>-t_before+0.1)[0]\n",
    "rss_mat,t_best,lon_best,lat_best,best_idx = gridsearch(lat_start,lon_start,\\\n",
    "                                              lat_end,lon_end,\\\n",
    "                                                sta_lats[ista],sta_lons[ista],arrivals[ista],vs=vs,\n",
    "                                                weight=dff['snr'].values[ista]**2)\n",
    "\n",
    "# lets plot rss_mat to see if we can find the minimum\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52937c",
   "metadata": {},
   "source": [
    "Now also find the location of the first max time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "ista =np.where(arrivals>-t_before+0.1)[0]\n",
    "t_best,mlon_best,mlat_best = gridsearch_parallel(lat_start,lon_start,\\\n",
    "                                              lat_end,lon_end,\\\n",
    "                                                sta_lats[ista],sta_lons[ista],dff['max_time'].values[ista],vs=vs)\n",
    "print(mlon_best,mlat_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa187a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ista =np.where(arrivals>-t_before+0.1)[0]\n",
    "t_best,clon_best,clat_best = gridsearch_parallel(lat_start,lon_start,\\\n",
    "                                              lat_end,lon_end,\\\n",
    "                                                sta_lats[ista],sta_lons[ista],dff['centroid_time'].values[ista],vs=vs)\n",
    "print(clon_best,clat_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3f571",
   "metadata": {},
   "source": [
    "Now we are going to figure out what is going on with earthquake location.\n",
    "\n",
    "1. Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs=500\n",
    "ista =np.where(arrivals>-t_before+0.1)[0]\n",
    "rss_mat,t_best,lon_best,lat_best,best_idx = gridsearch(lat_start,lon_start,\\\n",
    "                                              lat_end,lon_end,\\\n",
    "                                                sta_lats[ista],sta_lons[ista],arrivals[ista],vs=vs,\n",
    "                                                weight=dff['snr'].values[ista]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f61c7",
   "metadata": {},
   "source": [
    "## Solve for the best velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197513d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs=500\n",
    "ista =np.where(arrivals>-t_before+0.1)[0] \n",
    "t_best,lon_best,lat_best,v_best = gridsearch_parallel_vs(lat_start,lon_start,\\\n",
    "                                              lat_end,lon_end,\\\n",
    "                                                sta_lats[ista],sta_lons[ista],arrivals[ista],\n",
    "                                                weight=dff['snr'].values[ista]**2)\n",
    "\n",
    "print(t_best,lon_best,lat_best,v_best) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b295271",
   "metadata": {},
   "source": [
    "Plot the residual curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc014b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot rss_mat to see if we can find the minimum\n",
    "fig, ax = plt.subplots(figsize=(16, 10), dpi=400)\n",
    "im = plt.imshow(np.squeeze(rss_mat[0,:,:].T),extent=[lon_start,lon_end,lat_start,lat_end], cmap='hsv', interpolation='nearest',origin='lower')\n",
    "# Add contour lines\n",
    "num_contour_lines = 100  # Change this to the number of contour lines you want\n",
    "contours = plt.contour(np.squeeze(rss_mat[0,:,:].T), num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "plt.clabel(contours, inline=True, fontsize=8)\n",
    "plt.colorbar(im)\n",
    "plt.title(\"RSS for Mt. Rainier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64186eb9",
   "metadata": {},
   "source": [
    "#### Create synthetics and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eab22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the source location, calculate travel times\n",
    "proj = pyproj.Proj(proj='utm', zone=10, ellps='WGS84')\n",
    "\n",
    "if lon_start<0: \n",
    "    lon_start1 = lon_start+360\n",
    "    lon_end1 = lon_end + 360\n",
    "\n",
    "# Convert lat/long to Cartesian in meters\n",
    "x_step=100\n",
    "x1,y1=proj(lon_start,lat_start)\n",
    "x2,y2=proj(lon_end,lat_end)\n",
    "su_x,su_y=proj(lon_best,lat_best)\n",
    "# Generate the x and y coordinates for the grid\n",
    "x_coords = np.arange(x1, x2, x_step)\n",
    "y_coords = np.arange(y1, y2, x_step)\n",
    "synthetic_tt = np.zeros((len(x_coords),len(y_coords)))    \n",
    "for i in range(len(x_coords)):\n",
    "    for j in range(len(y_coords)):\n",
    "        synthetic_tt[i,j] = np.sqrt( (x_coords[i] - su_x  )**2 + (y_coords[j] - su_y)**2  ) / vs \n",
    "    \n",
    "# Existing imshow plot\n",
    "fig, ax = plt.subplots(figsize=(16, 10), dpi=400)\n",
    "im = plt.imshow(np.squeeze(rss_mat[0,:,:].T),extent=[lon_start,lon_end,lat_start,lat_end], cmap='hsv', interpolation='nearest',origin='lower')\n",
    "\n",
    "\n",
    "# Add contour lines\n",
    "num_contour_lines = 20  # Change this to the number of contour lines you want\n",
    "\n",
    "contours = plt.contour(synthetic_tt.T, num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "# contours = plt.contour(np.squeeze(rss_mat[0,:,:].T), num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "# contours = plt.contour(synthetic_tt, num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "plt.clabel(contours, inline=True, fontsize=8)\n",
    "# plt.plot(bootstrap_lon_best,bootstrap_lat_best,'o',color='k',markersize=10)\n",
    "cmap1 = plt.get_cmap('hot_r')\n",
    "cmap2 = plt.get_cmap('hsv')\n",
    "ik=np.where(arrivals>0)[0]\n",
    "for i in ik:\n",
    "    tt = arrivals[i]-np.min(arrivals[ik])\n",
    "    nmax=np.max(arrivals[ik])-np.min(arrivals[ik])\n",
    "    plt.plot(sta_lons[i],sta_lats[i],'o',color=cmap2(tt/10),markersize=dff[dff['stas']==stas[i]]['snr'].values,markeredgecolor='k')\n",
    "    plt.text(sta_lons[i],sta_lats[i],stas[i]+\":\"+str(np.ceil(tt*10)/10)+\" s\")\n",
    "plt.axis('equal')\n",
    "plt.plot(lon_best,lat_best,'o',color='k',markersize=10)\n",
    "plt.title(\"Travel time Relative to STAR, marker size shows SNR of waveform\")\n",
    "plt.xlim([-122,-121.5]);\n",
    "plt.savefig(\"../plots/\" + ava+ \"_res.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955aa085",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing imshow plot\n",
    "fig, ax = plt.subplots(figsize=(16, 10), dpi=400)\n",
    "im = plt.imshow(np.squeeze(rss_mat[0,:,:].T),extent=[lon_start,lon_end,lat_start,lat_end], cmap='hsv', interpolation='nearest',origin='lower')\n",
    "\n",
    "# Add contour lines\n",
    "num_contour_lines = 20  # Change this to the number of contour lines you want\n",
    "contours = plt.contour(np.squeeze(rss_mat[0,:,:].T), num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "plt.clabel(contours, inline=True, fontsize=8)\n",
    "plt.plot(lon_best,lat_best,'o',color='k',markersize=10)\n",
    "plt.plot(mlon_best,mlat_best,'o',color='r',markersize=10)\n",
    "plt.plot(clon_best,clat_best,'o',color='r',markersize=10)\n",
    "cmap1 = plt.get_cmap('hot_r')\n",
    "ik=np.where(arrivals>0)[0]\n",
    "for i in ik:\n",
    "    tt = arrivals[i]-np.min(arrivals[ik])\n",
    "    nmax=np.max(arrivals[ik])-np.min(arrivals[ik])\n",
    "    plt.plot(sta_lons[i],sta_lats[i],'o',color=cmap1(tt/nmax),markersize=20,markeredgecolor='k')\n",
    "    plt.text(sta_lons[i],sta_lats[i],stas[i]+\":\"+str(np.ceil(tt))+\" s\")\n",
    "plt.title(\"Travel time Relative to STAR\")\n",
    "plt.xlim([-122,-121.5]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e840f5",
   "metadata": {},
   "source": [
    "ok, the location works but seems quite innacurate: why is the location not closer to the first stations that saw it?\n",
    "We will do a bootstrap grid search and find an enemble of solution and take the median location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ista =np.where(arrivals>-t_before+0.1)[0]\n",
    "# # Number of bootstrap samples\n",
    "# n_bootstrap = 100\n",
    "\n",
    "# # Initialize an array to hold the bootstrap results\n",
    "# bootstrap_results = []\n",
    "# llon_best=np.zeros(n_bootstrap)\n",
    "# llat_best=np.zeros(n_bootstrap)\n",
    "# for ii in range(n_bootstrap):\n",
    "#     # Generate a bootstrap sample from ista\n",
    "#     ista_sample = np.random.choice(ista, size=len(ista), replace=True)\n",
    "#     # ista_sample=ista\n",
    "#     # Perform the grid search with the bootstrap sample\n",
    "#     t_best, lon_best, lat_best = gridsearch_parallel(lat_start, lon_start, lat_end, lon_end,\\\n",
    "#                                                                 sta_lats[ista_sample], sta_lons[ista_sample],\\\n",
    "#                                                                     arrivals[ista_sample])\n",
    "\n",
    "#     # Store the results\n",
    "#     llon_best[ii] = lon_best\n",
    "#     llat_best[ii] = lat_best\n",
    "#     print(llat_best[ii],llon_best[ii])\n",
    "#     break\n",
    "# bootstrap_lon_best=np.mean(llon_best)\n",
    "# bootstrap_lat_best=np.mean(llat_best)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b477d41",
   "metadata": {},
   "source": [
    "After some experimentation, we find that the locations found by the grid search are highly dependent on the choice of stations used. Weighting with the SNR brings even more variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c310da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Existing imshow plot\n",
    "# fig, ax = plt.subplots(figsize=(16, 10), dpi=400)\n",
    "# im = plt.imshow(np.squeeze(rss_mat[0,:,:].T),extent=[lon_start,lon_end,lat_start,lat_end], cmap='hsv', interpolation='nearest',origin='lower')\n",
    "\n",
    "# # Add contour lines\n",
    "# num_contour_lines = 20  # Change this to the number of contour lines you want\n",
    "# contours = plt.contour(np.squeeze(rss_mat[0,:,:].T), num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "# plt.clabel(contours, inline=True, fontsize=8)\n",
    "# plt.plot(llon_best,llat_best,'o',color='k',markersize=6)\n",
    "# plt.plot(bootstrap_lon_best,bootstrap_lat_best,'o',color='k',markersize=10)\n",
    "# cmap1 = plt.get_cmap('hot_r')\n",
    "# ik=np.where(arrivals>0)[0]\n",
    "# for i in ik:\n",
    "#     tt = arrivals[i]-np.min(arrivals[ik])\n",
    "#     nmax=np.max(arrivals[ik])-np.min(arrivals[ik])\n",
    "#     plt.plot(sta_lons[i],sta_lats[i],'o',color=cmap1(tt/nmax),markersize=dff[dff['stas']==stas[i]]['snr'].values,markeredgecolor='k')\n",
    "#     plt.text(sta_lons[i],sta_lats[i],stas[i]+\":\"+str(np.ceil(tt))+\" s\")\n",
    "# plt.title(\"Travel time Relative to RCM, marker size shows SNR of waveform\")\n",
    "# plt.xlim([-122,-121.5]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing imshow plot\n",
    "fig, ax = plt.subplots(figsize=(16, 10), dpi=400)\n",
    "im = plt.imshow(np.squeeze(rss_mat[0,:,:].T),extent=[lon_start,lon_end,lat_start,lat_end], cmap='hsv', interpolation='nearest',origin='lower')\n",
    "\n",
    "\n",
    "# Add contour lines\n",
    "num_contour_lines = 20  # Change this to the number of contour lines you want\n",
    "\n",
    "contours = plt.contour(synthetic_tt.T, num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "# contours = plt.contour(np.squeeze(rss_mat[0,:,:].T), num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "# contours = plt.contour(synthetic_tt, num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "plt.clabel(contours, inline=True, fontsize=8)\n",
    "# plt.plot(bootstrap_lon_best,bootstrap_lat_best,'o',color='k',markersize=10)\n",
    "cmap1 = plt.get_cmap('hot_r')\n",
    "cmap2 = plt.get_cmap('hsv')\n",
    "ik=np.where(arrivals>0)[0]\n",
    "for i in ik:\n",
    "    tt = arrivals[i]-np.min(arrivals[ik])\n",
    "    nmax=np.max(arrivals[ik])-np.min(arrivals[ik])\n",
    "    plt.plot(sta_lons[i],sta_lats[i],'o',color=cmap2(tt/10),markersize=dff[dff['stas']==stas[i]]['snr'].values,markeredgecolor='k')\n",
    "    plt.text(sta_lons[i],sta_lats[i],stas[i]+\":\"+str(np.ceil(tt*10)/10)+\" s\")\n",
    "plt.axis('equal')\n",
    "plt.plot(lon_best,lat_best,'o',color='k',markersize=10)\n",
    "plt.title(\"Travel time Relative to STAR, marker size shows SNR of waveform\")\n",
    "plt.xlim([-122,-121.5]);\n",
    "plt.savefig(\"../plots/\" + ava+ \"_res.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1adfed",
   "metadata": {},
   "source": [
    "Now locate the centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2624b4",
   "metadata": {},
   "source": [
    "## DEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49436a",
   "metadata": {},
   "source": [
    "We need to reproject the DEM onto our own grid (x_coord, y_coord)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rasterio\n",
    "from matplotlib.colors import LightSource\n",
    "from rasterio.warp import transform_bounds,  reproject, Resampling , calculate_default_transform\n",
    "from rasterio.transform import array_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b744ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_volcano = 'Mt_Rainier'\n",
    "        \n",
    "#get info for stations within 50km of volcano that event ocurred at\n",
    "stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a638d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center latitude, center longitude, elevation(m), left_trim, right_trim, bottom_trim, top_trim \n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5]\n",
    "#Find the lower left corner and grid size based on volcano elevation\n",
    "# define grid origin in lat,lon and grid dimensions in m\n",
    "lon_start = -121.9 #volc_lat_lon[associated_volcano][0]\n",
    "lon_end = -121.65 #volc_lat_lon[associated_volcano][0]\n",
    "lat_start = 46.6 #volc_lat_lon[associated_volcano][1]\n",
    "lat_end = 47 #volc_lat_lon[associated_volcano][1]\n",
    "\n",
    "\n",
    "# Create a light source\n",
    "ls = LightSource(azdeg=315, altdeg=45)\n",
    "\n",
    "\n",
    "# Load the DEM with rasterio\n",
    "with rasterio.open('../data/geospatial/Mt_Rainier/Mt_Rainier.tif') as src:\n",
    "    dem = src.read(1)  # read the first band\n",
    "    transform = src.transform\n",
    "    bounds = src.bounds\n",
    "    crs=src.crs\n",
    "    # dem = dem.astype('float64')\n",
    "    dem[dem == -32767] = np.nan #gets rid of edge effects\n",
    "    # dem = np.nan_to_num(dem,nan=1000)\n",
    "\n",
    "\n",
    "# Define the target CRS\n",
    "dst_crs = 'EPSG:4326'  # EPSG:4326 is the code for WGS84 lat/lon\n",
    "\n",
    "# Calculate the transform and dimensions for the reprojected DEM\n",
    "transform_latlon, width, height = calculate_default_transform(crs, dst_crs, dem.shape[1], dem.shape[0], *bounds)\n",
    "\n",
    "\n",
    "# Create an empty array for the reprojected DEM\n",
    "dem_latlon = np.empty(shape=(height, width))\n",
    "\n",
    "# Reproject the DEM\n",
    "reproject(\n",
    "    source=dem,\n",
    "    destination=dem_latlon,\n",
    "    src_transform=transform,\n",
    "    src_crs=crs,\n",
    "    dst_transform=transform_latlon,\n",
    "    dst_crs=dst_crs,\n",
    "    resampling=Resampling.nearest)\n",
    "\n",
    "# Transform the bounds to the target CRS\n",
    "left, bottom, right, top = transform_bounds(crs, dst_crs, *bounds)\n",
    "\n",
    "# Calculate the illumination intensity\n",
    "illumination = ls.hillshade(dem_latlon)\n",
    "# new bounds\n",
    "left, bottom, right, top = array_bounds(height, width, transform_latlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd288f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10), dpi=400)\n",
    "\n",
    "# find the right UTM zone for the data\n",
    "def get_utm_zone(longitude):\n",
    "    return int((longitude + 180) / 6) + 1\n",
    "utm_zone = get_utm_zone(np.mean(sta_lons))\n",
    "# # Define the projection: UTM zone 11 for Washington state\n",
    "proj = pyproj.Proj(proj='utm', zone=utm_zone, ellps='WGS84')\n",
    "# Convert lat/long to Cartesian in meters\n",
    "xsta, ysta = proj(sta_lons, sta_lats)\n",
    "cmap = plt.get_cmap('hot_r')\n",
    "\n",
    "plt.imshow(illumination, extent=[left, right, bottom,top ], cmap='gray',clim=[0,1],alpha=0.5,aspect='equal');\n",
    "# plt.imshow(illumination, extent=[lon_start, lon_end, lat_start, lat_end], cmap='gray',\\\n",
    "    # clim=[0,1],alpha=0.5,aspect='equal')\n",
    "\n",
    "issta=np.argmin(arrivals[ista])\n",
    "stass=stas[ista[issta]]\n",
    "\n",
    "# Plot the origin\n",
    "plt.plot(lon_best,lat_best,'*',color='r',markersize=20,markeredgecolor='k')\n",
    "\n",
    "plt.title(str(dff[dff['event_ID']==event_ID]['otime'].values[0])[0:13]+ \" Travel time Relative to \"+stass);\n",
    "\n",
    "# im = plt.imshow(np.squeeze(rss_mat[0,:,:].T),extent=[lon_start,lon_end,lat_start,lat_end], cmap='hsv', interpolation='nearest',origin='lower')\n",
    "\n",
    "# Add contour lines\n",
    "num_contour_lines = 20  # Change this to the number of contour lines you want\n",
    "\n",
    "# Add contour lines\n",
    "num_contour_lines = 20  # Change this to the number of contour lines you want\n",
    "\n",
    "contours = plt.contour(synthetic_tt.T, num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "# contours = plt.contour(np.squeeze(rss_mat[0,:,:].T), num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "# contours = plt.contour(synthetic_tt, num_contour_lines, extent=[lon_start,lon_end,lat_start,lat_end], colors='black')\n",
    "plt.clabel(contours, inline=True, fontsize=8)\n",
    "# plt.plot(bootstrap_lon_best,bootstrap_lat_best,'o',color='k',markersize=10)\n",
    "cmap1 = plt.get_cmap('hot_r')\n",
    "cmap2 = plt.get_cmap('hsv')\n",
    "ik=np.where(arrivals>0)[0]\n",
    "for i in ik:\n",
    "    tt = arrivals[i]-np.min(arrivals[ik])\n",
    "    nmax=np.max(arrivals[ik])-np.min(arrivals[ik])\n",
    "    plt.plot(sta_lons[i],sta_lats[i],'o',color=cmap2(tt/10),markersize=dff[dff['stas']==stas[i]]['snr'].values,markeredgecolor='k')\n",
    "    plt.text(sta_lons[i],sta_lats[i],stas[i]+\":\"+str(np.ceil(tt*10)/10)+\" s\")\n",
    "plt.axis('equal')\n",
    "# plt.plot(lon_best,lat_best,'o',color='k',markersize=10)\n",
    "plt.title(\"Travel time Relative to RCM, marker size shows SNR of waveform\")\n",
    "plt.xlim([-122,-121.5]);\n",
    "plt.savefig(\"../plots/\" + ava + \"_res_topo.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo_exo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
