{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8e775a",
   "metadata": {},
   "source": [
    "# Event Analysis\n",
    "This notebooke analyses the surface event data 10/10/22 fskene@uw.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8a9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.geodetics import *\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "import requests\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "import richdem as rd\n",
    "from pathlib import Path\n",
    "import os \n",
    "import glob\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import asarray as ar,exp\n",
    "from pyproj import Proj,transform,Geod\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584b666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish parameters\n",
    "window = 30 #window length of the signal\n",
    "thr = 12 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "v_s = 1000 #shear wave velocity at the surface\n",
    "colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3\n",
    "ratio = 5.6915196\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da6478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def volc_loc_thr(left_lat, bottom_lon, sidelength):\n",
    "    d = distance.geodesic(meters = sidelength)\n",
    "    right_lat = d.destination(point=[left_lat,bottom_lon], bearing=0)[0]\n",
    "    top_lon = d.destination(point=[left_lat,bottom_lon], bearing=90)[1]\n",
    "    return right_lat, top_lon\n",
    "\n",
    "def start_latlon(elevation, ratio, center_lat, center_lon):\n",
    "    side_length = elevation * ratio\n",
    "    l = side_length/2\n",
    "    hypotenuse = l*np.sqrt(2)\n",
    "    d = distance.geodesic(meters = hypotenuse)\n",
    "    start_lat = d.destination(point=[center_lat,center_lon], bearing=225)[0]\n",
    "    start_lon = d.destination(point=[center_lat,center_lon], bearing=225)[1]\n",
    "    return start_lat, start_lon, side_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220231f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and organize location data\n",
    "Event_Data = pd.read_csv(\"Analysis_Data/Event_Data_10_24.csv\")\n",
    "new_list = [np.nan]*(len(Event_Data))\n",
    "Event_Data['Label'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62283d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read xcel file with ground truth events (from Wes)\n",
    "\n",
    "# Mt Rainier\n",
    "inputExcelFile =\"Data/surfaceFlows_cloud.xlsx\"\n",
    "# Reading an excel file\n",
    "excelFile = pd.read_excel (inputExcelFile)\n",
    "# Converting excel file into CSV file\n",
    "excelFile.to_csv (\"Data/ResultCsvFile_rainier.csv\", index = None, header=True)\n",
    "# Reading and Converting the output csv file into a dataframe object\n",
    "known_events_r = pd.DataFrame(pd.read_csv(\"Data/ResultCsvFile_rainier.csv\"))\n",
    "starttimes_rainier = []\n",
    "for i in range(len(known_events_r['Date'])):\n",
    "    try:\n",
    "        time = known_events_r['Time Start'][i].split(':')\n",
    "        if time[1][0] == '0':\n",
    "            time[1] = time[1][1]\n",
    "        if time[2][0] == '0':\n",
    "            time[2] = time[2][1]\n",
    "        date = known_events_r['Date'][i].split('-')\n",
    "        if date[1][0] == '0':\n",
    "            date[1] = date[1][1]\n",
    "        if date[2][0] == '0':\n",
    "            date[2] = date[2][1]\n",
    "        starttimes_rainier.append(UTCDateTime(int(date[0]),int(date[1]),int(date[2]),int(time[0])))#,int(time[1])))#,int(time[2])))\n",
    "    except:\n",
    "        continue \n",
    "        \n",
    "# Mt St Helens\n",
    "inputExcelFile =\"Data/surfaceFlows_cloud_st_helens.xlsx\"\n",
    "excelFile = pd.read_excel (inputExcelFile)\n",
    "excelFile.to_csv (\"Data/ResultCsvFile_st_helens.csv\", index = None, header=True)\n",
    "known_events_sh = pd.DataFrame(pd.read_csv(\"Data/ResultCsvFile_st_helens.csv\"))\n",
    "starttimes_st_helens = []\n",
    "for i in range(len(known_events_sh['Date'])):\n",
    "    try:\n",
    "        time = known_events_sh['Time'][i].split(':')\n",
    "        if time[1][0] == '0':\n",
    "            time[1] = time[1][1]\n",
    "        if time[2][0] == '0':\n",
    "            time[2] = time[2][1]\n",
    "        date = known_events_sh['Date'][i].split('-')\n",
    "        if date[1][0] == '0':\n",
    "            date[1] = date[1][1]\n",
    "        if date[2][0] == '0':\n",
    "            date[2] = date[2][1]\n",
    "        starttimes_st_helens.append(UTCDateTime(int(date[0]),int(date[1]),int(date[2]),int(time[0])))#,int(time[1])))#,int(time[2])))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "# Mt Hood\n",
    "inputExcelFile =\"Data/surfaceFlows_cloud_hood.xlsx\"\n",
    "excelFile = pd.read_excel (inputExcelFile)\n",
    "excelFile.to_csv (\"Data/ResultCsvFile_hood.csv\", index = None, header=True)\n",
    "known_events_h = pd.DataFrame(pd.read_csv(\"Data/ResultCsvFile_hood.csv\"))\n",
    "starttimes_hood = []\n",
    "for i in range(len(known_events_h['Date'])):\n",
    "    try:\n",
    "        time = known_events_h['Time'][i].split(':')\n",
    "        if time[1][0] == '0':\n",
    "            time[1] = time[1][1]\n",
    "        if time[2][0] == '0':\n",
    "            time[2] = time[2][1]\n",
    "        date = known_events_h['Date'][i].split('-')\n",
    "        if date[1][0] == '0':\n",
    "            date[1] = date[1][1]\n",
    "        if date[2][0] == '0':\n",
    "            date[2] = date[2][1]\n",
    "        starttimes_hood.append(UTCDateTime(int(date[0]),int(date[1]),int(date[2]),int(time[0])))#,int(time[1])))#,int(time[2])))\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aea5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volcano location data\n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5, 10000, 3000, 15000, 7000]\n",
    "volc_lat_lon['Mt_Adams'] = [46.202621, -121.4906384, 3743.2, 5000, 3000, 4000, 2000]\n",
    "volc_lat_lon['Mt_Baker'] = [48.7773426,  -121.8132008, 3287.6, 0, 0, 0, 2000]\n",
    "volc_lat_lon['Mt_St_Helens'] =[46.200472222222224,-122.18883611111112,2549, 10000, 10000, 17000, 15000] #[46.1912, -122.1944, 2549]\n",
    "volc_lat_lon['Glacier_Peak'] = [48.1112273, -121.1139922, 3213, 14000, 10000, 8000, 10000]\n",
    "volc_lat_lon['Crater_Lake']=[42.907745, -122.143494, 1883, 60000, 0, 90000, 0]\n",
    "volc_lat_lon['Mt_Hood']=[45.373221, -121.696509, 3428.7, 18000, 50000, 35000, 65000]\n",
    "volc_lat_lon['Newberry']=[43.7220653, -121.2344654, 2435, 53000, 12000, 70000, 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb37acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEM data \n",
    "dem_data_dict = {}\n",
    "for name in volc_lat_lon:\n",
    "    if volc_lat_lon[name][0]>46:\n",
    "        dem = rio.open('Data/DEM_data/'+str(name)+'/'+str(name)+'.tif') #washington volcanoes\n",
    "        dem_array = dem.read(1).astype('float64')\n",
    "        dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "        crs = dem.crs\n",
    "    else:\n",
    "        dem = rio.open('Data/DEM_data/'+str(name)+'/_w001001.adf') #oregon volcanoes\n",
    "        dem_array = dem.read(1).astype('float64')\n",
    "        dem_array[dem_array == -3.4028234663852886e+38] = np.nan #gets rid of edge effects\n",
    "        crs = dem.crs\n",
    "#     volc = rd.rdarray(dem_array, no_data=-9999)\n",
    "#     slope = rd.TerrainAttribute(volc,attrib = 'slope_riserun')\n",
    "#     aspect = rd.TerrainAttribute(volc, attrib = 'aspect')\n",
    "#     dem_data_dict[name] = {'data':dem_array, 'elevation':volc, 'slope':slope, 'aspect':aspect}\n",
    "    dem_data_dict[name]={'data':dem_array, 'crs':crs, 'left':dem.bounds[0], 'right':dem.bounds[2], 'bottom':dem.bounds[1], 'top':dem.bounds[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0610d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lower left corner and grid size based on volcano elevation\n",
    "volc_grid = {}\n",
    "for volc in volc_lat_lon:\n",
    "    elevation = volc_lat_lon[volc][2]\n",
    "    center_lat = volc_lat_lon[volc][0]\n",
    "    center_lon = volc_lat_lon[volc][1]\n",
    "    left_lat, bottom_lon, sidelength = start_latlon(elevation, ratio, center_lat, center_lon)\n",
    "    right_lat, top_lon = volc_loc_thr(left_lat, bottom_lon, sidelength)\n",
    "    volc_grid[volc] = [left_lat, right_lat, bottom_lon, top_lon, sidelength]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b7c84",
   "metadata": {},
   "source": [
    "## Add Ground truth labels from Wes' Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262a5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-06-25T23:00:00.000000Z Youtube video of icefall off Nisqually transitioning into a surface flow\n",
      "2020-04-02T21:00:00.000000Z Rockfall (?)\n",
      "2020-04-05T04:00:00.000000Z Rockfall (?)\n",
      "2020-04-05T05:00:00.000000Z Rockfall (?)\n",
      "2020-04-06T14:00:00.000000Z Avalanche (?)\n",
      "2020-04-09T13:00:00.000000Z Avalanche.  Confirmed with video from the carbon glacier headwall west of liberty ridge\n",
      "2020-04-15T13:00:00.000000Z ?\n",
      "2020-05-10T09:00:00.000000Z double pulse cigar\n",
      "2020-05-18T11:00:00.000000Z Nice moving source upstream Puyallup (helicopter?)\n",
      "2020-07-16T04:00:00.000000Z Large signal (Liberty Cap verified w/ satellite) MIRR array recorded as well\n",
      "2020-08-28T12:00:00.000000Z No obvious infrasound signal on the westside. \n",
      "2020-09-28T20:00:00.000000Z Weak on PR03, non existant on Kautz, others yes\n",
      "2020-11-12T02:00:00.000000Z Well recorded on PARA/KAUT.  Not recorded on westside.\n",
      "2020-11-11T17:00:00.000000Z Well recorded on PARA/KAUT.  Not recorded on westside.\n",
      "2021-03-11T22:00:00.000000Z Confirmed avalanche from south side of sickle/sunset amplitheater, photos available from Craig Craker (photos in email)  \n",
      "2021-03-22T23:00:00.000000Z PARA/OPCH infra, strong seismic\n",
      "2021-03-27T11:00:00.000000Z KAUT/PARA/OPCH infrasound\n",
      "2021-04-22T19:00:00.000000Z KAUT/PARA infrasound (OPCH out)\n",
      "2021-06-21T12:00:00.000000Z nan\n",
      "2021-08-04T17:00:00.000000Z Carbon event (?) only shows up on CRBN\n",
      "2021-08-09T14:00:00.000000Z Seen on temp arrays, larger\n",
      "2021-08-11T14:00:00.000000Z Good records on south side, maybe PR05, temp arrays too\n",
      "2021-08-27T11:00:00.000000Z nan\n",
      "2021-08-29T23:00:00.000000Z nan\n",
      "2021-08-30T06:00:00.000000Z nan\n",
      "2021-09-17T02:00:00.000000Z Weak seismic, good infrasound on OPCH, PARA, KAUT (Nisually Icefall w/ video by Jost)\n",
      "2021-09-24T05:00:00.000000Z Strong seismic signal, amplitude locate near Carbon/Russell.  Infrasound on west side and CRBN\n",
      "2021-10-17T11:00:00.000000Z Good KAUT, less so on PARA.  Nice event overall\n",
      "2021-11-01T05:00:00.000000Z Strong seismic, impulsive on PARA/KAUT\n",
      "2021-11-12T17:00:00.000000Z PARA detected, mod seismic signal\n"
     ]
    }
   ],
   "source": [
    "#Mt Rainier\n",
    "\n",
    "# turning the times from strings to UTCDateTime objects\n",
    "times = list(Event_Data['origin_time'])\n",
    "temp = []\n",
    "for i in times:\n",
    "    a = UTCDateTime(i).strftime(\"%Y-%m-%d, %H\")\n",
    "    temp.append(UTCDateTime(a))\n",
    "    \n",
    "# aligning the timing og the cataloged events with those in the csv\n",
    "overlaps = []\n",
    "temp2 = []\n",
    "for i in range(len(starttimes_rainier)):\n",
    "    if starttimes_rainier[i] in temp:\n",
    "        overlaps.append(starttimes_rainier[i])\n",
    "        temp2.append(known_events_r['Remarks'][i])\n",
    "        print(starttimes_rainier[i],known_events_r['Remarks'][i])\n",
    "for i in range(len(overlaps)):\n",
    "    index = temp.index(overlaps[i])\n",
    "    Event_Data['Label'][index]= temp2[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd576606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-05-15T01:00:00.000000Z Avalanche on East Side contained on Edifice (see https://www.pnsn.org/blog/2014/05/15/warm-weather-triggers-snow-avalanches-at-st-helens)\n",
      "2017-02-09T11:00:00.000000Z Probable avalanche (UW sites out)\n",
      "2021-02-01T10:00:00.000000Z Avalanche on East Side (see MSH_20210201_avalanche.py)\n",
      "2021-02-03T05:00:00.000000Z Avalanche on East Side (see MSH_20210203_avalanche.py)\n",
      "2021-09-03T21:00:00.000000Z I have no idea, but itâ€™s a high amplitude signal.  And there were a bunch of others over that summer\n"
     ]
    }
   ],
   "source": [
    "# events on Mt St Helens\n",
    "    \n",
    "# aligning the timing og the cataloged events with those in the csv\n",
    "overlaps = []\n",
    "temp2 = []\n",
    "for i in range(len(starttimes_st_helens)):\n",
    "    if starttimes_st_helens[i] in temp:\n",
    "        overlaps.append(starttimes_st_helens[i])\n",
    "        temp2.append(known_events_sh['Remarks'][i])\n",
    "        print(starttimes_st_helens[i],known_events_sh['Remarks'][i])\n",
    "for i in range(len(overlaps)):\n",
    "    index = temp.index(overlaps[i])\n",
    "    Event_Data['Label'][index]= temp2[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "266d8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events on Mt Hood\n",
    "\n",
    "# aligning the timing og the cataloged events with those in the csv\n",
    "overlaps = []\n",
    "temp2 = []\n",
    "for i in range(len(starttimes_hood)):\n",
    "    if starttimes_hood[i] in temp:\n",
    "        overlaps.append(starttimes_hood[i])\n",
    "        temp2.append(known_events_h['Remarks'][i])\n",
    "        print(starttimes_hood[i],known_events_h['Remarks'][i])\n",
    "for i in range(len(overlaps)):\n",
    "    index = temp.index(overlaps[i])\n",
    "    Event_Data['Label'][index]= temp2[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f1efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Event_Data.to_csv('~/surface_events/Analysis_Data/Event_Data/Event_Data_w_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6ff06",
   "metadata": {},
   "source": [
    "# Histogram of frequencies at each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_data = pd.read_csv('Analysis_Data/Station_frequency_data_10_24.csv') \n",
    "sta_freq = {}\n",
    "for i in freq_data.columns:\n",
    "    df2=freq_data.dropna(subset=[i])\n",
    "    med_freq = np.median(df2[i])\n",
    "    if med_freq>0:\n",
    "        sta_freq[i] = med_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b95f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (17,4))\n",
    "plt.title('median frequency at each station')\n",
    "plt.bar(np.linspace(0,90,len(sta_freq)), sta_freq.values(), color=(['m','c']*41)+['m'],tick_label = list(sta_freq.keys()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('frequency(Hz)')\n",
    "plt.xlabel('station code')\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c955d",
   "metadata": {},
   "source": [
    "## Scatterplot of event locations + Histograms of Velocities on each Volcano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9267aea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mtn_list = ['Mt_Rainier','Mt_St_Helens','Mt_Hood']\n",
    "for n in mtn_list:\n",
    "    lats = []\n",
    "    lons = []\n",
    "    vel = []\n",
    "    evt_id = []\n",
    "    dir_snr = []\n",
    "    dir_sharp = []\n",
    "    direc = []\n",
    "    sharp = []\n",
    "    vel = []\n",
    "    times = []\n",
    "    new_vel = []\n",
    "    for i in range(len(Event_Data)):\n",
    "        if volc_grid[n][0]<Event_Data['location_latitude'][i]<volc_grid[n][1] and volc_grid[n][2]<Event_Data['location_longitude'][i]<volc_grid[n][3]:\n",
    "            lats.append(Event_Data['location_latitude'][i])\n",
    "            lons.append(Event_Data['location_longitude'][i])\n",
    "            vel.append(Event_Data['velocity(m/s)'][i])\n",
    "            evt_id.append(Event_Data['event_ID'][i])\n",
    "            dir_snr.append(Event_Data['direction_snr(degrees)'][i])\n",
    "            direc.append(Event_Data['direction(degrees)'][i])\n",
    "            dir_sharp.append(Event_Data['direction_sharpness(degrees)'][i])\n",
    "            vel.append(Event_Data['velocity(m/s)'][i])\n",
    "            times.append(Event_Data['origin_time'][i])\n",
    "    for i in vel:\n",
    "        if int(i) <= 300:\n",
    "            new_vel.append(i)\n",
    "    # histogram of velocities\n",
    "    a = np.median(new_vel)\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    fig,ax = plt.subplots(figsize = [10,4], dpi = 200)\n",
    "    if n == 'Mt_Rainier' or n =='Mt_Hood':\n",
    "        ax.set_title(n.split('_')[0]+' '+n.split('_')[1])\n",
    "    else:\n",
    "         ax.set_title(n.split('_')[0]+' '+n.split('_')[1]+' '+n.split('_')[2])   \n",
    "    ax.set_ylabel('number of events', fontsize = 15)\n",
    "    ax.set_xlabel('velocity(m/s)', fontsize = 15)\n",
    "    binwidth = 10\n",
    "    num_of_events = ax.hist(new_vel,bins=range(int(min(new_vel)), int(max(new_vel)) + binwidth, binwidth), color = 'dodgerblue',edgecolor = \"black\")\n",
    "    height = int(num_of_events[0].max()+5)\n",
    "    ax.grid('True')\n",
    "    ax.vlines(a,0,height-1,'r','--', label = 'median velocity')\n",
    "    ax.set_xlim([0,300])\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0,height)\n",
    "    plt.savefig('vels'+n+'.png')\n",
    "    \n",
    "    #prepare data for plots\n",
    "    data = dem_data_dict[n]['data']\n",
    "    volc = rd.rdarray(data, no_data=-9999)\n",
    "    aspect = np.array(rd.TerrainAttribute(volc, attrib = 'aspect'))\n",
    "    slope = rd.TerrainAttribute(volc,attrib = 'slope_riserun')\n",
    "    associated_volcano = n\n",
    "    crs = dem_data_dict[associated_volcano]['crs']\n",
    "    data = dem_data_dict[associated_volcano]['data']\n",
    "    info = volc_lat_lon[associated_volcano]\n",
    "    p2 = Proj(crs,preserve_units=False)\n",
    "    p1 = Proj(proj='latlong',preserve_units=False)\n",
    "    # gives the lower left grid point in the grid search\n",
    "    left_x,bottom_y = transform(p1,p2,volc_grid[associated_volcano][1],volc_grid[associated_volcano][0]) # p1,p2,lon,lat\n",
    "    # gives the left right, bottom, top of the grid\n",
    "    grid_bounds = [left_x, left_x+volc_grid[associated_volcano][2], bottom_y, bottom_y+volc_grid[associated_volcano][2]]\n",
    "    left, right = dem_data_dict[associated_volcano]['left'],dem_data_dict[associated_volcano]['right']\n",
    "    bottom, top = dem_data_dict[associated_volcano]['bottom'],dem_data_dict[associated_volcano]['top']\n",
    "    center_x, center_y = transform(p1,p2,info[1],info[0])\n",
    "    \n",
    "    #scatter plot of locations\n",
    "    loc_x,loc_y = [],[]\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    fig,ax = plt.subplots(1,1,figsize=(6,8),dpi = 200)\n",
    "    a = ax.imshow(data,extent=[left, right, bottom, top],cmap='gist_earth')\n",
    "    b = ax.imshow(aspect,extent=[left, right, bottom, top],cmap='bone', alpha = .2)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], marker='*', color='w', label='center of volcano',\n",
    "                              markerfacecolor='r', markersize=15),\n",
    "                       Line2D([0], [0], marker='.', color='w', label='estimated event location',\n",
    "                              markerfacecolor='k', markersize=15)]\n",
    "\n",
    "    for i, ii in enumerate(evt_id):\n",
    "        loc_lon,loc_lat = transform(p1,p2,lons[i],lats[i])\n",
    "        loc_x.append(loc_lon)\n",
    "        loc_y.append(loc_lat)\n",
    "        if left+info[3]<loc_lon<right-info[4] and bottom+info[5]<loc_lat<top-info[6]:\n",
    "            ax.scatter(loc_lon,loc_lat,s = 5, c='k', marker=\".\")\n",
    "    ax.scatter(center_x, center_y, s=120,marker='*',c='r')\n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])\n",
    "    if n == 'Mt_Rainier' or n =='Mt_Hood':\n",
    "        ax.set_title(n.split('_')[0]+' '+n.split('_')[1])\n",
    "    else:\n",
    "         ax.set_title(n.split('_')[0]+' '+n.split('_')[1]+' '+n.split('_')[2])   \n",
    "    ax.set_xlim(left+info[3],right-info[4])\n",
    "    ax.set_ylim(bottom+info[5]+1000,top-info[6])\n",
    "    ax.legend(handles=legend_elements, loc = 'upper right', fontsize = 12)\n",
    "    plt.savefig('locs_'+n+'.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
