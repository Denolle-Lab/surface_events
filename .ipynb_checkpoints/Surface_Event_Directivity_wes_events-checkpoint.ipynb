{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/wsd01/pnwstore/')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Figure\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.geodetics import *\n",
    "from obspy.signal.cross_correlation import *\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "import requests\n",
    "import glob\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "import richdem as rd\n",
    "from pathlib import Path\n",
    "from pyproj import Proj,transform,Geod\n",
    "import os \n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Parameters\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# define clients to download the station data\n",
    "client = WaveformClient()\n",
    "client2 = Client('IRIS')\n",
    "\n",
    "t_before = 120 #number of seconds before pick time\n",
    "t_after = 120 #number of seconds after pick time\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 30 #window length of the signal\n",
    "pr = 98 #percentile\n",
    "thr = 12 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "v_s = 1000 #shear wave velocity at the surface\n",
    "\n",
    "# range of dates that we are looking at\n",
    "t_beginning = UTCDateTime(2001,1,1,0,0,0) \n",
    "t_end = UTCDateTime(2021,12,31,23,59)\n",
    "\n",
    "smooth_length = 20 # constant for smoothing the waveform envelopes\n",
    "low_cut = 2 #low frequency threshold\n",
    "high_cut = 12 #high frequency threshold\n",
    "az_thr = 1000 #threshold of distance in meters from source location\n",
    "step = 100 #step every 100 m\n",
    "t_step = 1 #step every second\n",
    "ratio = 5.6915196 #used to define the grid \n",
    "colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3 # radius of the earth\n",
    "\n",
    "\n",
    "# ## Define Functions\n",
    "\n",
    "# In[ ]:\n",
    "def pick_time(time, ref_env, data_env_dict, st, t_diff, t_before, fs):\n",
    "    # time; picktime from the PNSN\n",
    "    # ref_env; reference envelope\n",
    "    # data_env_dict; dictionary of the envelope data, key is the network+station\n",
    "    # st; stream of traces of waveforms\n",
    "    pick_times,offsets, starttimes = [],[],[]\n",
    "    for i,key in enumerate(data_env_dict):\n",
    "        starttimes.append(st[i].stats.starttime) \n",
    "        xcor = correlate(data_env_dict[key],ref_env,int(50*fs))\n",
    "        index = np.argmax(xcor)\n",
    "        cc = round(xcor[index],9) #correlation coefficient\n",
    "        shift = 50*fs-index #how much it is shifted from the reference envelope in seconds\n",
    "        a = shift/fs\n",
    "        offset_time = time - a # shift from one envelope to the reference envelope in seconds\n",
    "        offsets.append(offset_time) # number of seconds from the beginning of the trace\n",
    "        pick_times.append(offset_time + 120)\n",
    "    return pick_times, offsets, starttimes\n",
    "\n",
    "def shift(offsets, starttimes, t_diff):\n",
    "    shifts, vals =[],[]\n",
    "    for i,ii in enumerate(t_diff):\n",
    "        t_shift = offsets[i]-min(offsets)\n",
    "        print(t_shift+1)\n",
    "        vals.append(t_shift+120)\n",
    "        shifts.append(t_shift)\n",
    "    return shifts, vals\n",
    "\n",
    "\n",
    "# #define a function that calculates picktimes at each station\n",
    "# def pick_time(time, ref_env, data_env_dict, st, t_diff, t_before, fs):\n",
    "#     # time; picktime from the PNSN\n",
    "#     # ref_env; reference envelope\n",
    "#     # data_env_dict; dictionary of the envelope data, key is the network+station\n",
    "#     # st; stream of \n",
    "    \n",
    "#     pick_times,offsets, starttimes = [],[],[]\n",
    "#     for i,key in enumerate(data_env_dict):\n",
    "#         starttimes.append(st[i].stats.starttime)\n",
    "#         xcor = correlate(data_env_dict[key],ref_env,int(50*fs))\n",
    "#         index = np.argmax(xcor)\n",
    "#         cc = round(xcor[index],9) #correlation coefficient\n",
    "#         shift = 50*fs-index #how much it is shifted from the reference envelope\n",
    "#         print(shift/fs)\n",
    "#         offset_time = time - shift/fs # shift from one envelope to the reference envelope in seconds\n",
    "#         offsets.append(offset_time) # number of seconds from the beginning of the trace\n",
    "#         pick_times.append(offset_time + 120)\n",
    "#     return pick_times, offsets, starttimes\n",
    "    \n",
    "# def shift(offsets, starttimes, t_diff):\n",
    "#     shifts, vals =[],[]\n",
    "#     for i,ii in enumerate(t_diff):\n",
    "#         t_shift = offsets[i]-min(offsets)\n",
    "#         vals.append((-1*t_diff[ii])+t_shift)\n",
    "#         shifts.append(t_shift)\n",
    "#     return shifts, vals\n",
    "\n",
    "# define functon that resamples the data\n",
    "def resample(st, fs):\n",
    "    for i in st:\n",
    "        i.detrend(type='demean')\n",
    "        i.taper(0.05)\n",
    "        i.resample(fs)   \n",
    "    return st\n",
    "\n",
    "# define function to fit data to\n",
    "def test_func(theta, a,theta0, c):\n",
    "    return a * np.cos(theta-theta0)+c\n",
    "\n",
    "# define a function to make plots of weighted data\n",
    "def weight_data(x_data,y_data,weight,test_func,v_s,stas):    \n",
    "    #weighting the data\n",
    "    tempx, tempy = [],[]\n",
    "    for i,ii in enumerate(x_data):\n",
    "        tempx.append([])\n",
    "        tempx[i].append([ii for l in range(0,weight[i])])\n",
    "        tempy.append([])\n",
    "        tempy[i].append([y_data[i] for l in range(0,weight[i])])   \n",
    "    weighted_x = sum(sum(tempx, []),[])\n",
    "    weighted_y = sum(sum(tempy, []),[])\n",
    "   \n",
    "    #optimizing parameters to fit weighted data to test_function\n",
    "    params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(weighted_x), weighted_y, p0=None)\n",
    "    d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "    if params[0]<0:\n",
    "        direction = params[1]+pi \n",
    "    else:\n",
    "        direction = params[1]   \n",
    "    fmax = max(d)\n",
    "    fmin = min(d)\n",
    "    v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "    return v, direction, d\n",
    "\n",
    "# define function to predict synthetic arrival times\n",
    "def travel_time(t0, x, y, vs, sta_x, sta_y):\n",
    "    dist = np.sqrt((sta_x - x)**2 + (sta_y - y)**2)\n",
    "    tt = t0 + dist/vs\n",
    "    return tt\n",
    "\n",
    "# define function to compute residual sum of squares\n",
    "def error(synth_arrivals,arrivals, weight):\n",
    "    res = (arrivals - synth_arrivals)* weight \n",
    "    res_sqr = res**2\n",
    "    mse = np.mean(res_sqr)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# define function to iterate through grid and calculate travel time residuals\n",
    "def gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals, weight):\n",
    "    rss_mat = np.zeros((len(t0),len(x_vect),len(y_vect)))\n",
    "    rss_mat[:,:,:] = np.nan\n",
    "    for i in range(len(t0)):\n",
    "        for j in range(len(x_vect)):\n",
    "            for k in range(len(y_vect)):\n",
    "                synth_arrivals = []\n",
    "                for h in range(len(sta_x)):\n",
    "                    tt = travel_time(t0[i],x_vect[j],y_vect[k],vs,sta_x[h],sta_y[h])\n",
    "                    synth_arrivals.append(tt)\n",
    "                rss = error(np.array(synth_arrivals),np.array(arrivals), np.array(weight))\n",
    "                rss_mat[i,j,k] = rss\n",
    "    return rss_mat\n",
    "\n",
    "# define function to find lower-left corner of grid and grid size based on height of volcano\n",
    "def start_latlon(elevation, ratio, center_lat, center_lon):\n",
    "    side_length = elevation * ratio\n",
    "    l = side_length/2\n",
    "    hypotenuse = l*np.sqrt(2)\n",
    "    d = distance.geodesic(meters = hypotenuse)\n",
    "    start_lat = d.destination(point=[center_lat,center_lon], bearing=225)[0]\n",
    "    start_lon = d.destination(point=[center_lat,center_lon], bearing=225)[1]\n",
    "    return start_lat, start_lon, side_length\n",
    "\n",
    "# define function to convert the location index into latitude and longitude\n",
    "def location(x_dist, y_dist, start_lat, start_lon):\n",
    "    bearing = 90-np.rad2deg(np.arctan(y_dist/x_dist))\n",
    "    dist = np.sqrt((x_dist)**2 + (y_dist)**2)\n",
    "    d = distance.geodesic(meters = dist)\n",
    "    loc_lat = d.destination(point=[start_lat,start_lon], bearing=bearing)[0]\n",
    "    loc_lon = d.destination(point=[start_lat,start_lon], bearing=bearing)[1]\n",
    "    return loc_lat, loc_lon, d\n",
    "\n",
    "# define function to find diameter in meters of the error on the location\n",
    "def error_diameter(new_array):\n",
    "    min_idx = np.min(new_array[:,1])\n",
    "    max_idx = np.max(new_array[:,1])\n",
    "    difference = max_idx-min_idx\n",
    "    diameter_m = difference*1000\n",
    "    return diameter_m \n",
    "\n",
    "\n",
    "# ##  Import and organize metadata\n",
    "\n",
    "# ### 1. Volcano Data (network and station, labeled with volcano name)\n",
    "\n",
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('Data/Volcano_Metadata_50km.csv')\n",
    "df_xd = pd.read_csv('Data/XD_Metadata_50km.csv')\n",
    "\n",
    "# center latitude, center longitude, elevation(m), left_trim, right_trim, bottom_trim, top_trim \n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_St_Helens'] =[46.200472222222224,-122.18883611111112,2549, 10000, 10000, 17000, 15000]\n",
    "\n",
    "#Find the lower left corner and grid size based on volcano elevation\n",
    "# [start_lat = lower_left latitude of gridsearch square, start lon = lower left longitude of gridsearhc square, side length of grid search square]\n",
    "volc_grid = {}\n",
    "for volc in volc_lat_lon:\n",
    "    elevation = volc_lat_lon[volc][2]\n",
    "    center_lat = volc_lat_lon[volc][0]\n",
    "    center_lon = volc_lat_lon[volc][1]\n",
    "    start_lat, start_lon, side_length = start_latlon(elevation, ratio, center_lat, center_lon)\n",
    "    volc_grid[volc] = [start_lat, start_lon, side_length]\n",
    "\n",
    "#DEM data \n",
    "dem_data_dict = {}\n",
    "name = 'Mt_St_Helens'\n",
    "if volc_lat_lon[name][0]>46:\n",
    "    dem = rio.open('Data/DEM_data/'+str(name)+'/'+str(name)+'.tif') #washington volcanoes\n",
    "    dem_array = dem.read(1).astype('float64')\n",
    "    dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "    crs = dem.crs\n",
    "\n",
    "dem_data_dict[name]={'data':dem_array, 'crs':crs, 'left':dem.bounds[0], 'right':dem.bounds[2], 'bottom':dem.bounds[1], 'top':dem.bounds[3]}\n",
    "\n",
    "lat_lon_dict = {}\n",
    "lat_lon_dict['Mt_St_Helens']={'tick_lons':[-122.10,-122.15,-122.2,-122.25],\n",
    "\n",
    "# ### 3. Surface Event Data from PNSN\n",
    "\n",
    "df3 = pd.read_csv('Data/PNSN_Pick_Label.csv')\n",
    "label = df3['Label'].values.tolist()\n",
    "surface_label = df3[df3['Label']== 'su']['Label'].values.tolist()\n",
    "net_temp = df3[df3['Label']== 'su']['Network'].values.tolist()\n",
    "sta_temp = df3[df3['Label']== 'su']['Station'].values.tolist()\n",
    "evt_id_temp = df3[df3['Label']== 'su']['Event_ID'].values.tolist()\n",
    "start_time_temp = df3[df3['Label']== 'su']['Picktime'].values.tolist()                               \n",
    "\n",
    "net,sta,evt_id,start_time = [],[],[],[]\n",
    "for i,ii in enumerate(start_time_temp):\n",
    "    if t_beginning<UTCDateTime(ii)<t_end:\n",
    "        net.append(net_temp[i])\n",
    "        sta.append(sta_temp[i])\n",
    "        evt_id.append(evt_id_temp[i])\n",
    "        start_time.append(ii)\n",
    "\n",
    "all_stas = set(sta)\n",
    "                              \n",
    "# DEM data\n",
    "associated_volcano = 'Mt_St_Helens'\n",
    "\n",
    "crs = dem_data_dict[associated_volcano]['crs']\n",
    "data = dem_data_dict[associated_volcano]['data']\n",
    "volc = rd.rdarray(data, no_data=-9999)\n",
    "slope = rd.TerrainAttribute(volc,attrib = 'slope_riserun')\n",
    "info = volc_lat_lon[associated_volcano]\n",
    "p2 = Proj(crs,preserve_units=False)\n",
    "p1 = Proj(proj='latlong',preserve_units=False)\n",
    "# gives the lower left grid point in the grid search\n",
    "left_x,bottom_y = transform(p1,p2,volc_grid[associated_volcano][1],volc_grid[associated_volcano][0]) # p1,p2,lon,lat\n",
    "# gives the left right, bottom, top of the grid\n",
    "grid_bounds = [left_x, left_x+volc_grid[associated_volcano][2], bottom_y, bottom_y+volc_grid[associated_volcano][2]]\n",
    "left, right = dem_data_dict[associated_volcano]['left'],dem_data_dict[associated_volcano]['right']\n",
    "bottom, top = dem_data_dict[associated_volcano]['bottom'],dem_data_dict[associated_volcano]['top']\n",
    "\n",
    "\n",
    "#getting lat and lon tick marks on the axis\n",
    "tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "ticks_x = []\n",
    "ticks_y = []\n",
    "for i in range(len(tick_lons)):\n",
    "    tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "    ticks_x.append(tick_x)\n",
    "    ticks_y.append(tick_y)\n",
    "    tick_lons[i]=str(tick_lons[i])\n",
    "    tick_lats[i]=str(tick_lats[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1601583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events in the catalog that have event ids in the PNSN catalog\n",
    "\n",
    "# gives indices of events in Wes' catalog\n",
    "with open(\"Data/event_ids_st.json\", 'r') as f:\n",
    "    event_ids_st = json.load(f)\n",
    "\n",
    "to_run = []\n",
    "for i in event_ids_st:\n",
    "    if i in evt_id:\n",
    "        index = evt_id.index(i)\n",
    "        to_run.append(index)      \n",
    "        \n",
    "# all events in wes' catalog to run\n",
    "with open(\"Data/wes_events_st.json\", 'r') as f:\n",
    "    wes_events_st = json.load(f)\n",
    "    \n",
    "# all events in wes' catalog to run\n",
    "with open(\"Data/event_labels_st.json\", 'r') as f:\n",
    "    event_labels_st = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373b4a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thr = 9\n",
    "for i,n in enumerate(to_run):\n",
    "    event_ID = str(evt_id[n])\n",
    "    label = event_labels_st[i]\n",
    "    time = UTCDateTime(start_time[n]) #picktime from PNSN\n",
    "    if event_ID == '1718543':\n",
    "        print(n)\n",
    "        print(event_ID)\n",
    "        print(time)\n",
    "        if net != 'CN' and evt_id[n]!=evt_id[n-1]:\n",
    "            reference = str(net[n]+'.'+sta[n])\n",
    "            try:\n",
    "                associated_volcano = df[df['Station']== sta[n]]['Volcano_Name'].values[0]\n",
    "            except: \n",
    "                pass\n",
    "            if associated_volcano == 'Mt_St_Helens':\n",
    "\n",
    "            #get info for stations within 50km of volcano that event ocurred at\n",
    "                stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "                networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "                latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "                longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "                elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "                #Get all waveforms for that event based on stations and times\n",
    "                bulk = [] \n",
    "                for m in range(0, len(networks)):\n",
    "                    bulk.append([networks[m], stations[m], '*', '*', time-t_before, time+t_after])\n",
    "                st = client.get_waveforms_bulk(bulk)\n",
    "\n",
    "                #remove unwanted data\n",
    "                for tr in st:\n",
    "                    cha = tr.stats.channel\n",
    "                    if cha[0:2] != 'BH' and cha[0:2] != 'EH' and cha[0:2] != 'HH':\n",
    "                        st.remove(tr)\n",
    "                    try:\n",
    "                        if len(tr.data)/tr.stats.sampling_rate < 239.9:\n",
    "                            st.remove(tr)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                #resampling the data to 40Hz for each trace\n",
    "                st = resample(st,fs) \n",
    "\n",
    "                #Plotting all traces for one event with channel z, SNR>10, and bandpasses between 2-12Hz\n",
    "                SNR,SNR_weight, no_weight,stas,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],[],{},{}\n",
    "                fig = plt.figure(figsize = (11,8), dpi=200)\n",
    "                fig.suptitle('evtID:UW'+ event_ID + associated_volcano)\n",
    "                plt.rcParams.update({'font.size': 20})\n",
    "                ax1 = plt.subplot(1,1,1)\n",
    "                iplot = 0\n",
    "                for i,ii in enumerate(st):\n",
    "                    network = ii.stats.network\n",
    "                    station = ii.stats.station\n",
    "                    ii.detrend(type = 'demean')\n",
    "                    ii.filter('bandpass',freqmin=2.0,freqmax=12.0,corners=2,zerophase=True)\n",
    "                    ii2 = ii.copy()\n",
    "                    ii2.filter('bandpass',freqmin=0.5,freqmax=2.0,corners=2,zerophase=True)\n",
    "                    cha = ii.stats.channel\n",
    "                    starttime = ii.stats.starttime\n",
    "                    max_amp_time = np.argmax(ii.data)/fs\n",
    "                    signal_window = ii.copy()\n",
    "                    noise_window = ii.copy()\n",
    "                    signal_window.trim(starttime + t_before - 20 +20 , starttime + t_before - 20 + window + 20)\n",
    "                    \n",
    "                    noise_window.trim(starttime + t_before - window -15, starttime + t_before - 15)\n",
    "                    snr = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                                   / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "\n",
    "                    if cha[-1] == 'Z' and snr>thr and 100<max_amp_time<200 and station != 'CINE' and station != 'PUPY' and station != 'TDL':\n",
    "                        t = ii.times()\n",
    "                        t_diff[network+'.'+station] = starttime-time \n",
    "                        # enveloping the data \n",
    "                        b,e = 0,239\n",
    "                        data_envelope = obspy.signal.filter.envelope(ii2.data[70*fs:170*fs])\n",
    "                        data_envelope /= np.max(data_envelope)\n",
    "                        \n",
    "                        # finding the time of max amplitude of each event\n",
    "                        max_amp_times.append(max_amp_time)\n",
    "                        max_amp = np.max(ii.data)      \n",
    "                        # creating envelope data dictionary to calculate picktimes\n",
    "                        data_envelope = obspy.signal.util.smooth(data_envelope,20)\n",
    "                        data_env_dict[network+'.'+station]= data_envelope\n",
    "                        ax1.plot(t[b*fs:e*fs],ii2.data[b*fs:e*fs]/np.max(np.abs(ii2.data))+iplot*1.5)\n",
    "                        ax1.plot(t[70*fs:170*fs], data_envelope+(iplot*1.5), color = 'k')\n",
    "                        ax1.set_xlabel('time (seconds)', fontsize = 10)\n",
    "                        plt.xticks(fontsize = 10)\n",
    "                        ax1.set_xlim([b,e])\n",
    "                        ax1.set_yticks([])\n",
    "                        plt.text(t[e*fs], iplot*1.5, 'SNR:'+str(int(snr)), fontsize = 15)\n",
    "                        plt.text(t[b*fs], (iplot*1.5)+0.2, station, fontsize = 15)\n",
    "                        plt.grid(True)\n",
    "                        iplot = iplot+1\n",
    "                        stas.append(ii.stats.station)\n",
    "                        nets.append(ii.stats.network)\n",
    "                        SNR.append(snr)\n",
    "                        SNR_weight.append(int(snr))\n",
    "                        no_weight.append(1)\n",
    "                    else:\n",
    "                        st.remove(ii)\n",
    "                        \n",
    "                index = 0\n",
    "                for i in range(len(stas)):\n",
    "                    if stas[i] == stas[i-1]:\n",
    "                        net_sta = nets[i]+'.'+stas[i]\n",
    "                        index = stas.index(stas[i])\n",
    "                        \n",
    "                if index != 0:        \n",
    "                    del stas[index]\n",
    "                    del nets[index]\n",
    "                    del SNR[index]\n",
    "                    del SNR_weight[index]\n",
    "                    del no_weight[index]\n",
    "#                     del t_diff[net_sta]\n",
    "                \n",
    "\n",
    "                if len(st)<4:  \n",
    "                    continue\n",
    "\n",
    "                # get peak frequency of each event\n",
    "                # read and preprocess data\n",
    "                st.taper(max_percentage=0.01,max_length=20)\n",
    "                st.trim(starttime=time-20,endtime=time+30) \n",
    "\n",
    "                lats, lons, elevs, r, theta = ([] for i in range(5))\n",
    "                \n",
    "                ref_index = SNR.index(np.max(SNR))\n",
    "                ref = str(nets[ref_index]+'.'+stas[ref_index])\n",
    "                try:\n",
    "                    ref_env = data_env_dict[reference]\n",
    "                except:\n",
    "                    print(ref)\n",
    "                    ref_env = data_env_dict[ref]\n",
    "                    \n",
    "                ####################################################################################################\n",
    "\n",
    "                # calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "                pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs)#calculate picktimes\n",
    "                \n",
    "                shifts, vals = shift(offsets, starttimes, t_diff)\n",
    "\n",
    "                print('offsets', offsets)\n",
    "                print('starttimes', starttimes)\n",
    "                print('shifts', shifts)\n",
    "                print('vals', vals)\n",
    "                ####################################################################################################\n",
    "                \n",
    "                iplot = 0 \n",
    "                durations = []\n",
    "                for i in range(len(stas)):\n",
    "                    max_amp_time = max_amp_times[i]\n",
    "                    duration = (max_amp_time-vals[i])*2\n",
    "                    durations.append(duration)\n",
    "                    ax1.vlines(vals[i], ymin = iplot*1.5-1, ymax = iplot*1.5+1, color = colors[i])\n",
    "                    a = stations.index(stas[i])\n",
    "                    lats.append(latitudes[a])\n",
    "                    lons.append(longitudes[a])\n",
    "                    elevs.append(elevations[a])\n",
    "                    iplot = iplot+1\n",
    "                avg_duration = np.mean(durations)\n",
    "            #plt.savefig('./Analysis_Data/st_events_Figs/_'+event_ID+'wiggles'+'.png')\n",
    "            \n",
    "        \n",
    "\n",
    "            # make plot of spectra\n",
    "            char_freq, sharp_weight= [],[]\n",
    "            fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "            \n",
    "            matplotlib.rc('xtick', labelsize=10)\n",
    "            for i in range(len(stas)):\n",
    "                data = st.select(station=stas[i],component=\"Z\")[0].data*100\n",
    "                f,psd=scipy.signal.welch(data,fs=st[0].stats.sampling_rate,nperseg=81,noverlap=1)\n",
    "                #just get the frequencies within the filter band\n",
    "                above_low_cut = [f>low_cut]\n",
    "                below_high_cut = [f<high_cut]\n",
    "                in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "                f = f[in_band]\n",
    "                psd = psd[in_band]\n",
    "\n",
    "                # calculate characteristic frequency and report\n",
    "                char_freq_max = f[np.argmax(psd)]\n",
    "                char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "                psd_cumsum = np.cumsum(psd)\n",
    "                psd_sum = np.sum(psd)\n",
    "                char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "                char_freq.append(char_freq_mean)\n",
    "\n",
    "                plt.rcParams.update({'font.size': 10})\n",
    "                plt.yticks(fontsize = 10)\n",
    "                ax.plot(f,psd,label=stas[i],linewidth=1.5)\n",
    "                ax.set_xscale('log')\n",
    "                ax.set_yscale('log')\n",
    "                ax.tick_params(axis = 'x', which = 'both', labelsize = 10)\n",
    "                ax.grid('True')\n",
    "                plt.xticks(fontsize = 10)\n",
    "                ax.set_xlabel('Frequency [Hz]', fontsize = 10)\n",
    "                ax.set_ylabel('PSD [$(mm/s)^2$/Hz]', fontsize = 10)\n",
    "                ax.vlines(char_freq_mean,ymin=np.min(psd)/10,ymax=np.max(psd)*10,linestyle=\"--\",colors=colors[i])\n",
    "\n",
    "                # weighting the data by the spikiness of the PSD vs frequency graphs\n",
    "                ratio = (np.mean(psd)/np.max(psd))\n",
    "                sharp_weight.append(int(1/(ratio**2)*20))\n",
    "\n",
    "            ax.legend(fontsize = 10) \n",
    "            #plt.savefig('./Analysis_Data/st_events_Figs/_'+event_ID+'psd'+'.png')\n",
    "            \n",
    "            # input necessary data for grid search\n",
    "            arrivals = shifts\n",
    "            sta_lats = lats\n",
    "            sta_lons= lons\n",
    "\n",
    "            # define grid origin in lat,lon and grid dimensions in m\n",
    "            lat_start = volc_grid[associated_volcano][0]\n",
    "            lon_start = volc_grid[associated_volcano][1]\n",
    "            side_length = volc_grid[associated_volcano][2]\n",
    "\n",
    "            # create the grid of locations\n",
    "            sta_x = []\n",
    "            sta_y = []\n",
    "            for i in range(len(sta_lats)):\n",
    "                x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "                y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "                sta_x.append(x_dist)\n",
    "                sta_y.append(y_dist)\n",
    "            x_vect = np.arange(0, side_length, step)\n",
    "            y_vect = np.arange(0, side_length, step)\n",
    "            t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "\n",
    "            # carry out the gridsearch weighted by SNR\n",
    "            weight = np.array(SNR_weight)/np.max(SNR_weight)\n",
    "            rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "            loc_idx_snr = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "\n",
    "            # gridsearch with no weight\n",
    "            weight = [1 for i in range(len(SNR_weight))]\n",
    "            rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "            loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "\n",
    "            # gridsearch weighted with SNR and Slope\n",
    "\n",
    "            slen = round(volc_grid['Mt_St_Helens'][2]/10)\n",
    "            a = int((left_x-left)/10)\n",
    "            b = a+slen\n",
    "            c = (slope.shape[0] - int((bottom_y-bottom)/10))-slen\n",
    "            d = slope.shape[0] - int((bottom_y-bottom)/10)\n",
    "\n",
    "            x = np.arange(a,b,1)\n",
    "            y = np.arange(c,d,1)\n",
    "\n",
    "            x2 = np.arange(a,b,10) # every 100m\n",
    "            y2 = np.arange(c,d,10) # every 100m\n",
    "\n",
    "            slope_data = np.array(slope[c:d,a:b])\n",
    "            \n",
    "            slope_data[slope_data < 1] = 1\n",
    "            slope_data[slope_data > 90] = 80\n",
    "\n",
    "            slope_norm = 1/slope_data\n",
    "\n",
    "            #slope_norm1 = slope_data/np.max(slope_data)\n",
    "\n",
    "            slope_interp_mat = RectBivariateSpline(y,x,slope_norm, s = 0)\n",
    "            interp = slope_interp_mat(x2,y2)/np.max(slope_interp_mat(x2,y2))*0.1+.9\n",
    "\n",
    "            rss_mat_slope = np.multiply(rss_mat[loc_idx[0],:,:],interp)\n",
    "            loc_idx_slope = np.unravel_index([np.argmin(rss_mat_slope)], rss_mat_slope.shape)\n",
    "            loc_lat_slope, loc_lon_slope, test_d = location(x_vect[loc_idx_slope[1]], y_vect[loc_idx_slope[2]], lat_start, lon_start)\n",
    "\n",
    "            # plot heatmap\n",
    "            fig,ax = plt.subplots(1,1,figsize=(8,8), dpi = 200)\n",
    "            ax.scatter(x_vect[loc_idx[1]],y_vect[loc_idx[2]],s=100,marker='*',c='r')\n",
    "            ax.scatter(x_vect[loc_idx_slope[1]],y_vect[loc_idx_slope[2]],s=100,marker='*',c='r')\n",
    "            im = ax.imshow(np.log10(rss_mat[loc_idx[0],:,:].T),origin=\"lower\",extent=[0,side_length,0,side_length])\n",
    "            ax.set_ylabel('(m)')\n",
    "            ax.set_ylabel('(m)')\n",
    "            cbar = plt.colorbar(im)\n",
    "            cbar.ax.tick_params()\n",
    "            cbar.set_label('RMS error on location', rotation=270)\n",
    "            #plt.savefig('heatmap'+ event_ID+associated_volcano+'.png')\n",
    "           \n",
    "            # find the latitude and longitude of the location index\n",
    "            loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "            err_thr = np.min(np.log10(rss_mat))+.05\n",
    "            thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "            diameter = error_diameter(thr_array)\n",
    "\n",
    "            # calculating azimuth for each station with respect to the middle of the volcano\n",
    "            for i in range(len(stas)):\n",
    "                u,b,c = (gps2dist_azimuth(loc_lat_slope, loc_lon_slope, lats[i], lons[i], a=6378137.0, f=0.0033528106647474805))\n",
    "                r.append(u)\n",
    "                theta.append(b)\n",
    "\n",
    "            bin1,bin2,bin3 = [],[],[]\n",
    "            for i in theta:\n",
    "                if 0<=i<=120:\n",
    "                    bin1.append(i)\n",
    "                if 121<=i<=240:\n",
    "                    bin2.append(i)\n",
    "                if 241<=i<=360:\n",
    "                    bin3.append(i)\n",
    "\n",
    "            if bin1 == [] or bin2 == [] or bin3 == []:\n",
    "                continue\n",
    "\n",
    "            #manipulating the data\n",
    "            data = {'azimuth_deg':theta, 'freq':char_freq, 'station':stas, 'distance_m':r, \n",
    "                    'weight':sharp_weight, 'SNR':SNR, 'colors':colors[0:len(stas)]}\n",
    "            DF = pd.DataFrame(data, index = None)\n",
    "            DF2 = DF.sort_values('azimuth_deg')\n",
    "\n",
    "            #Taking out stations that are too close to the location when looking at azimuth \n",
    "            drops = []\n",
    "            for i in range(len(DF2)):\n",
    "                value = DF2.loc[i,'distance_m']\n",
    "                if value < az_thr:\n",
    "                    drops.append(i)\n",
    "            DF3 = DF2.drop(drops)\n",
    "            y_data =  DF3[\"freq\"].values.tolist()\n",
    "            Sta2 = DF3[\"station\"].values.tolist()\n",
    "            dist2 = DF3[\"distance_m\"].values.tolist()\n",
    "            spike_weight = DF3[\"weight\"].values.tolist()\n",
    "            SNR2 = DF3['SNR'].values.tolist()\n",
    "            colors2 = DF3['colors'].values.tolist()\n",
    "            x_data =  np.asarray(DF3[\"azimuth_deg\"].values.tolist())\n",
    "            x_points = np.linspace(0, 360, 100)\n",
    "\n",
    "            #optimizing parameters to fit data to test_function\n",
    "            params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(x_data), y_data, p0=None)\n",
    "            perr = np.sqrt(np.diag(params_covariance))\n",
    "            std_deviation = str(round(perr[0],9))+','+str(round(perr[1],9))+','+str(round(perr[2],9))\n",
    "            d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "            len_r = int(max(r))\n",
    "\n",
    "            if params[0]<0:\n",
    "                direction = params[1]+pi \n",
    "            else:\n",
    "                direction = params[1]\n",
    "\n",
    "            fmax = max(d)\n",
    "            fmin = min(d)\n",
    "            v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "\n",
    "            #convert the direction from polar to cartesian coordinates\n",
    "            dy = len_r*np.sin(direction)\n",
    "            dx = len_r*np.cos(direction)     \n",
    "\n",
    "            # weight the data\n",
    "            title = 'Sharpness'\n",
    "            v_sharp,direction_sharp,d_sharp = weight_data(x_data,y_data,sharp_weight,test_func,v_s,stas)\n",
    "            dy_sharp = len_r*np.sin(direction_sharp)\n",
    "            dx_sharp = len_r*np.cos(direction_sharp)    \n",
    "\n",
    "            title = 'SNR'\n",
    "            v_snr,direction_snr,d_snr = weight_data(x_data,y_data,SNR_weight,test_func,v_s,stas)  \n",
    "            dy_snr = len_r*np.sin(direction_snr)\n",
    "            dx_snr = len_r*np.cos(direction_snr) \n",
    "\n",
    "            fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "            fig.suptitle('Fitted Cosine Curves', fontsize = 20)       \n",
    "            ax.set_ylabel('characteristic frequency(Hz)', fontsize = 10)\n",
    "            ax.set_xlabel('azimuth(degrees)', fontsize = 10)\n",
    "            for i in range (0,len(Sta2)):\n",
    "                ax.scatter(x_data[i], y_data[i], s = (SNR_weight[i]**2),label=Sta2[i], color = colors2[i])\n",
    "            ax.plot(x_data,y_data, '--', label='rawdata')\n",
    "            ax.plot(x_points, d, label = 'original')\n",
    "            ax.plot(x_points, d_sharp, label = 'sharpness')\n",
    "            ax.plot(x_points, d_snr, label = 'snr')\n",
    "            ax.legend(loc='upper right', fontsize = 10)\n",
    "            plt.grid(True)\n",
    "            #plt.savefig('./Analysis_Data/st_events_Figs/_'+event_ID+'curves_freq_data.png')\n",
    "\n",
    "            # convert loc data onto the DEM data\n",
    "            contour_x,contour_y = np.meshgrid(left_x+x_vect,bottom_y+y_vect)\n",
    "            center_x, center_y = transform(p1,p2,info[1],info[0])\n",
    "            loc_x,loc_y=transform(p1,p2,loc_lon_slope,loc_lat_slope)\n",
    "            duration=avg_duration\n",
    "            length_factor = duration/100\n",
    "            length_factor = v_snr/(np.max(v_snr)*5)\n",
    "\n",
    "            data = dem_data_dict[associated_volcano]['data']\n",
    "            \n",
    "            fig,ax = plt.subplots(1,1,figsize=(8,11), dpi = 200)\n",
    "\n",
    "            dem = ax.imshow(data,extent=[left, right, bottom, top],cmap='gist_earth', alpha = 0.8)\n",
    "            contours = ax.contour(contour_x,contour_y,np.log10(rss_mat_slope[int(loc_idx_slope[0]),:,:].T),cmap='plasma', linewidths = 0.7)\n",
    "            topo_countours = ax.contour(data,levels = [2000,4000,6000,8000], extent=[left, right, bottom, top],origin=\"upper\", colors = 'k',linewidths = 0.4, alpha = 0.6)\n",
    "            plt.arrow(loc_x,loc_y,dy_snr*length_factor,dx_snr*length_factor, color='w', width=100, zorder = 4)\n",
    "            ax.scatter(loc_x, loc_y, s=150,marker='*',c='aqua', zorder = 5)\n",
    "\n",
    "            #plotting the stations on top of this as triangles\n",
    "            for i, ii in enumerate(stas):\n",
    "                sta_x,sta_y = transform(p1,p2,lons[i],lats[i])\n",
    "                if left+info[3]<sta_x<right-info[4] and bottom+info[5]<sta_y<top-info[6]:\n",
    "                    ax.plot(sta_x,sta_y, c='k', marker=\"^\")\n",
    "                    ax.text(sta_x,sta_y,ii, c='k', fontsize = 15)\n",
    "\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax1 = divider.append_axes('right', size='2%', pad=0.1)\n",
    "            ax.set_title('Location and Directivity', fontsize = 20)\n",
    "            ax.set_xlabel('longitudes(DD)', fontsize = 10)\n",
    "            ax.set_ylabel('latitudes(DD)', fontsize = 10)\n",
    "            ax.set_xticks(ticks_x)\n",
    "            ax.set_xticklabels(tick_lons, fontsize = 10)\n",
    "            ax.set_yticks(ticks_y)\n",
    "            ax.set_yticklabels(tick_lats, fontsize = 10)\n",
    "            ax.clabel(contours, contours.levels, fontsize = 15, inline = True, inline_spacing = 0.5)\n",
    "\n",
    "            cbar = plt.colorbar(dem, cax=cax1)\n",
    "            cbar.ax.tick_params(labelsize=10)\n",
    "            cbar.set_label('Elevation(ft)\\n', rotation=270, labelpad = 13, fontsize = 10)\n",
    "            ax.set_xlim(left+info[3],right-info[4])\n",
    "            ax.set_ylim(bottom+info[5],top-info[6])\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            #getting lat and lon tick marks on the axis\n",
    "            tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "            tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "            ticks_x = []\n",
    "            ticks_y = []\n",
    "            for i in range(len(tick_lons)):\n",
    "                tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "                ticks_x.append(tick_x)\n",
    "                ticks_y.append(tick_y)\n",
    "                tick_lons[i]=str(tick_lons[i])\n",
    "                tick_lats[i]=str(tick_lats[i])\n",
    "            \n",
    "            #plt.savefig('./Analysis_Data/st_events_Figs/_'+event_ID +'loc_direction.png',bbox_inches=\"tight\")\n",
    "            \n",
    "            # make a dataframe of the data\n",
    "#             evt_data = evt_data.append({'event_ID':event_ID, \n",
    "#                         'location_latitude': loc_lat_slope,\n",
    "#                         'location_longitude': loc_lon_slope,\n",
    "#                         'location_uncertainty(m)':diameter/10,\n",
    "#                         'origin_time': min(offsets)-int(loc_idx[0]),\n",
    "#                         'direction(degrees)':np.rad2deg(direction),\n",
    "#                         'direction_sharpness(degrees)':np.rad2deg(direction_sharp),\n",
    "#                         'direction_snr(degrees)':np.rad2deg(direction_snr),\n",
    "#                         'duration(sec)':avg_duration,\n",
    "#                         'params_std_deviation':std_deviation, \n",
    "#                         'velocity(m/s)':v, \n",
    "#                         'number_of_stations':len(stas)}, ignore_index = True)\n",
    "\n",
    "#                 dict_temp = {}\n",
    "#                 for i in range(len(stas)):\n",
    "#                     dict_temp[stas[i]] = char_freq[i]   \n",
    "#                 sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n",
    "\n",
    "#                 evt_data.to_csv('~/surface_events/Analysis_Data/Event_Data_St_Helens.csv', index=False)\n",
    "#                 sta_freq.to_csv('~/surface_events/Analysis_Data/Station_frequency_data_St_Helens.csv', index=False)\n",
    "#     except:\n",
    "#         reject_evts = reject_evts.append({'event_ID':[event_ID]}, ignore_index = True)\n",
    "#         reject_evts.to_csv('~/surface_events/Analysis_Data/Rejects_St_Helens.csv', index=False)\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b33e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pick_time(time, ref_env, data_env_dict, st, t_diff, t_before, fs):\n",
    "    # time; picktime from the PNSN\n",
    "    # ref_env; reference envelope\n",
    "    # data_env_dict; dictionary of the envelope data, key is the network+station\n",
    "    # st; stream of traces of waveforms\n",
    "    pick_times,offsets, starttimes = [],[],[]\n",
    "    for i,key in enumerate(data_env_dict):\n",
    "        starttimes.append(st[i].stats.starttime) \n",
    "        xcor = correlate(data_env_dict[key],ref_env,int(50*fs))\n",
    "        index = np.argmax(xcor)\n",
    "        cc = round(xcor[index],9) #correlation coefficient\n",
    "        shift = 50*fs-index #how much it is shifted from the reference envelope in seconds\n",
    "        a = shift/fs\n",
    "        offset_time = time - a # shift from one envelope to the reference envelope in seconds\n",
    "        offsets.append(offset_time) # number of seconds from the beginning of the trace\n",
    "        pick_times.append(offset_time + 120)\n",
    "    return pick_times, offsets, starttimes\n",
    "\n",
    "def shift(offsets, starttimes, t_diff):\n",
    "    shifts, vals =[],[]\n",
    "    for i,ii in enumerate(t_diff):\n",
    "        t_shift = offsets[i]-min(offsets)\n",
    "        print(t_shift+1)\n",
    "       # vals.append((-1*t_diff[ii])+t_shift)\n",
    "        vals.append(t_shift+120)\n",
    "        shifts.append(t_shift)\n",
    "    return shifts, vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift(offsets, starttimes, t_diff):\n",
    "    shifts, vals =[],[]\n",
    "    for i,ii in enumerate(t_diff):\n",
    "        t_shift = offsets[i]-min(offsets)\n",
    "        print(t_shift+1)\n",
    "       # vals.append((-1*t_diff[ii])+t_shift)\n",
    "        vals.append(t_shift+120)\n",
    "        shifts.append(t_shift)\n",
    "    return shifts, vals\n",
    "\n",
    "shifts, vals = shift(offsets, starttimes, t_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908e4d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making a final figure of the map and the waveforms with the label\n",
    "\n",
    "thrs = [9,13,13,15,16]\n",
    "for i,n in enumerate(to_run):\n",
    "    event_ID = str(evt_id[n])\n",
    "    label = event_labels_st[i]\n",
    "    time = UTCDateTime(start_time[n]) #picktime from PNSN\n",
    "    thr = thrs[i]\n",
    "    if net != 'CN' and evt_id[n]!=evt_id[n-1]:\n",
    "        reference = str(net[n]+'.'+sta[n])\n",
    "        try:\n",
    "            associated_volcano = df[df['Station']== sta[n]]['Volcano_Name'].values[0]\n",
    "        except: \n",
    "            pass\n",
    "        if associated_volcano == 'Mt_St_Helens':\n",
    "\n",
    "        #get info for stations within 50km of volcano that event ocurred at\n",
    "            stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "            networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "            latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "            longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "            elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "            #Get all waveforms for that event based on stations and times\n",
    "            bulk = [] \n",
    "            for m in range(0, len(networks)):\n",
    "                bulk.append([networks[m], stations[m], '*', '*', time-t_before, time+t_after])\n",
    "            st = client.get_waveforms_bulk(bulk)\n",
    "\n",
    "            #remove unwanted data\n",
    "            for tr in st:\n",
    "                cha = tr.stats.channel\n",
    "                if cha[0:2] != 'BH' and cha[0:2] != 'EH' and cha[0:2] != 'HH':\n",
    "                    st.remove(tr)\n",
    "                try:\n",
    "                    if len(tr.data)/tr.stats.sampling_rate < 239.9:\n",
    "                        st.remove(tr)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            #resampling the data to 40Hz for each trace\n",
    "            st = resample(st,fs) \n",
    "\n",
    "            #Plotting all traces for one event with channel z, SNR>10, and bandpasses between 2-12Hz\n",
    "            SNR,SNR_weight, no_weight,stas,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],[],{},{}\n",
    "\n",
    "            fig, (ax1, ax) = plt.subplots(1, 2, figsize = (14,6), dpi = 200)\n",
    "            if event_ID == '1718543':\n",
    "                fig.suptitle('evtID:UW'+ event_ID +' '+ associated_volcano+'   '+label[0:43]+'\\n'+label[43:], fontsize = 15)\n",
    "            else:                   \n",
    "                fig.suptitle('evtID:UW'+ event_ID +' '+ associated_volcano+' \\n  '+label, fontsize = 15)\n",
    "\n",
    "            iplot = 0\n",
    "            for i,ii in enumerate(st):\n",
    "                network = ii.stats.network\n",
    "                station = ii.stats.station\n",
    "                ii.detrend(type = 'demean')\n",
    "                ii.filter('bandpass',freqmin=2.0,freqmax=12.0,corners=2,zerophase=True)\n",
    "                ii2 = ii.copy()\n",
    "                ii2.filter('bandpass',freqmin=0.5,freqmax=2.0,corners=2,zerophase=True)\n",
    "                cha = ii.stats.channel\n",
    "                starttime = ii.stats.starttime\n",
    "                max_amp_time = np.argmax(ii.data)/fs\n",
    "                signal_window = ii.copy()\n",
    "                noise_window = ii.copy()\n",
    "                signal_window.trim(starttime + t_before - 20 +20 , starttime + t_before - 20 + window + 40)\n",
    "\n",
    "                noise_window.trim(starttime + t_before - window -15, starttime + t_before - 15)\n",
    "                snr = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                               / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "\n",
    "                if cha[-1] == 'Z' and snr>thr and 100<max_amp_time<200 and station != 'CINE' and station != 'PUPY' and station != 'TDL':\n",
    "                    t = ii.times()\n",
    "                    t_diff[network+'.'+station] = starttime-time \n",
    "                    # enveloping the data \n",
    "                    b,e = 0,239\n",
    "                    data_envelope = obspy.signal.filter.envelope(ii.data[70*fs:190*fs])\n",
    "                    data_envelope /= np.max(data_envelope)\n",
    "\n",
    "                    # finding the time of max amplitude of each event\n",
    "                    max_amp_times.append(max_amp_time)\n",
    "                    max_amp = np.max(ii.data)      \n",
    "                    # creating envelope data dictionary to calculate picktimes\n",
    "                    data_envelope = obspy.signal.util.smooth(data_envelope,20)\n",
    "                    data_env_dict[network+'.'+station]= data_envelope\n",
    "                    ax1.plot(t[b*fs:e*fs],ii.data[b*fs:e*fs]/np.max(np.abs(ii.data))+iplot*1.5)\n",
    "                    ax1.plot(t[70*fs:190*fs], data_envelope+(iplot*1.5), color = 'k')\n",
    "                    ax1.set_xlabel('time (seconds)', fontsize = 10)\n",
    "                    plt.xticks(fontsize = 10)\n",
    "                    ax1.set_xlim([b,e])\n",
    "                    ax1.set_yticks([])\n",
    "                    ax1.text(t[e*fs], iplot*1.5, 'SNR:'+str(int(snr)), fontsize = 15)\n",
    "                    ax1.text(t[b*fs], (iplot*1.5)+0.2, station, fontsize = 15)\n",
    "                    ax1.grid(True)\n",
    "                    iplot = iplot+1\n",
    "                    stas.append(ii.stats.station)\n",
    "                    nets.append(ii.stats.network)\n",
    "                    SNR.append(snr)\n",
    "                    SNR_weight.append(int(snr))\n",
    "                    no_weight.append(1)\n",
    "                else:\n",
    "                    st.remove(ii)\n",
    "\n",
    "            index = 0\n",
    "            for i in range(len(stas)):\n",
    "                if stas[i] == stas[i-1]:\n",
    "                    net_sta = nets[i]+'.'+stas[i]\n",
    "                    index = stas.index(stas[i])\n",
    "            if index != 0:        \n",
    "                del stas[index]\n",
    "                del nets[index]\n",
    "                del SNR[index]\n",
    "                del SNR_weight[index]\n",
    "                del no_weight[index]\n",
    "                \n",
    "#             ax1.text(t[b*fs],(iplot*1.5)+1, 'SNR Threshold:'+str(np.max(SNR)*0.75))\n",
    "\n",
    "            if len(st)<4:  \n",
    "                continue\n",
    "            \n",
    "            # read and preprocess data\n",
    "            st.taper(max_percentage=0.01,max_length=20)\n",
    "            st.trim(starttime=time-20,endtime=time+30) \n",
    "\n",
    "            lats, lons, elevs, r, theta = ([] for i in range(5))\n",
    "\n",
    "            ref_index = SNR.index(np.max(SNR))\n",
    "            ref = str(nets[ref_index]+'.'+stas[ref_index])\n",
    "            try:\n",
    "                ref_env = data_env_dict[reference]\n",
    "            except:\n",
    "                print(ref)\n",
    "                ref_env = data_env_dict[ref]\n",
    "\n",
    "            # calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "            pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs)#calculate picktimes\n",
    "            shifts, vals = shift(offsets, starttimes, t_diff)\n",
    "\n",
    "            iplot = 0 \n",
    "            durations = []\n",
    "            for i in range(len(stas)):\n",
    "                max_amp_time = max_amp_times[i]\n",
    "                duration = (max_amp_time-vals[i])*2\n",
    "                durations.append(duration)\n",
    "                ax1.vlines(vals[i], ymin = iplot*1.5-1, ymax = iplot*1.5+1, color = colors[i])\n",
    "                a = stations.index(stas[i])\n",
    "                lats.append(latitudes[a])\n",
    "                lons.append(longitudes[a])\n",
    "                elevs.append(elevations[a])\n",
    "                iplot = iplot+1\n",
    "            avg_duration = np.mean(durations)\n",
    "            \n",
    "        # make plot of spectra\n",
    "        char_freq, sharp_weight= [],[]\n",
    "        for i in range(len(stas)):\n",
    "            data = st.select(station=stas[i],component=\"Z\")[0].data*100\n",
    "            f,psd=scipy.signal.welch(data,fs=st[0].stats.sampling_rate,nperseg=81,noverlap=1)\n",
    "            #just get the frequencies within the filter band\n",
    "            above_low_cut = [f>low_cut]\n",
    "            below_high_cut = [f<high_cut]\n",
    "            in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "            f = f[in_band]\n",
    "            psd = psd[in_band]\n",
    "\n",
    "            # calculate characteristic frequency and report\n",
    "            char_freq_max = f[np.argmax(psd)]\n",
    "            char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "            psd_cumsum = np.cumsum(psd)\n",
    "            psd_sum = np.sum(psd)\n",
    "            char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "            char_freq.append(char_freq_mean)\n",
    "\n",
    "#                 plt.rcParams.update({'font.size': 10})\n",
    "#                 plt.yticks(fontsize = 10)\n",
    "#                 ax.plot(f,psd,label=stas[i],linewidth=1.5)\n",
    "#                 ax.set_xscale('log')\n",
    "#                 ax.set_yscale('log')\n",
    "#                 ax.tick_params(axis = 'x', which = 'both', labelsize = 10)\n",
    "#                 ax.grid('True')\n",
    "#                 plt.xticks(fontsize = 10)\n",
    "#                 ax.set_xlabel('Frequency [Hz]', fontsize = 10)\n",
    "#                 ax.set_ylabel('PSD [$(mm/s)^2$/Hz]', fontsize = 10)\n",
    "#                 ax.vlines(char_freq_mean,ymin=np.min(psd)/10,ymax=np.max(psd)*10,linestyle=\"--\",colors=colors[i])\n",
    "\n",
    "#             ax.legend(fontsize = 10) \n",
    "\n",
    "        # input necessary data for grid search\n",
    "        arrivals = shifts\n",
    "        sta_lats = lats\n",
    "        sta_lons= lons\n",
    "\n",
    "        # define grid origin in lat,lon and grid dimensions in m\n",
    "        lat_start = volc_grid[associated_volcano][0]\n",
    "        lon_start = volc_grid[associated_volcano][1]\n",
    "        side_length = volc_grid[associated_volcano][2]\n",
    "\n",
    "        # create the grid of locations\n",
    "        sta_x = []\n",
    "        sta_y = []\n",
    "        for i in range(len(sta_lats)):\n",
    "            x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "            y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "            sta_x.append(x_dist)\n",
    "            sta_y.append(y_dist)\n",
    "        x_vect = np.arange(0, side_length, step)\n",
    "        y_vect = np.arange(0, side_length, step)\n",
    "        t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "\n",
    "        # carry out the gridsearch weighted by SNR\n",
    "        weight = np.array(SNR_weight)/np.max(SNR_weight)\n",
    "        rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "        loc_idx_snr = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "\n",
    "        # gridsearch with no weight\n",
    "        weight = [1 for i in range(len(SNR_weight))]\n",
    "        rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "        loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "\n",
    "        # gridsearch weighted with SNR and Slope\n",
    "        slen = round(volc_grid['Mt_St_Helens'][2]/10)\n",
    "        a = int((left_x-left)/10)\n",
    "        b = a+slen\n",
    "        c = (slope.shape[0] - int((bottom_y-bottom)/10))-slen\n",
    "        d = slope.shape[0] - int((bottom_y-bottom)/10)\n",
    "\n",
    "        x = np.arange(a,b,1)\n",
    "        y = np.arange(c,d,1)\n",
    "\n",
    "        x2 = np.arange(a,b,10) # every 100m\n",
    "        y2 = np.arange(c,d,10) # every 100m\n",
    "\n",
    "        slope_data = np.array(slope[c:d,a:b])\n",
    "\n",
    "        slope_data[slope_data < 1] = 1\n",
    "        slope_data[slope_data > 90] = 80\n",
    "\n",
    "        slope_norm = 1/slope_data\n",
    "\n",
    "        slope_interp_mat = RectBivariateSpline(y,x,slope_norm, s = 0)\n",
    "        interp = slope_interp_mat(x2,y2)/np.max(slope_interp_mat(x2,y2))*0.1+.9\n",
    "\n",
    "        rss_mat_slope = np.multiply(rss_mat[loc_idx[0],:,:],interp)\n",
    "        loc_idx_slope = np.unravel_index([np.argmin(rss_mat_slope)], rss_mat_slope.shape)\n",
    "        loc_lat_slope, loc_lon_slope, test_d = location(x_vect[loc_idx_slope[1]], y_vect[loc_idx_slope[2]], lat_start, lon_start)\n",
    "           \n",
    "        # find the latitude and longitude of the location index\n",
    "        loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "        err_thr = np.min(np.log10(rss_mat))+.05\n",
    "        thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "        diameter = error_diameter(thr_array)\n",
    "\n",
    "        # calculating azimuth for each station with respect to the middle of the volcano\n",
    "        for i in range(len(stas)):\n",
    "            u,b,c = (gps2dist_azimuth(loc_lat_slope, loc_lon_slope, lats[i], lons[i], a=6378137.0, f=0.0033528106647474805))\n",
    "            r.append(u)\n",
    "            theta.append(b)\n",
    "\n",
    "        bin1,bin2,bin3 = [],[],[]\n",
    "        for i in theta:\n",
    "            if 0<=i<=120:\n",
    "                bin1.append(i)\n",
    "            if 121<=i<=240:\n",
    "                bin2.append(i)\n",
    "            if 241<=i<=360:\n",
    "                bin3.append(i)\n",
    "        print(bin1,bin2,bin3)\n",
    "        if bin1 == [] or bin2 == [] or bin3 == []:\n",
    "            continue\n",
    "\n",
    "        #manipulating the data\n",
    "        data = {'azimuth_deg':theta, 'freq':char_freq, 'station':stas, 'distance_m':r, 'SNR':SNR, 'colors':colors[0:len(stas)]}\n",
    "        DF = pd.DataFrame(data, index = None)\n",
    "        DF2 = DF.sort_values('azimuth_deg')\n",
    "\n",
    "        #Taking out stations that are too close to the location when looking at azimuth \n",
    "        drops = []\n",
    "        for i in range(len(DF2)):\n",
    "            value = DF2.loc[i,'distance_m']\n",
    "            if value < az_thr:\n",
    "                drops.append(i)\n",
    "        DF3 = DF2.drop(drops)\n",
    "        y_data =  DF3[\"freq\"].values.tolist()\n",
    "        Sta2 = DF3[\"station\"].values.tolist()\n",
    "        dist2 = DF3[\"distance_m\"].values.tolist()\n",
    "        SNR2 = DF3['SNR'].values.tolist()\n",
    "        colors2 = DF3['colors'].values.tolist()\n",
    "        x_data =  np.asarray(DF3[\"azimuth_deg\"].values.tolist())\n",
    "        x_points = np.linspace(0, 360, 100)\n",
    "\n",
    "        #optimizing parameters to fit data to test_function\n",
    "        params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(x_data), y_data, p0=None)\n",
    "        perr = np.sqrt(np.diag(params_covariance))\n",
    "        std_deviation = str(round(perr[0],9))+','+str(round(perr[1],9))+','+str(round(perr[2],9))\n",
    "        d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "        len_r = int(max(r))\n",
    "\n",
    "        if params[0]<0:\n",
    "            direction = params[1]+pi \n",
    "        else:\n",
    "            direction = params[1]\n",
    "\n",
    "        fmax = max(d)\n",
    "        fmin = min(d)\n",
    "        v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "\n",
    "        #convert the direction from polar to cartesian coordinates\n",
    "        dy = len_r*np.sin(direction)\n",
    "        dx = len_r*np.cos(direction)         \n",
    "\n",
    "        title = 'SNR'\n",
    "        v_snr,direction_snr,d_snr = weight_data(x_data,y_data,SNR_weight,test_func,v_s,stas)  \n",
    "        dy_snr = len_r*np.sin(direction_snr)\n",
    "        dx_snr = len_r*np.cos(direction_snr) \n",
    "\n",
    "#             fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "#             fig.suptitle('Fitted Cosine Curves', fontsize = 20)       \n",
    "#             ax.set_ylabel('characteristic frequency(Hz)', fontsize = 10)\n",
    "#             ax.set_xlabel('azimuth(degrees)', fontsize = 10)\n",
    "#             for i in range (0,len(Sta2)):\n",
    "#                 ax.scatter(x_data[i], y_data[i], s = (SNR_weight[i]**2),label=Sta2[i], color = colors2[i])\n",
    "#             ax.plot(x_data,y_data, '--', label='rawdata')\n",
    "#             ax.plot(x_points, d, label = 'original')\n",
    "#             ax.plot(x_points, d_sharp, label = 'sharpness')\n",
    "#             ax.plot(x_points, d_snr, label = 'snr')\n",
    "#             ax.legend(loc='upper right', fontsize = 10)\n",
    "#             plt.grid(True)\n",
    "        #plt.savefig('./Analysis_Data/st_events_Figs/_'+event_ID+'curves_freq_data.png')\n",
    "\n",
    "        # convert loc data onto the DEM data\n",
    "        contour_x,contour_y = np.meshgrid(left_x+x_vect,bottom_y+y_vect)\n",
    "        center_x, center_y = transform(p1,p2,info[1],info[0])\n",
    "        loc_x,loc_y=transform(p1,p2,loc_lon_slope,loc_lat_slope)\n",
    "        duration=avg_duration\n",
    "        length_factor = duration/100\n",
    "        length_factor = v_snr/(np.max(v_snr)*5)\n",
    "\n",
    "        data = dem_data_dict[associated_volcano]['data']\n",
    "\n",
    "        dem = ax.imshow(data,extent=[left, right, bottom, top],cmap='gist_earth', alpha = 0.8)\n",
    "        contours = ax.contour(contour_x,contour_y,np.log10(rss_mat_slope[int(loc_idx_slope[0]),:,:].T),cmap='plasma', linewidths = 0.7)\n",
    "        topo_countours = ax.contour(data,levels = [2000,4000,6000,8000], extent=[left, right, bottom, top],origin=\"upper\", colors = 'k',linewidths = 0.4, alpha = 0.6)\n",
    "        ax.arrow(loc_x,loc_y,dy_snr*length_factor,dx_snr*length_factor, color='w', width=100, zorder = 4)\n",
    "        ax.scatter(loc_x, loc_y, s=150,marker='*',c='aqua', zorder = 5)\n",
    "\n",
    "        #plotting the stations on top of this as triangles\n",
    "        for i, ii in enumerate(stas):\n",
    "            sta_x,sta_y = transform(p1,p2,lons[i],lats[i])\n",
    "            if left+info[3]<sta_x<right-info[4] and bottom+info[5]<sta_y<top-info[6]:\n",
    "                ax.plot(sta_x,sta_y, c='k', marker=\"^\")\n",
    "                ax.text(sta_x,sta_y,ii, c='k', fontsize = 15)\n",
    "\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax1 = divider.append_axes('right', size='2%', pad=0.1)\n",
    "        ax.set_title('Location and Directivity', fontsize = 20)\n",
    "        ax.set_xlabel('longitudes(DD)', fontsize = 10)\n",
    "        ax.set_ylabel('latitudes(DD)', fontsize = 10)\n",
    "        ax.set_xticks(ticks_x)\n",
    "        ax.set_xticklabels(tick_lons, fontsize = 10)\n",
    "        ax.set_yticks(ticks_y)\n",
    "        ax.set_yticklabels(tick_lats, fontsize = 10)\n",
    "        ax.clabel(contours, contours.levels, fontsize = 15, inline = True, inline_spacing = 0.5)\n",
    "\n",
    "        cbar = plt.colorbar(dem, cax=cax1)\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "        cbar.set_label('Elevation(ft)\\n', rotation=270, labelpad = 13, fontsize = 10)\n",
    "        ax.set_xlim(left+info[3],right-info[4])\n",
    "        ax.set_ylim(bottom+info[5],top-info[6])\n",
    "        plt.tight_layout()\n",
    "\n",
    "        #getting lat and lon tick marks on the axis\n",
    "        tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "        tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "        ticks_x = []\n",
    "        ticks_y = []\n",
    "        for i in range(len(tick_lons)):\n",
    "            tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "            ticks_x.append(tick_x)\n",
    "            ticks_y.append(tick_y)\n",
    "            tick_lons[i]=str(tick_lons[i])\n",
    "            tick_lats[i]=str(tick_lats[i])\n",
    "                \n",
    "            \n",
    "        plt.savefig('./Analysis_Data/st_events_finalfigs/_'+event_ID +'finalfigs.png',bbox_inches=\"tight\")\n",
    "\n",
    "# saving the figures into a zip file\n",
    "save_path = './Analysis_Data/st_events_finalfigs'\n",
    "image_list = []\n",
    "for file in os.listdir(save_path):\n",
    "    if file.endswith(\".png\"):\n",
    "        image_list.append(os.path.join(save_path, file))\n",
    "\n",
    "with ZipFile(os.path.join(save_path, 'labeled_event_figs.zip'), 'w') as zip:\n",
    "    for file in image_list:\n",
    "        zip.write(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
