{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f6cb81",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Surface Event Analysis\n",
    "###### This notebook analyzes surface event waveforms and calculates location, directivity, and velocity, it is updated from the previous as the gridsearch algorithm is weighted by slope\n",
    "###### Francesca Skene\n",
    "###### fskene@uw.edu\n",
    "###### Created: 7/22/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021dc6f",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf759fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/wsd01/pnwstore/')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.geodetics import *\n",
    "from obspy.signal.cross_correlation import *\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "import requests\n",
    "import glob\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "import richdem as rd\n",
    "from pathlib import Path\n",
    "from pyproj import Proj,transform,Geod\n",
    "import os \n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "import json\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30de3d",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clients to download the station data\n",
    "client = WaveformClient()\n",
    "client2 = Client('IRIS')\n",
    "\n",
    "t_before = 120 #number of seconds before pick time\n",
    "t_after = 120 #number of seconds after pick time\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 30 #window length of the signal\n",
    "pr = 98 #percentile\n",
    "thr = 12 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "v_s = 1000 #shear wave velocity at the surface\n",
    "\n",
    "# range of dates that we are looking at\n",
    "t_beginning = UTCDateTime(2001,1,1,0,0,0) \n",
    "t_end = UTCDateTime(2021,12,31,23,59)\n",
    "\n",
    "smooth_length = 5 # constant for smoothing the waveform envelopes\n",
    "low_cut = 2 #low frequency threshold\n",
    "high_cut = 12 #high frequency threshold\n",
    "az_thr = 1000 #threshold of distance in meters from source location\n",
    "step = 100 #step every 100 m\n",
    "t_step = 1 #step every second\n",
    "ratio = 5.6915196 #used to define the grid \n",
    "colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3 # radius of the earth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96853aa",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that calculates picktimes at each station\n",
    "def pick_time(time, ref_env, data_env_dict, st, t_diff, t_before, fs):\n",
    "    pick_times,offsets, starttimes = [],[],[]\n",
    "    for i,key in enumerate(data_env_dict):\n",
    "        starttimes.append(st[i].stats.starttime)\n",
    "        xcor = correlate(data_env_dict[key],ref_env,int(50*fs))\n",
    "        index = np.argmax(xcor)\n",
    "        cc = round(xcor[index],9) #correlation coefficient\n",
    "        shift = 50*fs-index #how much it is shifted from the reference envelope\n",
    "        offset_time = time - shift/fs\n",
    "        p = time - shift/fs  # p is the new phase pick for each station\n",
    "        pick_times.append(p+t_diff[key])\n",
    "        offsets.append(offset_time + t_diff[key])\n",
    "    return pick_times, offsets, starttimes\n",
    "    \n",
    "def shift(pick_times, offsets, starttimes, t_diff):\n",
    "    shifts, vals =[],[]\n",
    "    for i,ii in enumerate(t_diff):\n",
    "        t_shift = offsets[i]-min(offsets)\n",
    "        vals.append((-1*t_diff[ii])+t_shift)\n",
    "        shifts.append(t_shift)\n",
    "        #plt.vlines(val, ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "    return shifts, vals\n",
    "\n",
    "# define functon that resamples the data\n",
    "def resample(st, fs):\n",
    "    for i in st:\n",
    "        i.detrend(type='demean')\n",
    "        i.taper(0.05)\n",
    "        i.resample(fs)   \n",
    "    return st\n",
    "\n",
    "# define function to calculate number of surface events per month\n",
    "def events_per_month(starttimes, events):\n",
    "    num_events = {}\n",
    "    for year in range (2001, 2021):\n",
    "        for month in range (1, 13):\n",
    "            Nevt = []\n",
    "            period = str(year)+\"_\"+str(month)\n",
    "            t0 = UTCDateTime(year, month, 1)\n",
    "            t1 = t0+3600*24*30\n",
    "            for i in range(0, len(starttimes)):\n",
    "                if t0<starttimes[i]<t1:\n",
    "                    Nevt.append(events[i])\n",
    "            if len(Nevt) != 0:\n",
    "                num_events[period]=len(Nevt)\n",
    "            if len(Nevt) == 0:\n",
    "                num_events[period] = 0\n",
    "\n",
    "    periods = list(num_events.keys())\n",
    "    num_of_events = list(num_events.values())\n",
    "    return periods, num_of_events\n",
    "\n",
    "# define function to fit data to\n",
    "def test_func(theta, a,theta0, c):\n",
    "    return a * np.cos(theta-theta0)+c\n",
    "\n",
    "# define a function to make plots of weighted data\n",
    "def weight_data(x_data,y_data,weight,test_func,v_s,stas):    \n",
    "    #weighting the data\n",
    "    tempx, tempy = [],[]\n",
    "    for i,ii in enumerate(x_data):\n",
    "        tempx.append([])\n",
    "        tempx[i].append([ii for l in range(0,weight[i])])\n",
    "        tempy.append([])\n",
    "        tempy[i].append([y_data[i] for l in range(0,weight[i])])   \n",
    "    weighted_x = sum(sum(tempx, []),[])\n",
    "    weighted_y = sum(sum(tempy, []),[])\n",
    "   \n",
    "    #optimizing parameters to fit weighted data to test_function\n",
    "    params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(weighted_x), weighted_y, p0=None)\n",
    "    d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "    if params[0]<0:\n",
    "        direction = params[1]+pi \n",
    "    else:\n",
    "        direction = params[1]   \n",
    "    fmax = max(d)\n",
    "    fmin = min(d)\n",
    "    v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "    return v, direction, d\n",
    "\n",
    "# define function to predict synthetic arrival times\n",
    "def travel_time(t0, x, y, vs, sta_x, sta_y):\n",
    "    dist = np.sqrt((sta_x - x)**2 + (sta_y - y)**2)\n",
    "    tt = t0 + dist/vs\n",
    "    return tt\n",
    "\n",
    "# define function to compute residual sum of squares\n",
    "def error(synth_arrivals,arrivals, weight):\n",
    "    res = (arrivals - synth_arrivals)* weight \n",
    "    res_sqr = res**2\n",
    "    mse = np.mean(res_sqr)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# define function to iterate through grid and calculate travel time residuals\n",
    "def gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals, weight):\n",
    "    rss_mat = np.zeros((len(t0),len(x_vect),len(y_vect)))\n",
    "    rss_mat[:,:,:] = np.nan\n",
    "    for i in range(len(t0)):\n",
    "        for j in range(len(x_vect)):\n",
    "            for k in range(len(y_vect)):\n",
    "                synth_arrivals = []\n",
    "                for h in range(len(sta_x)):\n",
    "                    tt = travel_time(t0[i],x_vect[j],y_vect[k],vs,sta_x[h],sta_y[h])\n",
    "                    synth_arrivals.append(tt)\n",
    "                rss = error(np.array(synth_arrivals),np.array(arrivals), np.array(weight))\n",
    "                rss_mat[i,j,k] = rss\n",
    "    return rss_mat\n",
    "\n",
    "# define function to find lower-left corner of grid and grid size based on height of volcano\n",
    "def start_latlon(elevation, ratio, center_lat, center_lon):\n",
    "    side_length = elevation * ratio\n",
    "    l = side_length/2\n",
    "    hypotenuse = l*np.sqrt(2)\n",
    "    d = distance.geodesic(meters = hypotenuse)\n",
    "    start_lat = d.destination(point=[center_lat,center_lon], bearing=225)[0]\n",
    "    start_lon = d.destination(point=[center_lat,center_lon], bearing=225)[1]\n",
    "    return start_lat, start_lon, side_length\n",
    "\n",
    "# define function to convert the location index into latitude and longitude\n",
    "def location(x_dist, y_dist, start_lat, start_lon):\n",
    "    bearing = 90-np.rad2deg(np.arctan(y_dist/x_dist))\n",
    "    dist = np.sqrt((x_dist)**2 + (y_dist)**2)\n",
    "    d = distance.geodesic(meters = dist)\n",
    "    loc_lat = d.destination(point=[start_lat,start_lon], bearing=bearing)[0]\n",
    "    loc_lon = d.destination(point=[start_lat,start_lon], bearing=bearing)[1]\n",
    "    return loc_lat, loc_lon, d\n",
    "\n",
    "# define function to find diameter in meters of the error on the location\n",
    "def error_diameter(new_array):\n",
    "    min_idx = np.min(new_array[:,1])\n",
    "    max_idx = np.max(new_array[:,1])\n",
    "    difference = max_idx-min_idx\n",
    "    diameter_m = difference*1000\n",
    "    return diameter_m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d472ef4",
   "metadata": {},
   "source": [
    "##  Import and organize metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22a659",
   "metadata": {},
   "source": [
    "### 1. Volcano Data (network and station, labeled with volcano name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b099703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('Data/Volcano_Metadata_50km.csv')\n",
    "df_xd = pd.read_csv('Data/XD_Metadata_50km.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center latitude, center longitude, elevation(m), left_trim, right_trim, bottom_trim, top_trim \n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5, 10000, 17000, 13500, 5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the lower left corner and grid size based on volcano elevation\n",
    "# [start_lat = lower_left latitude of gridsearch square, start lon = lower left longitude of gridsearhc square, side length of grid search square]\n",
    "volc_grid = {}\n",
    "for volc in volc_lat_lon:\n",
    "    elevation = volc_lat_lon[volc][2]\n",
    "    center_lat = volc_lat_lon[volc][0]\n",
    "    center_lon = volc_lat_lon[volc][1]\n",
    "    start_lat, start_lon, side_length = start_latlon(elevation, ratio, center_lat, center_lon)\n",
    "    volc_grid[volc] = [start_lat, start_lon, side_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d569229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEM data \n",
    "r_dem_data_dict = {}\n",
    "name = 'Mt_Rainier'\n",
    "if volc_lat_lon[name][0]>46:\n",
    "    dem = rio.open('Data/DEM_data/'+str(name)+'/'+str(name)+'1.tif') #washington volcanoes\n",
    "    dem_array = dem.read(1).astype('float64')\n",
    "    dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "    crs = dem.crs\n",
    "\n",
    "r_dem_data_dict[name]={'data':dem_array, 'crs':crs, 'left':dem.bounds[0], 'right':dem.bounds[2], 'bottom':dem.bounds[1], 'top':dem.bounds[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_dict = {}\n",
    "lat_lon_dict['Mt_Rainier']={'tick_lons':[-121.65, -121.7, -121.75, -121.8, -121.85],\n",
    "                            'tick_lats':[46.75,46.8,46.85,46.9,46.95]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ca268",
   "metadata": {},
   "source": [
    "### 3. Surface Event Data from PNSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3894296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('Data/PNSN_Pick_Label.csv')\n",
    "label = df3['Label'].values.tolist()\n",
    "surface_label = df3[df3['Label']== 'su']['Label'].values.tolist()\n",
    "net_temp = df3[df3['Label']== 'su']['Network'].values.tolist()\n",
    "sta_temp = df3[df3['Label']== 'su']['Station'].values.tolist()\n",
    "evt_id_temp = df3[df3['Label']== 'su']['Event_ID'].values.tolist()\n",
    "start_time_temp = df3[df3['Label']== 'su']['Picktime'].values.tolist()                               \n",
    "\n",
    "net,sta,evt_id,start_time = [],[],[],[]\n",
    "for i,ii in enumerate(start_time_temp):\n",
    "    if t_beginning<UTCDateTime(ii)<t_end:\n",
    "        net.append(net_temp[i])\n",
    "        sta.append(sta_temp[i])\n",
    "        evt_id.append(evt_id_temp[i])\n",
    "        start_time.append(ii)\n",
    "\n",
    "all_stas = set(sta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c410c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "ticks_x = []\n",
    "ticks_y = []\n",
    "for i in range(len(tick_lons)):\n",
    "    tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "    ticks_x.append(tick_x)\n",
    "    ticks_y.append(tick_y)\n",
    "    tick_lons[i]=str(tick_lons[i])\n",
    "    tick_lats[i]=str(tick_lats[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives indices of events in Wes' catalog\n",
    "with open(\"Data/wes_events_r.json\", 'r') as f:\n",
    "    wes_events_r = json.load(f)\n",
    "    \n",
    "# gives event ids of events in Wes' catalog\n",
    "with open(\"Data/event_ids_r.json\", 'r') as f:\n",
    "    event_ids_r = json.load(f)\n",
    "    \n",
    "to_run = []\n",
    "for i in event_ids_r:\n",
    "    if i in evt_id:\n",
    "        print(i)\n",
    "        index = evt_id.index(i)\n",
    "        to_run.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract information from the DEM data\n",
    "associated_volcano = 'Mt_Rainier'\n",
    "crs = r_dem_data_dict[associated_volcano]['crs']\n",
    "data = r_dem_data_dict[associated_volcano]['data']\n",
    "volc = rd.rdarray(data, no_data=-9999)\n",
    "slope = rd.TerrainAttribute(volc,attrib = 'slope_riserun')\n",
    "aspect = np.array(rd.TerrainAttribute(volc, attrib = 'aspect'))\n",
    "info = volc_lat_lon[associated_volcano]\n",
    "p2 = Proj(crs,preserve_units=False)\n",
    "p1 = Proj(proj='latlong',preserve_units=False)\n",
    "# gives the lower left grid point in the grid search\n",
    "left_x,bottom_y = transform(p1,p2,volc_grid[associated_volcano][1],volc_grid[associated_volcano][0]) # p1,p2,lon,lat\n",
    "# gives the left right, bottom, top of the grid\n",
    "grid_bounds = [left_x, left_x+volc_grid[associated_volcano][2], bottom_y, bottom_y+volc_grid[associated_volcano][2]]\n",
    "left, right = r_dem_data_dict[associated_volcano]['left'],r_dem_data_dict[associated_volcano]['right']\n",
    "bottom, top = r_dem_data_dict[associated_volcano]['bottom'],r_dem_data_dict[associated_volcano]['top']\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fa267",
   "metadata": {},
   "source": [
    "## Calculating directivity and velocity of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c702477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evt_data = pd.DataFrame(columns = ['event_ID','location_latitude','location_longitude','location_uncertainty(m)',\n",
    "#                                    'origin_time','direction(degrees)', \n",
    "#                                    'direction_sharpness(degrees)','direction_snr(degrees)','duration(sec)',\n",
    "#                                    'params_std_deviation', 'velocity(m/s)','number_of_stations'])\n",
    "\n",
    "# sta_freq=pd.DataFrame(columns = list(all_stas))\n",
    "# reject_evts=pd.DataFrame(columns = ['event_ID'])\n",
    "\n",
    "\n",
    "for n in to_run:    \n",
    "    event_ID = str(evt_id[n])\n",
    "#     try:\n",
    "    time = UTCDateTime(start_time[n])\n",
    "    print(time)\n",
    "    if net != 'CN' and evt_id[n]!=evt_id[n-1]:\n",
    "        reference = str(net[n]+'.'+sta[n])\n",
    "#         try:\n",
    "        associated_volcano = df[df['Station']== sta[n]]['Volcano_Name'].values[0]\n",
    "#         except: \n",
    "#             pass\n",
    "        if associated_volcano == 'Mt_Rainier': \n",
    "            #get info for stations within 50km of volcano that event ocurred at\n",
    "            stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "            networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "            latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "            longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "            elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "            if stations.count(\"LON\")>0 and stations.count(\"LO2\")>0:\n",
    "                index = stations.index(\"LO2\")\n",
    "                del stations[index]\n",
    "                del networks[index]\n",
    "                del latitudes[index]\n",
    "                del longitudes[index]\n",
    "                del elevations[index]\n",
    "            \n",
    "            #Get all waveforms for that event based on stations and times\n",
    "            bulk = [] \n",
    "            for m in range(0, len(networks)):\n",
    "                bulk.append([networks[m], stations[m], '*', '*', time-t_before, time+t_after])\n",
    "            st = client2.get_waveforms_bulk(bulk)\n",
    "\n",
    "            #remove unwanted data\n",
    "            for tr in st:\n",
    "                cha = tr.stats.channel\n",
    "                if cha[0:2] != 'BH' and cha[0:2] != 'EH' and cha[0:2] != 'HH':\n",
    "                    st.remove(tr)\n",
    "                try:\n",
    "                    if len(tr.data)/tr.stats.sampling_rate < 239.9:\n",
    "                        st.remove(tr)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            #resampling the data to 40Hz for each trace\n",
    "            st = resample(st,fs) \n",
    "\n",
    "            #Plotting all traces for one event with channel z, SNR>10, and bandpasses between 2-12Hz\n",
    "            SNR,SNR_weight, no_weight,stas,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],[],[],{},{}\n",
    "            fig = plt.figure(figsize = (11,8), dpi=200)\n",
    "            fig.suptitle('evtID:UW'+ event_ID+associated_volcano)\n",
    "            plt.rcParams.update({'font.size': 20})\n",
    "            ax1 = plt.subplot(1,1,1)\n",
    "            iplot = 0\n",
    "            for i,ii in enumerate(st):\n",
    "                network = ii.stats.network\n",
    "                station = ii.stats.station\n",
    "                ii.detrend(type = 'demean')\n",
    "                ii.filter('bandpass',freqmin=2.0,freqmax=12.0,corners=2,zerophase=True)\n",
    "                cha = ii.stats.channel\n",
    "                starttime = ii.stats.starttime\n",
    "                max_amp_time = np.argmax(ii.data)/fs\n",
    "                signal_window = ii.copy()\n",
    "                noise_window = ii.copy()\n",
    "                signal_window.trim(starttime + t_before - 20, starttime + t_before - 20 + window)\n",
    "                noise_window.trim(starttime + t_before - window -10, starttime + t_before - 10)\n",
    "                snr = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                               / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "\n",
    "                if cha[-1] == 'Z' and snr>thr and 100<max_amp_time<200:\n",
    "                    t = ii.times()\n",
    "                    t_diff[network+'.'+station] = starttime-time \n",
    "                    # enveloping the data \n",
    "                    data_envelope = obspy.signal.filter.envelope(ii.data[115*fs:150*fs])\n",
    "                    data_envelope /= np.max(data_envelope)\n",
    "                    data_envelope += iplot*1.5\n",
    "                    # finding the time of max amplitude of each event\n",
    "                    max_amp_times.append(max_amp_time)\n",
    "                    max_amp = np.max(ii.data)      \n",
    "                    # creating envelope data dictionary to calculate picktimes\n",
    "                    data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "                    data_env_dict[network+'.'+station]= data_envelope\n",
    "                    b,e = 115,150\n",
    "                    ax1.plot(t[b*fs:e*fs],ii.data[b*fs:e*fs]/np.max(np.abs(ii.data))+iplot*1.5)\n",
    "                    ax1.plot(t[115*fs:150*fs], data_envelope, color = 'k')\n",
    "                    ax1.set_xlabel('time (seconds)', fontsize = 10)\n",
    "                    plt.xticks(fontsize = 10)\n",
    "                    ax1.set_xlim([b,e])\n",
    "                    ax1.set_yticks([])\n",
    "                    plt.text(t[e*fs], iplot*1.5, 'SNR:'+str(int(snr)), fontsize = 15)\n",
    "                    plt.text(t[b*fs], (iplot*1.5)+0.2, station, fontsize = 15)\n",
    "                    iplot = iplot+1\n",
    "                    stas.append(ii.stats.station)\n",
    "                    nets.append(ii.stats.network)\n",
    "                    SNR.append(snr)\n",
    "                    SNR_weight.append(int(snr))\n",
    "                    no_weight.append(1)\n",
    "                else:\n",
    "                    st.remove(ii)\n",
    "\n",
    "            if len(st)<4:  \n",
    "                continue\n",
    "\n",
    "            # get peak frequency of each event\n",
    "            # read and preprocess data\n",
    "            st.taper(max_percentage=0.01,max_length=20)\n",
    "            st.trim(starttime=time-20,endtime=time+30)\n",
    "            \n",
    "            lats, lons, elevs, r, theta = ([] for i in range(5)) \n",
    "            ref = str(nets[0]+'.'+stas[0])\n",
    "            try:\n",
    "                ref_env = data_env_dict[reference]\n",
    "            except:\n",
    "                ref_env = data_env_dict[ref]\n",
    "\n",
    "            # calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "            pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs) #calculate picktimes\n",
    "            shifts, vals = shift(pick_times, offsets, starttimes, t_diff)\n",
    "\n",
    "            iplot = 0 \n",
    "            durations = []\n",
    "            for i in range(len(stas)):\n",
    "                max_amp_time = max_amp_times[i]\n",
    "                duration = (max_amp_time-vals[i])*2\n",
    "                durations.append(duration)\n",
    "                ax1.vlines(vals[i], ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "#                 plt.text(t[110*fs], iplot*1.5, 'duration:'+str(int(duration))+'s')\n",
    "                a = stations.index(stas[i])\n",
    "                lats.append(latitudes[a])\n",
    "                lons.append(longitudes[a])\n",
    "                elevs.append(elevations[a])\n",
    "                iplot = iplot+1\n",
    "            avg_duration = np.mean(durations)\n",
    "            \n",
    "            plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'wiggles'+'.png')\n",
    "\n",
    "#             # make plot of spectra\n",
    "            char_freq, sharp_weight= [],[]\n",
    "            fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "        \n",
    "            matplotlib.rc('xtick', labelsize = 10)\n",
    "            for i in range(len(stas)):\n",
    "                data = st.select(station=stas[i],component=\"Z\")[0].data*100\n",
    "                f,psd=scipy.signal.welch(data,fs=st[0].stats.sampling_rate,nperseg=81,noverlap=1)\n",
    "                #just get the frequencies within the filter band\n",
    "                above_low_cut = [f>low_cut]\n",
    "                below_high_cut = [f<high_cut]\n",
    "                in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "                f = f[in_band]\n",
    "                psd = psd[in_band]\n",
    "\n",
    "                # calculate characteristic frequency and report\n",
    "                char_freq_max = f[np.argmax(psd)]\n",
    "                char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "                psd_cumsum = np.cumsum(psd)\n",
    "                psd_sum = np.sum(psd)\n",
    "                char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "                char_freq.append(char_freq_mean)\n",
    "\n",
    "                plt.rcParams.update({'font.size': 10})\n",
    "                plt.yticks(fontsize = 10)\n",
    "                ax.plot(f,psd,label=stas[i],linewidth=1.5)\n",
    "                ax.set_xscale('log')\n",
    "                ax.set_yscale('log')\n",
    "                ax.tick_params(axis = 'x', which = 'both', labelsize = 10)\n",
    "                ax.grid('True')\n",
    "                ax.set_xlabel('Frequency [Hz]', fontsize = 10)\n",
    "                ax.set_ylabel('PSD [$(mm/s)^2$/Hz]', fontsize = 10)\n",
    "                ax.vlines(char_freq_mean,ymin=np.min(psd)/10,ymax=np.max(psd)*10,linestyle=\"--\",colors=colors[i])\n",
    "\n",
    "#                 weighting the data by the spikiness of the PSD vs frequency graphs\n",
    "                ratio = (np.mean(psd)/np.max(psd))\n",
    "                sharp_weight.append(int(1/(ratio**2)*20))\n",
    "\n",
    "            ax.legend() \n",
    "            plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'psd'+'.png')\n",
    "\n",
    "#             lats, lons, elevs, r, theta = ([] for i in range(5)) \n",
    "#             ref = str(nets[0]+'.'+stas[0])\n",
    "#             try:\n",
    "#                 ref_env = data_env_dict[reference]\n",
    "#             except:\n",
    "#                 ref_env = data_env_dict[ref]\n",
    "\n",
    "#             # calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "#             pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs) #calculate picktimes\n",
    "#             shifts, vals = shift(pick_times, offsets, starttimes, t_diff)\n",
    "\n",
    "#             iplot = 0 \n",
    "#             durations = []\n",
    "#             for i in range(len(stas)):\n",
    "#                 max_amp_time = max_amp_times[i]\n",
    "#                 duration = (max_amp_time-vals[i])*2\n",
    "#                 durations.append(duration)\n",
    "# #                     ax1.vlines(vals[i], ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "# #                 plt.text(t[110*fs], iplot*1.5, 'duration:'+str(int(duration))+'s')\n",
    "#                 a = stations.index(stas[i])\n",
    "#                 lats.append(latitudes[a])\n",
    "#                 lons.append(longitudes[a])\n",
    "#                 elevs.append(elevations[a])\n",
    "#                 iplot = iplot+1\n",
    "#             avg_duration = np.mean(durations)\n",
    "# #                 plt.savefig('wiggles'+event_ID+associated_volcano+'.png')\n",
    "\n",
    "            # input necessary data for grid search\n",
    "            arrivals = shifts\n",
    "            sta_lats = lats\n",
    "            sta_lons= lons\n",
    "\n",
    "            # define grid origin in lat,lon and grid dimensions in m\n",
    "            lat_start = volc_grid[associated_volcano][0]\n",
    "            lon_start = volc_grid[associated_volcano][1]\n",
    "            side_length = volc_grid[associated_volcano][2]\n",
    "\n",
    "            # create the grid of locations\n",
    "            sta_x = []\n",
    "            sta_y = []\n",
    "            for i in range(len(sta_lats)):\n",
    "                x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "                y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "                sta_x.append(x_dist)\n",
    "                sta_y.append(y_dist)\n",
    "            x_vect = np.arange(0, side_length, step)\n",
    "            y_vect = np.arange(0, side_length, step)\n",
    "            t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "\n",
    "            # gridsearch with no weight\n",
    "            weight = [1 for i in range(len(SNR_weight))]\n",
    "            rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "            loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "            # find the latitude and longitude of the location index \n",
    "            loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "            err_thr = np.min(np.log10(rss_mat))+.05\n",
    "            thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "            diameter = error_diameter(thr_array)\n",
    "\n",
    "            # gridsearch weighted by SNR\n",
    "            weight = np.array(SNR_weight)/np.max(SNR_weight)\n",
    "            rss_mat_snr = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,weight)\n",
    "            loc_idx_snr = np.unravel_index([np.argmin(rss_mat_snr)], rss_mat_snr.shape)\n",
    "            loc_lat_snr, loc_lon_snr, test_d = location(x_vect[loc_idx_snr[1]], y_vect[loc_idx_snr[2]], lat_start, lon_start)\n",
    "\n",
    "            # gridsearch weighted with SNR and Slope\n",
    "            # gives the lower left grid point in the grid search\n",
    "            # gives the left right, bottom, top of the grid\n",
    "            left, right = r_dem_data_dict[associated_volcano]['left'],r_dem_data_dict[associated_volcano]['right']\n",
    "            bottom, top = r_dem_data_dict[associated_volcano]['bottom'],r_dem_data_dict[associated_volcano]['top']\n",
    "\n",
    "            a = int((left_x-left)/10)\n",
    "            b = a+2500\n",
    "            c = (slope.shape[0] - int((bottom_y-bottom)/10))-2500\n",
    "            d = slope.shape[0] - int((bottom_y-bottom)/10)\n",
    "\n",
    "            x = np.arange(a,b,1)\n",
    "            y = np.arange(c,d,1)\n",
    "\n",
    "            x2 = np.arange(a,b,10) # every 100m\n",
    "            y2 = np.arange(c,d,10) # every 100m\n",
    "\n",
    "            slope_data = np.array(slope[c:d,a:b])\n",
    "\n",
    "            slope_data[slope_data < 1] = 1\n",
    "            slope_data[slope_data > 90] = 80\n",
    "\n",
    "            slope_norm = 1/slope_data\n",
    "\n",
    "            slope_interp_mat = RectBivariateSpline(y,x,slope_norm, s = 0)\n",
    "            interp = (slope_interp_mat(x2,y2)/np.max(slope_interp_mat(x2,y2)))*0.1+.9\n",
    "\n",
    "            # gridsearch weighted with slope\n",
    "            rss_mat_slope = np.multiply(rss_mat[loc_idx[0],:,:],(interp))\n",
    "            loc_idx_slope = np.unravel_index([np.argmin(rss_mat_slope)], rss_mat_slope.shape)\n",
    "            loc_lat_slope, loc_lon_slope, test_d = location(x_vect[loc_idx_slope[1]], y_vect[loc_idx_slope[2]], lat_start, lon_start)\n",
    "\n",
    "            # gridsearch weighted with snr and slope\n",
    "            rss_mat_slopesnr = np.multiply(rss_mat_snr[loc_idx[0],:,:],(interp))\n",
    "            loc_idx_slopesnr = np.unravel_index([np.argmin(rss_mat_slopesnr)], rss_mat_slopesnr.shape)\n",
    "            loc_lat_slopesnr, loc_lon_slopesnr, test_d = location(x_vect[loc_idx_slopesnr[1]], y_vect[loc_idx_slopesnr[2]], lat_start, lon_start)\n",
    "            \n",
    "            # plot heatmap\n",
    "#             fig,ax = plt.subplots(1,1,figsize=(8,8), dpi = 200)\n",
    "#             ax.scatter(x_vect[loc_idx[1]],y_vect[loc_idx[2]],s=100,marker='*',c='r')\n",
    "#             ax.scatter(x_vect[loc_idx_slope[1]],y_vect[loc_idx_slope[2]],s=50,marker='*',c='b')\n",
    "#             ax.scatter(x_vect[loc_idx_snr[1]],y_vect[loc_idx_snr[2]],s=25,marker='*',c='w')\n",
    "#             ax.scatter(x_vect[loc_idx_slopesnr[1]],y_vect[loc_idx_slopesnr[2]],s=50,marker='*',c='k')\n",
    "#             im = ax.imshow(np.log10(rss_mat_slope[loc_idx[0],:,:].T),origin=\"lower\",extent=[0,side_length,0,side_length])\n",
    "#             ax.set_ylabel('(m)')\n",
    "#             ax.set_ylabel('(m)')\n",
    "#             cbar = plt.colorbar(im)\n",
    "#             cbar.ax.tick_params()\n",
    "#             cbar.set_label('RMS error on location', rotation=270)\n",
    "            #plt.savefig('heatmap'+ event_ID+associated_volcano+'.png')\n",
    "\n",
    "            # calculating azimuth for each station with respect to the location of the event\n",
    "            for i in range(len(stas)):\n",
    "                u,b,c = (gps2dist_azimuth(loc_lat_slope, loc_lon_slope, lats[i], lons[i], a=6378137.0, f=0.0033528106647474805))\n",
    "                r.append(u)\n",
    "                theta.append(b)\n",
    "\n",
    "            bin1,bin2,bin3 = [],[],[]\n",
    "            for i in theta:\n",
    "                if 0<=i<=120:\n",
    "                    bin1.append(i)\n",
    "                if 121<=i<=240:\n",
    "                    bin2.append(i)\n",
    "                if 241<=i<=360:\n",
    "                    bin3.append(i)\n",
    "\n",
    "            if bin1 == [] or bin2 == [] or bin3 == []:\n",
    "                continue\n",
    "\n",
    "            #manipulating the data\n",
    "            data = {'azimuth_deg':theta, 'freq':char_freq, 'station':stas, 'distance_m':r, \n",
    "                    'weight':sharp_weight, 'SNR':SNR, 'colors':colors[0:len(stas)]}\n",
    "            DF = pd.DataFrame(data, index = None)\n",
    "            DF2 = DF.sort_values('azimuth_deg')\n",
    "\n",
    "            #Taking out stations that are too close to the location when looking at azimuth \n",
    "            drops = []\n",
    "            for i in range(len(DF2)):\n",
    "                value = DF2.loc[i,'distance_m']\n",
    "                if value < az_thr:\n",
    "                    drops.append(i)\n",
    "            DF3 = DF2.drop(drops)\n",
    "            y_data =  DF3[\"freq\"].values.tolist()\n",
    "            Sta2 = DF3[\"station\"].values.tolist()\n",
    "            dist2 = DF3[\"distance_m\"].values.tolist()\n",
    "            spike_weight = DF3[\"weight\"].values.tolist()\n",
    "            SNR2 = DF3['SNR'].values.tolist()\n",
    "            colors2 = DF3['colors'].values.tolist()\n",
    "            x_data =  np.asarray(DF3[\"azimuth_deg\"].values.tolist())\n",
    "            x_points = np.linspace(0,360, 100)\n",
    "\n",
    "            #optimizing parameters to fit data to test_function\n",
    "            params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(x_data), y_data, p0=None)\n",
    "            perr = np.sqrt(np.diag(params_covariance))\n",
    "            std_deviation = str(round(perr[0],9))+','+str(round(perr[1],9))+','+str(round(perr[2],9))\n",
    "            d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "            len_r = int(max(r))\n",
    "\n",
    "            if params[0]<0:\n",
    "                direction = params[1]+pi \n",
    "            else:\n",
    "                direction = params[1]\n",
    "\n",
    "            fmax = max(d)\n",
    "            fmin = min(d)\n",
    "            v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "\n",
    "            #convert the direction from polar to cartesian coordinates\n",
    "            dy = len_r*np.sin(direction)\n",
    "            dx = len_r*np.cos(direction)     \n",
    "\n",
    "            # weight the data\n",
    "            title = 'Sharpness'\n",
    "            v_sharp,direction_sharp,d_sharp = weight_data(x_data,y_data,sharp_weight,test_func,v_s,stas)\n",
    "            dy_sharp = len_r*np.sin(direction_sharp)\n",
    "            dx_sharp = len_r*np.cos(direction_sharp)    \n",
    "\n",
    "            title = 'SNR'\n",
    "            v_snr,direction_snr,d_snr = weight_data(x_data,y_data,SNR_weight,test_func,v_s,stas)  \n",
    "            dy_snr = len_r*np.sin(direction_snr)\n",
    "            dx_snr = len_r*np.cos(direction_snr) \n",
    "\n",
    "            fig,ax = plt.subplots(1,1,figsize=(11,8), dpi = 200)\n",
    "            fig.suptitle('Fitted Cosine Curves')       \n",
    "            ax.set_ylabel('characteristic frequency(Hz)', fontsize = 10)\n",
    "            ax.set_xlabel('azimuth(degrees)', fontsize = 10)\n",
    "            for i in range (0,len(Sta2)):\n",
    "                ax.scatter(x_data[i], y_data[i], s = (SNR_weight[i]**2),label=Sta2[i], color = colors2[i])\n",
    "            ax.plot(x_data,y_data, '--', label='rawdata')\n",
    "            ax.plot(x_points, d, label = 'original')\n",
    "            ax.plot(x_points, d_sharp, label = 'sharpness')\n",
    "            ax.plot(x_points, d_snr, label = 'snr')\n",
    "            ax.legend(loc='upper right', fontsize = 10)\n",
    "            plt.grid(True)\n",
    "            plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'curves_freq_data'+'.png')\n",
    "\n",
    "            #making plots of directivity and location\n",
    "#             crs = r_dem_data_dict[associated_volcano]['crs']\n",
    "            data = r_dem_data_dict[associated_volcano]['data']\n",
    "\n",
    "            # convert loc data onto the DEM data\n",
    "            contour_x,contour_y = np.meshgrid(left_x+x_vect,bottom_y+y_vect)\n",
    "            center_x, center_y = transform(p1,p2,info[1],info[0])\n",
    "            loc_x,loc_y=transform(p1,p2,loc_lon_slope,loc_lat_slope)\n",
    "            duration=avg_duration\n",
    "            length_factor = duration/100\n",
    "            length_factor = v_snr/(np.max(v_snr)*4)\n",
    "\n",
    "            fig,ax = plt.subplots(1,1,figsize=(8,11), dpi = 200)\n",
    "            \n",
    "            dem = ax.imshow(data,extent=[left, right, bottom, top],cmap='gist_earth', alpha = 0.8)\n",
    "            contours = ax.contour(contour_x,contour_y,np.log10(rss_mat_slope[int(loc_idx[0]),:,:].T),cmap='plasma', linewidths = 0.5)\n",
    "            topo_contours = ax.contour(data, levels = [1000,2000,3000,4000], extent=[left, right, bottom, top],origin=\"upper\", colors = 'k',linewidths = 0.3, alpha = 0.6)\n",
    "            ax.scatter(loc_x, loc_y, s=150,marker='*',c='aqua', zorder = 5)\n",
    "#             plt.arrow(loc_x,loc_y,dy*length_factor,dx*length_factor, color='m', width=170, label='no weight')\n",
    "#             plt.arrow(loc_x,loc_y,dy_sharp*length_factor,dx_sharp*length_factor, color='k', width=170, label='sharpness')\n",
    "            plt.arrow(loc_x,loc_y,dy_snr*length_factor,dx_snr*length_factor, color='w', width=100, zorder = 4)\n",
    "            \n",
    "            #plotting the stations on top of this as triangles\n",
    "            for i, ii in enumerate(stas):\n",
    "                sta_x,sta_y = transform(p1,p2,lons[i],lats[i])\n",
    "                if left+info[3]<sta_x<right-info[4] and bottom+info[5]<sta_y<top-info[6]:\n",
    "                    ax.plot(sta_x,sta_y, c='k', marker=\"^\")\n",
    "                    ax.text(sta_x,sta_y,ii, c='k', fontsize = 15)\n",
    "\n",
    "#             #getting lat and lon tick marks on the axis\n",
    "#             tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "#             tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "#             ticks_x = []\n",
    "#             ticks_y = []\n",
    "#             for i in range(len(tick_lons)):\n",
    "#                 tick_x,tick_y=transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "#                 ticks_x.append(tick_x)\n",
    "#                 ticks_y.append(tick_y)\n",
    "#                 tick_lons[i]=str(tick_lons[i])\n",
    "#                 tick_lats[i]=str(tick_lats[i])\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax1 = divider.append_axes('right', size='2%', pad=0.1)\n",
    "            ax.set_title('Location and Directivity', fontsize = 20)\n",
    "            ax.set_xlabel('longitudes(DD)', fontsize = 10)\n",
    "            ax.set_ylabel('latitudes(DD)', fontsize = 10)\n",
    "            ax.set_xticks(ticks_x)\n",
    "            ax.set_xticklabels(tick_lons, fontsize = 10)\n",
    "            ax.set_yticks(ticks_y)\n",
    "            ax.set_yticklabels(tick_lats, fontsize = 10)\n",
    "            ax.clabel(contours, contours.levels, fontsize = 15, inline = True, inline_spacing = 0.5)\n",
    "            cbar = plt.colorbar(dem, cax=cax1)\n",
    "            cbar.ax.tick_params(labelsize=10)\n",
    "            cbar.set_label('Elevation(m)\\n', rotation=270, labelpad = 13, fontsize = 10)\n",
    "            ax.set_xlim(left+info[3],right-info[4])\n",
    "            ax.set_ylim(bottom+info[5],top-info[6])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./Analysis_Data/r_events_Figs/_'+event_ID+'loc_direction.png',bbox_inches=\"tight\")\n",
    "            \n",
    "            # make a dataframe of the data\n",
    "#             evt_data = evt_data.append({'event_ID':event_ID, \n",
    "#                         'location_latitude': loc_lat_slope,\n",
    "#                         'location_longitude': loc_lon_slope,\n",
    "#                         'location_uncertainty(m)':diameter/10,\n",
    "#                         'origin_time': min(offsets)-int(loc_idx[0]),\n",
    "#                         'direction(degrees)':np.rad2deg(direction),\n",
    "#                         'direction_sharpness(degrees)':np.rad2deg(direction_sharp),\n",
    "#                         'direction_snr(degrees)':np.rad2deg(direction_snr),\n",
    "#                         'duration(sec)':avg_duration,\n",
    "#                         'params_std_deviation':std_deviation, \n",
    "#                         'velocity(m/s)':v, \n",
    "#                         'number_of_stations':len(stas)}, ignore_index = True)\n",
    "\n",
    "#             dict_temp = {}\n",
    "#             for i in range(len(stas)):\n",
    "#                 dict_temp[stas[i]] = char_freq[i]    \n",
    "#             sta_freq = sta_freq.append(dict_temp,ignore_index = True)\n",
    "\n",
    "#             evt_data.to_csv('~/surface_events/Analysis_Data/Event_Data_Rainier.csv', index=False)\n",
    "#             sta_freq.to_csv('~/surface_events/Analysis_Data/Station_frequency_data_Rainier.csv', index=False)\n",
    "#     except:\n",
    "#         reject_evts = reject_evts.append({'event_ID':[event_ID]}, ignore_index = True)\n",
    "#         reject_evts.to_csv('~/surface_events/Rejects5.csv', index=False)\n",
    "#         continue\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
