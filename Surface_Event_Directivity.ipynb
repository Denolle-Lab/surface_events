{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f6cb81",
   "metadata": {},
   "source": [
    "# Surface Event Analysis\n",
    "###### This notebook analyzes surface event waveforms and calculates location, directivity, and velocity\n",
    "###### Francesca Skene\n",
    "###### fskene@uw.edu\n",
    "###### Created: 7/22/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021dc6f",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf759fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from obspy.core import UTCDateTime\n",
    "import pandas as pd\n",
    "from obspy.clients.fdsn.client import Client\n",
    "client = Client(\"IRIS\")\n",
    "from obspy.geodetics import *\n",
    "import requests\n",
    "import glob\n",
    "import h5py\n",
    "import sys\n",
    "sys.path.append(\"/data/wsd01/pnwstore/\")\n",
    "from obspy.signal.cross_correlation import *\n",
    "from mpl_toolkits import mplot3d\n",
    "import tsfresh as tf\n",
    "import scipy\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "client = WaveformClient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30de3d",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_before = 120 #number of seconds before pick time\n",
    "t_after = 120 #number of seconds after pick time\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 30 #window length of the signal\n",
    "pr = 98 #percentile\n",
    "thr = 7 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "v_s = 500 #shear wave velocity at the surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96853aa",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4c152",
   "metadata": {},
   "source": [
    "This functions cross correlates envelopes of waveforms to calculate picktimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d04cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_time(ref_env, data_env_dict, est_picktimes):\n",
    "    xcor = obspy.signal.cross_correlation.correlate(data_env_dict,ref_env,int(5*fs))\n",
    "    index = np.argmax(xcor)\n",
    "    cc = round(xcor[index],9) #correlation coefficient\n",
    "    shift = 5*fs-index #how much it is shifted from the reference envelope\n",
    "    #print(shift, cc, key)\n",
    "    \n",
    "    p = est_picktimes + shift/fs  # p is the new phase pick for each station\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91bd26",
   "metadata": {},
   "source": [
    "This function matches start_times between two lists to label waveforms with Event IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fad3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_id(start_time, evt_id, st, t_before):\n",
    "    for i,tr in enumerate(st): #Need to add t_before because the pick time is two minutes after the the trace_start_time\n",
    "        est_picktimes.append(str(tr.stats.starttime + t_before)) \n",
    "    #Making list of strings for picktimes in order to match the format of start_time\n",
    "    p_times_str = []\n",
    "    for i in range(0, len(est_picktimes)):\n",
    "        a = est_picktimes[i][:-1] #There is an extra 'Z' at the end of each string in est_picktimes so need to eliminate it\n",
    "        p_times_str.append(a)\n",
    "\n",
    "    start_time_set = set(start_time)\n",
    "    p_times_set = set(p_times_str)\n",
    "    try:\n",
    "        c = list(p_times_set.intersection(start_time_set))[0] \n",
    "\n",
    "    # For the index of start_time in the file that matches the pick_time from the trace, give corresponding event ID\n",
    "        for i in range(0, len(start_time)):\n",
    "            if start_time[i]== c:\n",
    "                event_ID = str(evt_id[i])\n",
    "        return event_ID\n",
    "    except:\n",
    "        pass\n",
    "        return 'no event ID match'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b14b1",
   "metadata": {},
   "source": [
    "This function resamples the data in the streams to 40 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f20a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(st, fs):\n",
    "    for i in st:\n",
    "        i.detrend(type='demean')\n",
    "        i.taper(0.05)\n",
    "        i.resample(fs)   \n",
    "    return st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a8154",
   "metadata": {},
   "source": [
    "Function to fit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5199fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def test_func(theta, a,theta0, c):\n",
    "                    return a * np.cos(theta-theta0)+c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d472ef4",
   "metadata": {},
   "source": [
    "##  Import and organize metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22a659",
   "metadata": {},
   "source": [
    "### 1. Volcano Data (network and station, labeled with volcano name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b099703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('Volcano_Metadata_50km.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb31546",
   "metadata": {},
   "source": [
    "Input Volcano Names and Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data obtained from www.lat-long.com\n",
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5]\n",
    "volc_lat_lon['Mt_Adams'] = [46.202621, -121.4906384, 3743.2]\n",
    "volc_lat_lon['Mt_Baker'] = [48.7773426,  -121.8132008, 3287.6]\n",
    "volc_lat_lon['Mt_St_Helens'] = [46.1912, -122.1944, 2549]\n",
    "volc_lat_lon['Glacier_Peak'] = [48.1112273, -121.1139922, 3213]\n",
    "volc_lat_lon['Crater_Lake']=[42.907745, -122.143494, 1883]\n",
    "volc_lat_lon['Mt_Hood']=[45.373221, -121.696509, 3428.7]\n",
    "volc_lat_lon['Newberry']=[43.7220653, -121.2344654, 2435]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4e92e",
   "metadata": {},
   "source": [
    "### 2. Station Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84b5c8",
   "metadata": {},
   "source": [
    "Find Volcanoes within 100km of each station\n",
    "Create dictionary of station metadata: network, station, latitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data obtained from IRIS\n",
    "net_sta_dict = {}\n",
    "url = 'https://service.iris.edu/fdsnws/station/1/query?net=CC,UW,UO&level=channel&format=text&includecomments=true&nodata=404'\n",
    "r = requests.get(url)\n",
    "\n",
    "#This for loop extrapolates the necessary data components from the url and adds them to the dictionary\n",
    "for line in r.iter_lines():\n",
    "    line = line.decode('utf-8')\n",
    "    if ( \"#\" not in line ):\n",
    "        net = line.split(\"|\")[0]\n",
    "        sta = line.split(\"|\")[1]\n",
    "        loc = line.split(\"|\")[2]\n",
    "        cha = line.split(\"|\")[3]\n",
    "        lat = float(line.split(\"|\")[4])\n",
    "        lon = float(line.split(\"|\")[5])\n",
    "        elev = line.split(\"|\")[6] \n",
    "        netsta = net + \".\" + sta\n",
    "        net_sta_dict[netsta] = [ net, sta, lat, lon]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ca268",
   "metadata": {},
   "source": [
    "### 3. Surface Event Data from PNSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3894296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this data to loop through\n",
    "\n",
    "df3= pd.read_csv('../surface_events/PNSN_Pick_Label.csv')\n",
    "\n",
    "net = df3[\"Network\"].values.tolist()\n",
    "sta = df3[\"Station\"].values.tolist()\n",
    "evt_id = df3['Event_ID'].values.tolist()\n",
    "start_time = df3['Picktime'].values.tolist()\n",
    "label = df3['Label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df3[df3['Label']== 'su']['Event_ID'].values.tolist()\n",
    "print(len(np.unique(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299f9b1",
   "metadata": {},
   "source": [
    "For each volcano, determining a time series of events per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ad1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in surface event data from PNSN\n",
    "dir1 = \"/data/wsd01/PNSN_exotic/MLdataset_PNSN_non_earthquake_sources_MSEED/\"\n",
    "file_list = glob.glob(dir1 + \"/*su*\" )\n",
    "\n",
    "#Splitting PNSN data into the wanted data: net, sta, cha, and time\n",
    "NET = ([i.split('/')[-1].split(\".\")[0] for i in file_list])\n",
    "STA = ([i.split('/')[-1].split(\".\")[1] for i in file_list])\n",
    "CHA = ([i.split('/')[-1].split(\".\")[3] for i in file_list])\n",
    "year = ([i.split('/')[-1].split(\".\")[4] for i in file_list])\n",
    "month = ([i.split('/')[-1].split(\".\")[5] for i in file_list])\n",
    "day = ([i.split('/')[-1].split(\".\")[6] for i in file_list])\n",
    "hour = ([i.split('/')[-1].split(\".\")[7] for i in file_list])\n",
    "minute = ([i.split('/')[-1].split(\".\")[8] for i in file_list])\n",
    "second = ([i.split('/')[-1].split(\".\")[9] for i in file_list])\n",
    "millisecond = ([i.split('/')[-1].split(\".\")[10] for i in file_list])\n",
    "impulsivity = ([i.split('/')[-1].split(\".\")[12] for i in file_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7e896",
   "metadata": {},
   "source": [
    "## Calculating seasonal occurence of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c967e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in volc_lat_lon:\n",
    "    events = []\n",
    "    starttimes = []\n",
    "    stas = []\n",
    "    nets = []\n",
    "    for i in range(0, len(start_time)):\n",
    "        if label[i]=='su':\n",
    "            try:\n",
    "                #associates a volcano to each station based on 'Volcano_Metadata50km.csv'\n",
    "                associated_volcano = df[df['Station']== sta[i]]['Volcano_Name'].values[0:]\n",
    "            except: \n",
    "                associated_volcano = 'unknown'\n",
    "                \n",
    "            for volc in associated_volcano:\n",
    "                if volc == name and evt_id[i]!=evt_id[i-1]:\n",
    "                    events.append(evt_id[i])\n",
    "                    starttimes.append(start_time[i])\n",
    "                    stas.append(sta[i])\n",
    "                    nets.append(net[i])\n",
    "    print(len(starttimes))\n",
    "    print(len(nets))\n",
    "    print(len(stas))\n",
    "\n",
    "    \n",
    "    num_events = {}\n",
    "    for year in range (1980, 2021):\n",
    "        for month in range (1, 13):\n",
    "            Nevt = []\n",
    "            period = str(year)+\"_\"+str(month)\n",
    "            t0 = UTCDateTime(year, month, 1)\n",
    "            t1 = t0+3600*24*30\n",
    "            for i in range(0, len(starttimes)):\n",
    "                if t0<starttimes[i]<t1:\n",
    "                    Nevt.append(events[i])\n",
    "                    num_events[period]=len(Nevt)\n",
    "\n",
    "    periods = list(num_events.keys())\n",
    "    num_of_events = list(num_events.values())\n",
    "\n",
    "    fig = plt.figure(figsize = (100, 10))\n",
    "    plt.bar(periods,num_of_events, color ='b', width = 0.4)\n",
    "    plt.xlabel(\"year_month\")\n",
    "    plt.ylabel(\"No. of events\")\n",
    "    plt.title(\"Number of surface events per month at\" + str(name))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fa267",
   "metadata": {},
   "source": [
    "## Plotting and gathering waveforms based on pick times from all event picks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553108e",
   "metadata": {},
   "source": [
    "Putting it all into a for loop to run through all of the streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c702477",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2020,2021):\n",
    "    print(a)\n",
    "    a = str(nets[n]+'.'+stas[n])\n",
    "    if a != 'CN.SNB':\n",
    "        #Gather all of the waveforms for a specific starttime as a stream\n",
    "        t = UTCDateTime(starttimes[n])\n",
    "        print(t)\n",
    "        #t = UTCDateTime(int(year[n]), int(month[n]), int(day[n]), int(hour[n]), int(minute[n]), int(second[n]), int(millisecond[n]))\n",
    "        #Gather which station picked the event from the dataset\n",
    "        reference = a\n",
    "        try:\n",
    "            #associates a volcano to each station based on 'Volcano_Metadata.csv'\n",
    "            associated_volcano = df[df['Station']== stas[n]]['Volcano_Name'].values[0]\n",
    "            print(associated_volcano)\n",
    "        except: \n",
    "            associated_volcano = 'unknown'\n",
    "\n",
    "        if associated_volcano == 'unknown':\n",
    "            print('unknown')\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            #If the station does have a volcano nearby, give the other stations within 100km of that Volcano and pull\n",
    "            #waveforms for each station based on the given starttime of the pick\n",
    "            #find Stations list labeled with the volcano as well as Latitude, Longitude, and Elevation\n",
    "            stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "            print(stations)\n",
    "            networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "            latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "            longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "            elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "            volc_lat = volc_lat_lon[associated_volcano][0]\n",
    "            volc_lon = volc_lat_lon[associated_volcano][1]\n",
    "        \n",
    "            bulk = [] \n",
    "        \n",
    "            for m in range(0, len(networks)):\n",
    "                bulk.append([networks[m], stations[m], '*', '*', t-t_before, t+t_after])\n",
    "            st = client.get_waveforms_bulk(bulk)\n",
    "            for tr in st:\n",
    "                if tr.stats.channel[0:2] != 'BH' and tr.stats.channel[0:2] != 'EH' and tr.stats.channel[0:2] != 'HH':\n",
    "                        st.remove(tr)\n",
    "                        continue\n",
    "                if len(tr.data)/tr.stats.sampling_rate < 239.9:\n",
    "                    st.remove(tr)\n",
    "    \n",
    "        #resampling the data to 40Hz for each trace\n",
    "            st = resample(st,fs)\n",
    "            print(st)\n",
    "            break\n",
    "        #Finding the EventIDs\n",
    "            est_picktimes = []\n",
    "            event_ID = event_id(start_time, evt_id, st, t_before)\n",
    "\n",
    "        #Plotting all traces for one event with channel z, SNR>10, and bandpasses between 2-15Hz\n",
    "            SNR = []\n",
    "            stas = []\n",
    "            data_env_dict = {}\n",
    "\n",
    "            fig = plt.figure(figsize = (20,30), dpi=80)\n",
    "            plt.subplots_adjust(hspace = .4)\n",
    "            fig.suptitle('evtID:UW'+ event_ID+associated_volcano)\n",
    "\n",
    "            ax1 = plt.subplot(4,1,1)\n",
    "            iplot = 0\n",
    "            for g,x in enumerate(st):\n",
    "                t = x.times()\n",
    "                x.detrend(type = 'demean')\n",
    "                x.filter('bandpass',freqmin=2.0,freqmax=19.0,corners=2,zerophase=True)\n",
    "\n",
    "                net = x.stats.network\n",
    "                sta = x.stats.station\n",
    "                cha = x.stats.channel\n",
    "                starttime = x.stats.starttime\n",
    "                smooth_length = 2*fs\n",
    "\n",
    "                signal_window = x.copy()\n",
    "                noise_window = x.copy()\n",
    "                signal_window.trim(starttime+t_before-1, starttime+t_before-1+window)\n",
    "                noise_window.trim(starttime-window+t_before-10, starttime+t_before-10)\n",
    "\n",
    "                SNR.append(20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                               / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "\n",
    "                if cha[-1] == 'Z' and SNR[g]>thr:\n",
    "                    #enveloping the data\n",
    "                    data_envelope = obspy.signal.filter.envelope(x.data[110*fs:140*fs])\n",
    "                    data_envelope /= np.max(data_envelope)\n",
    "                    data_envelope += iplot*1.5\n",
    "                    data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "                    data_env_dict[net+'.'+sta]= data_envelope\n",
    "\n",
    "                    ax1.plot(t[100*fs:175*fs],x.data[100*fs:175*fs]/np.max(np.abs(x.data))+iplot*1.5)\n",
    "                    ax1.plot(t[110*fs:140*fs], data_envelope, color = 'k')\n",
    "                    ax1.set_ylabel('Velocity (m/s)')\n",
    "                    ax1.set_xlabel('time (seconds)')\n",
    "                    #ax1.vlines(120, ymin = 0, ymax = 1.5*iplot, color = 'k')\n",
    "                    ax1.set_xlim([100,175])\n",
    "                    #plt.text(len(x.data)/fs, iplot*1.5, str(SNR[g]))\n",
    "                    iplot = iplot+1\n",
    "\n",
    "                    stas.append(x.stats.station)\n",
    "                else:\n",
    "                    st.remove(x)\n",
    "\n",
    "            if len(st)>=4:\n",
    "                pick_times = []\n",
    "                ref_env = data_env_dict[reference]\n",
    "                for key in data_env_dict:\n",
    "                    p = pick_time(ref_env, data_env_dict[key], UTCDateTime(est_picktimes[0]))\n",
    "                    pick_times.append(p)\n",
    "\n",
    "                lats = []\n",
    "                lons = []\n",
    "                elevs = []\n",
    "                for i in stas:\n",
    "                    a = stations.index(i)\n",
    "                    lats.append(latitudes[a])\n",
    "                    lons.append(longitudes[a])\n",
    "                    elevs.append(elevations[a])\n",
    "\n",
    "                #calculating azimuth for each station with respect to the middle of the volcano\n",
    "                azimuth = []\n",
    "                for i in range(0, len(stas)):\n",
    "                    lat2 = lats[i]\n",
    "                    lon2 = lons[i]\n",
    "                    lat1 = volc_lat_lon[associated_volcano][0]\n",
    "                    lon1 = volc_lat_lon[associated_volcano][1]\n",
    "                    a,b,c = (gps2dist_azimuth(lat1, lon1, lat2, lon2, a=6378137.0, f=0.0033528106647474805))\n",
    "                    azimuth.append(b)\n",
    "                \n",
    "\n",
    "                #select station and component\n",
    "                ax2 = plt.subplot(4,1,2)\n",
    "                ax2.set_title('20 second time window')\n",
    "                stations = stas\n",
    "                channel = \"*Z\"\n",
    "                spectra_method = \"welch\"\n",
    "                char_freq_method = \"mean\"\n",
    "\n",
    "                # read and preprocess data\n",
    "                # which frequencies do I want the low_cut and high_cut to be?\n",
    "                low_cut = 2\n",
    "                high_cut = 12\n",
    "                st.filter(\"bandpass\",freqmin=low_cut,freqmax=high_cut)\n",
    "                st.taper(max_percentage=0.01,max_length=20)\n",
    "                st.trim(starttime=min(pick_times),endtime=min(pick_times)+20) \n",
    "\n",
    "                # make plot of spectra\n",
    "                colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "                char_freq = []\n",
    "                spectra_list = []\n",
    "                for i in range(len(stations)):\n",
    "                    data = st.select(station=stations[i],channel=channel)[0].data*1000\n",
    "                    if spectra_method == \"fft\":\n",
    "                        spectra = abs(np.fft.rfft(data))\n",
    "                        f = np.fft.fftfreq(len(data), d=1/st[0].stats.sampling_rate)\n",
    "                        f = f[:len(f)//2+1]\n",
    "                        power = np.square(spectra)\n",
    "                        psd = power/(f[1]-f[0])\n",
    "\n",
    "\n",
    "                    elif spectra_method == \"welch\":\n",
    "                        f,psd=scipy.signal.welch(data,fs=st[0].stats.sampling_rate,nperseg=81,noverlap=1)\n",
    "\n",
    "                    # just get the indices of frequencies within the filter band\n",
    "                    above_low_cut = [f>low_cut]\n",
    "                    below_high_cut = [f<high_cut]\n",
    "                    in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "                    f = f[in_band]\n",
    "                    psd = psd[in_band]\n",
    "\n",
    "                    ax2.plot(f,psd,label=stations[i],linewidth=2)\n",
    "                    ax2.set_xscale('log')\n",
    "                    ax2.set_yscale('log')\n",
    "                    ax2.set_xlabel('Frequency [Hz]')\n",
    "                    ax2.set_ylabel('PSD [$(mm/s)^2$/Hz]')\n",
    "                    spectra_list.append(psd)\n",
    "                    ax2.legend()\n",
    "\n",
    "                    # calculate characteristic frequency and report\n",
    "                    if char_freq_method == \"max\":\n",
    "                        char_freq_max = f[np.argmax(psd)]\n",
    "                    elif char_freq_method == \"mean\":\n",
    "                        char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "                    elif char_freq_method == \"median\":\n",
    "                        psd_cumsum = np.cumsum(psd)\n",
    "                        psd_sum = np.sum(psd)\n",
    "                        char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "\n",
    "                    char_freq.append(char_freq_mean)\n",
    "                    ymax=np.max(psd)*10\n",
    "\n",
    "        #             plt.vlines(char_freq_max, ymin=0, ymax = ymax, color = 'k')\n",
    "                    plt.vlines(char_freq_mean, ymin=0, ymax = ymax, color = 'k')\n",
    "        #             plt.vlines(char_freq_median, ymin=0, ymax = ymax, color = 'deeppink')\n",
    "                    ax2.grid(True)\n",
    "\n",
    "                #plotting azimuth versus characteristic frequency for each station\n",
    "                ydata = char_freq\n",
    "#                 xdata = []\n",
    "#                 xdata2 = []\n",
    "#                 for d in azimuth:\n",
    "#                     xdata.append(np.deg2rad(d[1]))\n",
    "#                     xdata2.append(d[0])     \n",
    "#                 print(xdata)\n",
    "                print(azimuth)\n",
    "                #optimizing parameters to fit data to test_function\n",
    "                params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(azimuth), ydata, p0=None)\n",
    "                print(params)\n",
    "                \n",
    "                #manipulating the data\n",
    "                data = {'azimuth':azimuth, 'freq':ydata, 'station':stas}\n",
    "                DF = pd.DataFrame(data, index = None)\n",
    "                DF2 = DF.sort_values('azimuth')\n",
    "                y_data =  DF2[\"freq\"].values.tolist()\n",
    "                x_data =  DF2[\"azimuth\"].values.tolist()\n",
    "                x_data = np.asarray(x_data)\n",
    "                x_points = np.linspace(0,360, 100)\n",
    "                print(x_data)\n",
    "                #plotting the raw data and the fitted curve\n",
    "                ax3 = plt.subplot(4,1,3)\n",
    "                ax3.set_title('Fitting Sin curve')\n",
    "                ax3.set_ylabel('characteristic frequency(Hz)')\n",
    "                ax3.set_xlabel('azimuth(degrees)')\n",
    "                ax3.scatter(x_data, y_data, label='Data')\n",
    "                d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "                ax3.plot(x_points, d, label='Fitted function')\n",
    "                ax3.plot(x_data,y_data, '--', label='rawdata')\n",
    "                ax3.legend(loc='best')\n",
    "                \n",
    "#                 ax4= plt.subplot(4,1,4, polar=True)\n",
    "#                 ax4.set_theta_offset(pi/2)\n",
    "#                 r = []\n",
    "#                 theta = []\n",
    "#                 for y in azimuth:\n",
    "#                     r.append(y[0])\n",
    "#                     theta.append(360 - y[1])\n",
    "#                 for t in theta:\n",
    "#                     if t > 360:\n",
    "#                         i = theta.index(t)\n",
    "#                         theta = theta[:i]+[t-360]+theta[i+1:]       \n",
    "#                 for i in range(0,len(r)):\n",
    "#                     ax4.plot(np.deg2rad(theta[i]), r[i], 'g.')\n",
    "#                     ax4.text(np.deg2rad(theta[i]),r[i],stas[i]) \n",
    "                \n",
    "#                 len_r = int(max(r))\n",
    "#                 line_length = np.linspace(0,len_r,len_r+1)\n",
    "#                 direction=[]\n",
    "#                 direction = [params[2] for i in range(len_r+1)] \n",
    "#                 ax4.plot(direction,line_length, 'k-')  #plot the estimated direction of the event\n",
    "                \n",
    "#                 #calculating velocity from the frequency shift\n",
    "#                 fmax = max(d)\n",
    "#                 fmin = min(d)\n",
    "#                 v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "#                 print(v,'m/s')\n",
    "\n",
    "               # plt.savefig('evtID:UW'+ event_ID+associated_volcano+'.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e6537",
   "metadata": {},
   "source": [
    "Testing the correlation against fake data to ensure curve fitting is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7205ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y =  [1.0, 0.866025624917, 0.500000766025, 1.32679489668e-06, -0.499998467948, -0.86602429812, -0.999999999996, -0.866026951708, -0.500003064098, -3.98038469047e-06, 0.499996169868, 0.866022971317 ]\n",
    "x = [ 0,30,60,90,120,150,180,210,240,270,300,330 ]\n",
    "\n",
    "\n",
    "params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(x), y, p0=None)\n",
    "\n",
    "\n",
    "d = test_func(np.deg2rad(x), params[0], params[1], params[2])\n",
    "plt.plot(x,y, label = 'raw data')\n",
    "plt.plot(x,d,label = 'fitted function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f669dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
