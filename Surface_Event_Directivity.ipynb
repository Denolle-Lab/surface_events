{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f6cb81",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Surface Event Analysis\n",
    "###### This notebook analyzes surface event waveforms and calculates location, directivity, and velocity\n",
    "###### Francesca Skene\n",
    "###### fskene@uw.edu\n",
    "###### Created: 7/22/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021dc6f",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf759fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.geodetics import *\n",
    "from obspy.signal.cross_correlation import *\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "import requests\n",
    "import glob\n",
    "import sys\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from mpl_toolkits import mplot3d\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "from Functions import *\n",
    "import richdem as rd\n",
    "from pathlib import Path\n",
    "from pyproj import Proj,transform,Geod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30de3d",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/data/wsd01/pnwstore/')\n",
    "client = WaveformClient()\n",
    "client2 = Client('IRIS')\n",
    "\n",
    "t_before = 120 #number of seconds before pick time\n",
    "t_after = 120 #number of seconds after pick time\n",
    "fs = 40 #sampling rate that all waveforms are resampled to\n",
    "window = 30 #window length of the signal\n",
    "pr = 98 #percentile\n",
    "thr = 12 #SNR threshold\n",
    "station_distance_threshold = 25\n",
    "pi = np.pi\n",
    "v_s = 1000 #shear wave velocity at the surface\n",
    "t_beginning = UTCDateTime(2001,1,1,0,0,0)\n",
    "t_end = UTCDateTime(2021,12,31,23,59)\n",
    "smooth_length = 5\n",
    "low_cut = 2\n",
    "high_cut = 8\n",
    "az_thr = 1000 #threshold of distance in meters from source location\n",
    "step = 1000 #step every km\n",
    "t_step = 1 #step every second\n",
    "ratio = 5.6915196\n",
    "colors = list(plt.cm.tab10(np.arange(10)))*3\n",
    "radius = 6371e3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96853aa",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that calculates picktimes at each station\n",
    "def pick_time(time, ref_env, data_env_dict, st, t_diff, t_before, fs):\n",
    "    pick_times,offsets, starttimes = [],[],[]\n",
    "    for i,key in enumerate(data_env_dict):\n",
    "        starttimes.append(st[i].stats.starttime)\n",
    "        xcor = correlate(data_env_dict[key],ref_env,int(50*fs))\n",
    "        index = np.argmax(xcor)\n",
    "        cc = round(xcor[index],9) #correlation coefficient\n",
    "        shift = 50*fs-index #how much it is shifted from the reference envelope\n",
    "        offset_time = time - shift/fs\n",
    "        p = time - shift/fs  # p is the new phase pick for each station\n",
    "        pick_times.append(p+t_diff[key])\n",
    "        offsets.append(offset_time + t_diff[key])\n",
    "    return pick_times, offsets, starttimes\n",
    "    \n",
    "def shift(pick_times, offsets, starttimes, t_diff):\n",
    "    shifts, vals =[],[]\n",
    "    for i,ii in enumerate(t_diff):\n",
    "        t_shift = offsets[i]-min(offsets)\n",
    "        vals.append((-1*t_diff[ii])+t_shift)\n",
    "        shifts.append(t_shift)\n",
    "        #plt.vlines(val, ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "    return shifts, vals\n",
    "\n",
    "# define functon that resamples the data\n",
    "def resample(st, fs):\n",
    "    for i in st:\n",
    "        i.detrend(type='demean')\n",
    "        i.taper(0.05)\n",
    "        i.resample(fs)   \n",
    "    return st\n",
    "\n",
    "# define function to calculate number of surface events per month\n",
    "def events_per_month(starttimes, events):\n",
    "    num_events = {}\n",
    "    for year in range (2001, 2021):\n",
    "        for month in range (1, 13):\n",
    "            Nevt = []\n",
    "            period = str(year)+\"_\"+str(month)\n",
    "            t0 = UTCDateTime(year, month, 1)\n",
    "            t1 = t0+3600*24*30\n",
    "            for i in range(0, len(starttimes)):\n",
    "                if t0<starttimes[i]<t1:\n",
    "                    Nevt.append(events[i])\n",
    "            if len(Nevt) != 0:\n",
    "                num_events[period]=len(Nevt)\n",
    "            if len(Nevt) == 0:\n",
    "                num_events[period] = 0\n",
    "\n",
    "    periods = list(num_events.keys())\n",
    "    num_of_events = list(num_events.values())\n",
    "    return periods, num_of_events\n",
    "\n",
    "# define function to fit data to\n",
    "def test_func(theta, a,theta0, c):\n",
    "    return a * np.cos(theta-theta0)+c\n",
    "\n",
    "# define a function to make plots of weighted data\n",
    "def make_weight_plts(title,x_data,y_data,weight,test_func,x_points,v_s,theta,r,stas,az_thr,e,f,g):\n",
    "    ax = plt.subplot(2,1,e)\n",
    "    for i in range (0,len(Sta2)):\n",
    "        ax.scatter(x_data[i], y_data[i], s = (weight[i]**2*g),label=Sta2[i])\n",
    "    ax.set_ylabel('characteristic frequency(Hz)')\n",
    "    ax.set_xlabel('azimuth(degrees)')\n",
    "    ax.plot(x_data,y_data, '--', label='rawdata')\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    #weighting the data\n",
    "    tempx, tempy = [],[]\n",
    "    for i,ii in enumerate(x_data):\n",
    "        tempx.append([])\n",
    "        tempx[i].append([ii for l in range(0,weight[i])])\n",
    "        tempy.append([])\n",
    "        tempy[i].append([y_data[i] for l in range(0,weight[i])])   \n",
    "    weighted_x = sum(sum(tempx, []),[])\n",
    "    weighted_y = sum(sum(tempy, []),[])\n",
    "   \n",
    "    #optimizing parameters to fit weighted data to test_function\n",
    "    params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(weighted_x), weighted_y, p0=None)\n",
    "    d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "    fmax = max(d)\n",
    "    fmin = min(d)\n",
    "    v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "    ax.set_title(title+str(v)+'m/s')\n",
    "    ax.plot(x_points, d, label='Fitted function')\n",
    "\n",
    "    len_r = int(max(r))\n",
    "    line_length = np.linspace(0,len_r,len_r+1)\n",
    "    rads = np.arange(0, (2 *pi), 0.01)\n",
    "    direction=[]\n",
    "    direction = [(params[1]) for i in range(len_r+1)]\n",
    "\n",
    "#     ax1= plt.subplot(2,2,f, polar=True)\n",
    "#     ax1.set_title(title+str(v)+'m/s')\n",
    "#     ax1.set_theta_offset(pi/2)\n",
    "#     ax1.set_theta_direction(-1)\n",
    "#     for i in range(0,len(r)):\n",
    "#         ax1.plot(np.deg2rad(theta[i]),r[i], 'g.')\n",
    "#         ax1.text(np.deg2rad(theta[i]),r[i],stas[i]) \n",
    "#     ax1.plot(direction,line_length, 'k-')  #plot the estimated direction of the event\n",
    "#     for rad in rads:\n",
    "#         ax1.plot(rad, az_thr, 'b.', markersize = 2)\n",
    "    plt.show()\n",
    "    return v, direction\n",
    "\n",
    "# define function to predict synthetic arrival times\n",
    "def travel_time(t0, x, y, vs, sta_x, sta_y):\n",
    "    dist = np.sqrt((sta_x - x)**2 + (sta_y - y)**2)\n",
    "    tt = t0 + dist/vs\n",
    "    return tt\n",
    "\n",
    "# define function to compute residual sum of squares\n",
    "def error(synth_arrivals,arrivals, weight):\n",
    "    res = (arrivals - synth_arrivals)*weight \n",
    "    res_sqr = res**2\n",
    "    mse = np.mean(res_sqr)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# define function to iterate through grid and calculate travel time residuals\n",
    "def gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals, weight):\n",
    "    rss_mat = np.zeros((len(t0),len(x_vect),len(y_vect)))\n",
    "    rss_mat[:,:,:] = np.nan\n",
    "    for i in range(len(t0)):\n",
    "        for j in range(len(x_vect)):\n",
    "            for k in range(len(y_vect)):\n",
    "                synth_arrivals = []\n",
    "                for h in range(len(sta_x)):\n",
    "                    tt = travel_time(t0[i],x_vect[j],y_vect[k],vs,sta_x[h],sta_y[h])\n",
    "                    synth_arrivals.append(tt)\n",
    "                rss = error(np.array(synth_arrivals),np.array(arrivals), np.array(weight))\n",
    "                rss_mat[i,j,k] = rss\n",
    "    return rss_mat\n",
    "\n",
    "# define function to find lower-left corner of grid and grid size based on height of volcano\n",
    "def start_latlon(elevation, ratio, center_lat, center_lon):\n",
    "    side_length = elevation * ratio\n",
    "    l = side_length/2\n",
    "    hypotenuse = l*np.sqrt(2)\n",
    "    d = distance.geodesic(meters = hypotenuse)\n",
    "    start_lat = d.destination(point=[center_lat,center_lon], bearing=225)[0]\n",
    "    start_lon = d.destination(point=[center_lat,center_lon], bearing=225)[1]\n",
    "    return start_lat, start_lon, side_length\n",
    "\n",
    "# define function to convert the location index into latitude and longitude\n",
    "def location(x_dist, y_dist, start_lat, start_lon):\n",
    "    bearing = 90-np.rad2deg(np.arctan(y_dist/x_dist))\n",
    "    dist = np.sqrt((x_dist)**2 + (y_dist)**2)\n",
    "    d = distance.geodesic(meters = dist)\n",
    "    loc_lat = d.destination(point=[start_lat,start_lon], bearing=bearing)[0]\n",
    "    loc_lon = d.destination(point=[start_lat,start_lon], bearing=bearing)[1]\n",
    "    return loc_lat, loc_lon, d\n",
    "\n",
    "# define function to find diameter in meters of the error on the location\n",
    "def error_diameter(new_array):\n",
    "    min_idx = np.min(new_array[:,1])\n",
    "    max_idx = np.max(new_array[:,1])\n",
    "    difference = max_idx-min_idx\n",
    "    diameter_m = difference*1000\n",
    "    return diameter_m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d472ef4",
   "metadata": {},
   "source": [
    "##  Import and organize metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22a659",
   "metadata": {},
   "source": [
    "### 1. Volcano Data (network and station, labeled with volcano name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b099703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this data includes all stations within 50km of each volcano and the lat, lon, elev of each station\n",
    "df = pd.read_csv('Volcano_Metadata_50km.csv')\n",
    "df_xd = pd.read_csv('XD_Metadata_50km.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_lat_lon = {}\n",
    "volc_lat_lon['Mt_Rainier'] = [46.8528857, -121.7603744, 4392.5, 100000, 0, 15000, 5000]\n",
    "volc_lat_lon['Mt_Adams'] = [46.202621, -121.4906384, 3743.2, 5000, 3000, 4000, 2000]\n",
    "volc_lat_lon['Mt_Baker'] = [48.7773426,  -121.8132008, 3287.6, 0, 0, 0, 2000]\n",
    "# change the lat and lon of mt st helens to the middle of the dome instead of the highest point\n",
    "volc_lat_lon['Mt_St_Helens'] =[46.200472222222224,-122.18883611111112,2549, 10000, 10000, 17000, 15000] #[46.1912, -122.1944, 2549]\n",
    "volc_lat_lon['Glacier_Peak'] = [48.1112273, -121.1139922, 3213, 14000, 10000, 8000, 10000]\n",
    "volc_lat_lon['Crater_Lake']=[42.907745, -122.143494, 1883, 60000, 0, 90000, 0]\n",
    "volc_lat_lon['Mt_Hood']=[45.373221, -121.696509, 3428.7, 18000, 50000, 35000, 65000]\n",
    "volc_lat_lon['Newberry']=[43.7220653, -121.2344654, 2435, 53000, 12000, 70000, 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the lower left corner and grid size based on volcano elevation\n",
    "volc_grid = {}\n",
    "for volc in volc_lat_lon:\n",
    "    elevation = volc_lat_lon[volc][2]\n",
    "    center_lat = volc_lat_lon[volc][0]\n",
    "    center_lon = volc_lat_lon[volc][1]\n",
    "    start_lat, start_lon, side_length = start_latlon(elevation, ratio, center_lat, center_lon)\n",
    "    volc_grid[volc] = [start_lat, start_lon, side_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEM data \n",
    "dem_data_dict = {}\n",
    "for name in volc_lat_lon:\n",
    "    if volc_lat_lon[name][0]>46:\n",
    "        dem = rio.open('DEM_data/'+str(name)+'/'+str(name)+'.tif') #washington volcanoes\n",
    "        dem_array = dem.read(1).astype('float64')\n",
    "        dem_array[dem_array == -32767] = np.nan #gets rid of edge effects\n",
    "        crs = dem.crs\n",
    "    else:\n",
    "        dem = rio.open('DEM_data/'+str(name)+'/_w001001.adf') #oregon volcanoes\n",
    "        dem_array = dem.read(1).astype('float64')\n",
    "        dem_array[dem_array == -3.4028234663852886e+38] = np.nan #gets rid of edge effects\n",
    "        crs = dem.crs\n",
    "#     volc = rd.rdarray(dem_array, no_data=-9999)\n",
    "#     slope = rd.TerrainAttribute(volc,attrib = 'slope_riserun')\n",
    "#     aspect = rd.TerrainAttribute(volc, attrib = 'aspect')\n",
    "#     dem_data_dict[name] = {'data':dem_array, 'elevation':volc, 'slope':slope, 'aspect':aspect}\n",
    "    dem_data_dict[name]={'data':dem_array, 'crs':crs, 'left':[dem.bounds[0],, 'right':dem.bounds[2], 'bottom':dem.bounds[1], 'top':dem.bounds[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_dict = {}\n",
    "lat_lon_dict['Mt_Rainier']={'tick_lons':[-121.65, -121.7, -121.75, -121.8, -121.85],\n",
    "                            'tick_lats':[46.75,46.8,46.85,46.9,46.95]}\n",
    "lat_lon_dict['Mt_St_Helens']={'tick_lons':[-122.10,-122.15,-122.2,-122.25],\n",
    "                              'tick_lats':[46.16, 46.18, 46.20, 46.22]}\n",
    "lat_lon_dict['Mt_Adams']={'tick_lons':[-121.6, -121.55, -121.5, -121.45, -121.4],\n",
    "                          'tick_lats':[46.16, 46.18, 46.20, 46.22]}\n",
    "lat_lon_dict['Mt_Baker']={'tick_lons':[ -121.7, -121.75, -121.80, -121.85, -121.90, -121.95],\n",
    "                          'tick_lats':[48.71, 48.74, 48.77, 48.80, 48.83, 48.86]}\n",
    "lat_lon_dict['Mt_Hood']={'tick_lons':[-121.58, -121.62, -121.66, -121.70, -121.74],\n",
    "                         'tick_lats':[45.3, 45.33, 45.36, 45.39, 45.42]}\n",
    "lat_lon_dict['Glacier_Peak']={'tick_lons':[ -121.07, -121.09, -121.11, -121.13, -121.15],\n",
    "                              'tick_lats':[48.08, 48.10, 48.12, 48.14, 48.16]}\n",
    "lat_lon_dict['Newberry']={'tick_lons':[-121.11, -121.15, -121.19, -121.23, -121.27, -121.31, -121.34],\n",
    "                          'tick_lats':[43.64, 43.67, 43.70, 43.73, 43.76, 43.79, 43.82]}\n",
    "lat_lon_dict['Crater_Lake']={'tick_lons':[ -121.98, -122.06, -122.14, -122.22, -122.30],\n",
    "                             'tick_lats':[42.80, 42.85, 42.90, 42.95, 43.00]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ca268",
   "metadata": {},
   "source": [
    "### 3. Surface Event Data from PNSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3894296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"su\" is the label for surface event\n",
    "\n",
    "df3= pd.read_csv('../surface_events/PNSN_Pick_Label.csv')\n",
    "\n",
    "label = df3['Label'].values.tolist()\n",
    "surface_label = df3[df3['Label']== 'su']['Label'].values.tolist()\n",
    "net_temp = df3[df3['Label']== 'su']['Network'].values.tolist()\n",
    "sta_temp = df3[df3['Label']== 'su']['Station'].values.tolist()\n",
    "evt_id_temp = df3[df3['Label']== 'su']['Event_ID'].values.tolist()\n",
    "start_time_temp = df3[df3['Label']== 'su']['Picktime'].values.tolist()                               \n",
    "\n",
    "net,sta,evt_id,start_time = [],[],[],[]\n",
    "for i,ii in enumerate(start_time_temp):\n",
    "    if t_beginning<UTCDateTime(ii)<t_end:\n",
    "        net.append(net_temp[i])\n",
    "        sta.append(sta_temp[i])\n",
    "        evt_id.append(evt_id_temp[i])\n",
    "        start_time.append(ii)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7e896",
   "metadata": {},
   "source": [
    "## Calculating seasonal occurence of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c967e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 30})\n",
    "for name in volc_lat_lon:\n",
    "    events = []\n",
    "    stations = []\n",
    "    starttimes = []\n",
    "    networks = []\n",
    "    for i in range(0, len(start_time)):\n",
    "        try:\n",
    "            associated_volcano = df[df['Station']== sta[i]]['Volcano_Name'].values[0]\n",
    "        except: \n",
    "            associated_volcano = 'unknown'\n",
    "       \n",
    "        if associated_volcano == name and evt_id[i]!=evt_id[i-1]:\n",
    "            events.append(evt_id[i])\n",
    "            starttimes.append(start_time[i])\n",
    "            stations.append(sta[i])\n",
    "            networks.append(net[i])\n",
    "\n",
    "    periods, num_of_events = events_per_month(starttimes, events)\n",
    "\n",
    "    fig = plt.figure(figsize = (60, 10))\n",
    "    \n",
    "    for x in range(0,len(periods)):\n",
    "        if '5'<=periods[x][-1]<='9' or periods[x][-2]=='10':\n",
    "            plt.bar(periods[x], num_of_events[x], color = 'r', width = 0.4)\n",
    "        else:\n",
    "            plt.bar(periods[x],num_of_events[x], color ='b', width = 0.4)\n",
    "    plt.xlabel('year_month')\n",
    "    plt.xticks(np.arange(0, len(periods)+1, 12)) #make every year\n",
    "    plt.ylabel('No. of events')\n",
    "    plt.title('Number of surface events per month at ' + str(name))   \n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2d06c",
   "metadata": {},
   "source": [
    "Station counts as a function of time for each volcano. Permanent and permanent+temporary XD network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44999bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 30})\n",
    "for name in volc_lat_lon:\n",
    "    for i in range(0, len(start_time)):\n",
    "        try:\n",
    "            associated_volcano = df[df['Station']== sta[i]]['Volcano_Name'].values[0]\n",
    "        except: \n",
    "            associated_volcano = 'unknown'\n",
    "        if associated_volcano == 'Mt_St_Helens':\n",
    "            stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "            stations_w_temp = stations+df_xd[df_xd['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "            starts_perm = df[df['Volcano_Name'] == associated_volcano]['Start'].values.tolist() \n",
    "            ends_perm = df[df['Volcano_Name'] == associated_volcano]['End'].values.tolist()\n",
    "            starts_w_temp = starts_perm + df_xd[df_xd['Volcano_Name'] == associated_volcano]['Start'].values.tolist()\n",
    "            ends_w_temp = ends_perm + df_xd[df_xd['Volcano_Name'] == associated_volcano]['End'].values.tolist()\n",
    "\n",
    "            num_tempstas_50km = {}\n",
    "            num_permstas_50km = {}\n",
    "            for year in range (2001, 2021):\n",
    "                for month in range (1, 13):\n",
    "                    nsta = []\n",
    "                    period = str(year)+\"_\"+str(month)\n",
    "                    t0 = UTCDateTime(year, month, 1)\n",
    "                    t1 = t0+3600*24*30\n",
    "                    for i in range(0, len(starts_w_temp)):\n",
    "                        try:\n",
    "                            if UTCDateTime(starts_w_temp[i])<t1<UTCDateTime(ends_w_temp[i]):\n",
    "                                nsta.append(stations_w_temp[i])\n",
    "                        except:\n",
    "                            if UTCDateTime(starts_w_temp[i])<t1:\n",
    "                                nsta.append(stations_w_temp[i])\n",
    "                    if len(nsta) != 0:\n",
    "                        num_tempstas_50km[period]=len(nsta)\n",
    "                    if len(nsta) == 0:\n",
    "                        num__tempstas_50km[period] = 0\n",
    "                    nsta = []\n",
    "                    for i in range(0, len(starts_perm)):\n",
    "                        try:\n",
    "                            if UTCDateTime(starts_perm[i])<t1<UTCDateTime(ends_perm[i]):\n",
    "                                nsta.append(stations[i])\n",
    "                        except:\n",
    "                            if UTCDateTime(starts_perm[i])<t1:\n",
    "                                nsta.append(stations[i])\n",
    "                    if len(nsta) != 0:\n",
    "                        num_permstas_50km[period]=len(nsta)\n",
    "                    if len(nsta) == 0:\n",
    "                        num__permstas_50km[period] = 0\n",
    "\n",
    "            periods_temps = list(num_tempstas_50km.keys())\n",
    "            num_of_tempstas = list(num_tempstas_50km.values())\n",
    "            periods_perms = list(num_permstas_50km.keys())\n",
    "            num_of_permstas = list(num_permstas_50km.values())\n",
    "\n",
    "            fig = plt.figure(figsize = (60, 10))\n",
    "            for x in range(0,len(periods_temps)):\n",
    "                plt.bar(periods_temps[x], num_of_tempstas[x], color = 'b', width = 0.4)\n",
    "            plt.xlabel('year_month')\n",
    "            plt.xticks(np.arange(0, len(periods_temps)+1, 12)) #make every year\n",
    "            plt.ylabel('No. of stations')\n",
    "            plt.ylim([0,100])\n",
    "            plt.title('Number of permanent+temporary stations per month at ' + str(associated_volcano))\n",
    "\n",
    "            fig = plt.figure(figsize = (60, 10))\n",
    "            for x in range(0,len(periods_perms)):\n",
    "                plt.bar(periods_perms[x], num_of_permstas[x], color = 'r', width = 0.4)\n",
    "            plt.xlabel('year_month')\n",
    "            plt.xticks(np.arange(0, len(periods_perms)+1, 12)) #make every year\n",
    "            plt.ylabel('No. of stations')\n",
    "            plt.title('Number of permanent stations per month at ' + str(associated_volcano))\n",
    "\n",
    "    else:\n",
    "        stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "        starts_perm = df[df['Volcano_Name'] == associated_volcano]['Start'].values.tolist() \n",
    "        ends_perm = df[df['Volcano_Name'] == associated_volcano]['End'].values.tolist()\n",
    "\n",
    "        num_permstas_50km = {}\n",
    "        for year in range (2001,2021):\n",
    "            for month in range (1,13):\n",
    "                nsta = []\n",
    "                period = str(year)+\"_\"+str(month)\n",
    "                t0 = UTCDateTime(year, month, 1)\n",
    "                t1 = t0+3600*24*30\n",
    "                for i in range(0, len(starts_perm)):\n",
    "                    try:\n",
    "                        if UTCDateTime(starts_perm[i])<t1<UTCDateTime(ends_perm[i]):\n",
    "                            nsta.append(stations[i])\n",
    "                    except:\n",
    "                        if UTCDateTime(starts_perm[i])<t1:\n",
    "                            nsta.append(stations[i])\n",
    "                if len(nsta) != 0:\n",
    "                    num_permstas_50km[period]=len(nsta)\n",
    "                if len(nsta) == 0:\n",
    "                    num__permstas_50km[period] = 0\n",
    "\n",
    "        periods_perms = list(num_permstas_50km.keys())\n",
    "        num_of_permstas = list(num_permstas_50km.values())\n",
    "\n",
    "        fig = plt.figure(figsize = (60, 10))\n",
    "        for x in range(0,len(periods_perms)):\n",
    "            plt.bar(periods_perms[x], num_of_permstas[x], color = 'r', width = 0.4)\n",
    "        plt.xlabel('year_month')\n",
    "        plt.xticks(np.arange(0, len(periods_perms)+1, 12)) \n",
    "        plt.ylabel('No. of stations')\n",
    "        plt.title('Number of permanent stations per month at ' + str(associated_volcano))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fa267",
   "metadata": {},
   "source": [
    "## Calculating directivity and velocity of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c702477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evt_data = pd.DataFrame(columns = ['event_ID', 'location','location_uncertainty(m)','origin_time','direction(degrees)', \n",
    "                'direction_sharpness(degrees)''params_std_deviation', 'velocity(m/s)', 'number_of_stations'\n",
    "                'direction_snr(degrees)'])\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "for n in range(750,755):    \n",
    "    event_ID = str(evt_id[n])\n",
    "    time = UTCDateTime(start_time[n])\n",
    "    if net != 'CN' and evt_id[n]!=evt_id[n-1]:\n",
    "        reference = str(net[n]+'.'+sta[n])\n",
    "        try:\n",
    "            associated_volcano = df[df['Station']== sta[n]]['Volcano_Name'].values[0]\n",
    "        except: \n",
    "            pass\n",
    "        #get info for stations within 50km of volcano that event ocurred at\n",
    "        stations = df[df['Volcano_Name'] == associated_volcano]['Station'].values.tolist()\n",
    "        networks = df[df['Volcano_Name'] == associated_volcano]['Network'].values.tolist()\n",
    "        latitudes = df[df['Volcano_Name'] == associated_volcano]['Latitude'].values.tolist()\n",
    "        longitudes = df[df['Volcano_Name'] == associated_volcano]['Longitude'].values.tolist()\n",
    "        elevations = df[df['Volcano_Name']== associated_volcano]['Elevation'].values.tolist()\n",
    "\n",
    "        if stations.count(\"LON\")>0 and stations.count(\"LO2\")>0:\n",
    "            index = stations.index(\"LO2\")\n",
    "            del stations[index]\n",
    "            del networks[index]\n",
    "            del latitudes[index]\n",
    "            del longitudes[index]\n",
    "            del elevations[index]\n",
    "            \n",
    "        #Get all waveforms for that event based on stations and times\n",
    "        bulk = [] \n",
    "        for m in range(0, len(networks)):\n",
    "            bulk.append([networks[m], stations[m], '*', '*', time-t_before, time+t_after])\n",
    "        st = client.get_waveforms_bulk(bulk)\n",
    "        #remove unwanted data\n",
    "        for tr in st:\n",
    "            cha = tr.stats.channel\n",
    "            if cha[0:2] != 'BH' and cha[0:2] != 'EH' and cha[0:2] != 'HH':\n",
    "                st.remove(tr)\n",
    "            try:\n",
    "                if len(tr.data)/tr.stats.sampling_rate < 239.9:\n",
    "                    st.remove(tr)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        #resampling the data to 40Hz for each trace\n",
    "        st = resample(st,fs) \n",
    "        \n",
    "        #Plotting all traces for one event with channel z, SNR>10, and bandpasses between 2-12Hz\n",
    "        SNR,stas,nets,max_amp_times,durations,data_env_dict,t_diff = [],[],[],[],[],{},{}\n",
    "        fig = plt.figure(figsize = (20,50), dpi=80)\n",
    "        fig.suptitle('evtID:UW'+ event_ID+associated_volcano)\n",
    "        ax1 = plt.subplot(4,1,1)\n",
    "        iplot = 0\n",
    "        for i,ii in enumerate(st):\n",
    "            network = ii.stats.network\n",
    "            station = ii.stats.station\n",
    "            ii.detrend(type = 'demean')\n",
    "            ii.filter('bandpass',freqmin=2.0,freqmax=12.0,corners=2,zerophase=True)\n",
    "            cha = ii.stats.channel\n",
    "            starttime = ii.stats.starttime\n",
    "            max_amp_time = np.argmax(ii.data)/fs\n",
    "            signal_window = ii.copy()\n",
    "            noise_window = ii.copy()\n",
    "            signal_window.trim(starttime + t_before - 20, starttime + t_before - 20 + window)\n",
    "            noise_window.trim(starttime + t_before - window -10, starttime + t_before - 10)\n",
    "            snr = (20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                           / np.percentile(np.abs(noise_window.data),pr))/np.log(10))\n",
    "\n",
    "            if cha[-1] == 'Z' and snr>thr and 100<max_amp_time<200:\n",
    "                t = ii.times()\n",
    "                t_diff[network+'.'+station] = starttime-time \n",
    "                # enveloping the data \n",
    "                data_envelope = obspy.signal.filter.envelope(ii.data[115*fs:150*fs])\n",
    "                data_envelope /= np.max(data_envelope)\n",
    "                data_envelope += iplot*1.5\n",
    "                # finding the duration of each event\n",
    "                max_amp_times.append(max_amp_time)\n",
    "                max_amp = np.max(ii.data)\n",
    "#                 a = np.argwhere(ii.data < max_amp*.2)\n",
    "#                 for j,jj in enumerate(a):\n",
    "#                     if int(jj)-1 != int(j)-1:\n",
    "#                         duration = (max_amp_time - (int(jj)/fs))*2\n",
    "#                         durations.append(duration)\n",
    "#                         break        \n",
    "                # creating envelope data dictionary to calculate picktimes\n",
    "                data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length)\n",
    "                data_env_dict[network+'.'+station]= data_envelope\n",
    "                b,e = 115,150\n",
    "                ax1.plot(t[b*fs:e*fs],ii.data[b*fs:e*fs]/np.max(np.abs(ii.data))+iplot*1.5)\n",
    "                ax1.plot(t[115*fs:150*fs], data_envelope, color = 'k')\n",
    "                ax1.set_xlabel('time (seconds)')\n",
    "                ax1.set_xlim([b,e])\n",
    "                ax1.vlines(max_amp_time, ymin = iplot*1.5-1, ymax = iplot*1.5+1, colors = 'k',linewidths = 3)\n",
    "                plt.text(t[e*fs], iplot*1.5, str(snr))\n",
    "                plt.text(t[b*fs], iplot*1.5, station)\n",
    "                iplot = iplot+1\n",
    "                stas.append(ii.stats.station)\n",
    "                nets.append(ii.stats.network)\n",
    "                SNR.append(snr)\n",
    "            else:\n",
    "                st.remove(ii)\n",
    "\n",
    "        if len(st)<4:  \n",
    "            continue\n",
    "\n",
    "             #Get peak frequency of each event\n",
    "    #             ax2.set_title('Power Spectral Density')\n",
    "    #             ax2.set_xscale('log')\n",
    "    #             ax2.set_yscale('log')\n",
    "    #             ax2.set_xlabel('Frequency [Hz]')\n",
    "    #             ax2.set_ylabel('PSD [$(mm/s)^2$/Hz]')\n",
    "    #             ax2.grid(True)\n",
    "\n",
    "        # read and preprocess data\n",
    "        st.taper(max_percentage=0.01,max_length=20)\n",
    "        st.trim(starttime=time-20,endtime=time+30) \n",
    "\n",
    "        # make plot of spectra\n",
    "        char_freq, sharp_weight= [],[]\n",
    "        for i in range(len(stas)):\n",
    "            data = st.select(station=stas[i],component=\"Z\")[0].data*100\n",
    "            f,psd=scipy.signal.welch(data,fs=st[0].stats.sampling_rate,nperseg=81,noverlap=1)\n",
    "            #j ust get the frequencies within the filter band\n",
    "            above_low_cut = [f>low_cut]\n",
    "            below_high_cut = [f<high_cut]\n",
    "            in_band = np.logical_and(above_low_cut,below_high_cut)[0]\n",
    "            f = f[in_band]\n",
    "            psd = psd[in_band]\n",
    "#                 ax2.plot(f,psd,label=stas[i],linewidth=2)\n",
    "\n",
    "            # calculate characteristic frequency and report\n",
    "            char_freq_max = f[np.argmax(psd)]\n",
    "            char_freq_mean= np.sum(psd*f)/np.sum(psd)\n",
    "            psd_cumsum = np.cumsum(psd)\n",
    "            psd_sum = np.sum(psd)\n",
    "            char_freq_median = f[np.argmin(np.abs(psd_cumsum-psd_sum/2))]\n",
    "            char_freq.append(char_freq_mean)\n",
    "\n",
    "            # weighting the data by the spikiness of the PSD vs frequency graphs\n",
    "            ratio = (np.mean(psd)/np.max(psd))\n",
    "            sharp_weight.append(int(1/(ratio**2)*20))\n",
    "\n",
    "\n",
    "        lats, lons, elevs, r, theta = ([] for i in range(5)) \n",
    "        ref = str(nets[0]+'.'+stas[0])\n",
    "        try:\n",
    "            ref_env = data_env_dict[reference]\n",
    "        except:\n",
    "            ref_env = data_env_dict[ref]\n",
    "\n",
    "        # calculating the picktimes and shift in arrival times using envelope cross_correlation\n",
    "        pick_times, offsets, starttimes = pick_time(time, ref_env, data_env_dict,st,t_diff, t_before, fs) #calculate picktimes\n",
    "        shifts, vals = shift(pick_times, offsets, starttimes, t_diff)\n",
    "\n",
    "        iplot = 0 \n",
    "        durations_2 = []\n",
    "        for i in range(len(stas)):\n",
    "            max_amp_time = max_amp_times[i]\n",
    "            duration = (max_amp_time-vals[i])*2\n",
    "            durations_2.append(duration)\n",
    "            ax1.vlines(vals[i], ymin = iplot*1.5-.5, ymax = iplot*1.5+.5, color = colors[i])\n",
    "            ax1.vlines(max_amp_time, ymin = iplot*1.5-1, ymax = iplot*1.5+1, colors = 'k', linewidths = 3)\n",
    "            ax1.hlines(y = iplot*1.5, xmin = max_amp_time-(duration/2), xmax = max_amp_time+(duration/2), colors = 'm',linewidths = 3)\n",
    "            a = stations.index(stas[i])\n",
    "            lats.append(latitudes[a])\n",
    "            lons.append(longitudes[a])\n",
    "            elevs.append(elevations[a])\n",
    "            iplot = iplot+1\n",
    "            \n",
    "        avg_duration_2 = np.mean(durations_2)\n",
    "\n",
    "        # input necessary data for grid search\n",
    "        arrivals = shifts\n",
    "        sta_lats = lats\n",
    "        sta_lons= lons\n",
    "\n",
    "#         # define grid origin in lat,lon and grid dimensions in m\n",
    "#         lat_start = volc_grid[associated_volcano][0]\n",
    "#         lon_start = volc_grid[associated_volcano][1]\n",
    "#         side_length = volc_grid[associated_volcano][2]\n",
    "\n",
    "#         # weighting each station based on SNR, these locations will have a larger weight in the error estimate\n",
    "#         SNR_weight = [int(i) for i in SNR] \n",
    "#         # create the grid of locations\n",
    "#         sta_x = []\n",
    "#         sta_y = []\n",
    "#         for i in range(len(sta_lats)):\n",
    "#             x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "#             y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "#             sta_x.append(x_dist)\n",
    "#             sta_y.append(y_dist)\n",
    "#         x_vect = np.arange(0, side_length, step)\n",
    "#         y_vect = np.arange(0, side_length, step)\n",
    "#         t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "\n",
    "#         # carry out the gridsearch\n",
    "#         rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,1000,arrivals,SNR_weight)\n",
    "#         loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "#         print(loc_idx[0])\n",
    "#         # plot a spatial map of error for lowest-error origin time\n",
    "#         fig2,ax = plt.subplots()\n",
    "#         ax.set_title('Vs = 1000m/s')\n",
    "#         ax.scatter(x_vect[loc_idx[1]],y_vect[loc_idx[2]],s=100,marker='*',c='r')\n",
    "#         im = ax.imshow(np.log10(rss_mat[loc_idx[0],:,:].T),origin=\"lower\",extent=[0,side_length,0,side_length])\n",
    "#         fig2.colorbar(im)\n",
    "#         contours = ax.contour(x_vect,y_vect,np.log10(rss_mat[int(loc_idx[0]),:,:].T),cmap='plasma_r')\n",
    "#         ax.clabel(contours)\n",
    "#         #ax.arrow(int(x_vect[loc_idx[1]]),int(y_vect[loc_idx[2]]),dy/2,dx/2, color = 'w')\n",
    "#         plt.colorbar(contours)\n",
    "#         plt.show()\n",
    "#         # find the latitude and longitude of the location index\n",
    "#         loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "#         err_thr = np.min(np.log10(rss_mat))+.05\n",
    "#         thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "#         diameter = error_diameter(thr_array)\n",
    "#         print(loc_lat, loc_lon)\n",
    "#         print('diameter of error on the location',diameter,'meters')\n",
    "\n",
    "#         # calculating azimuth for each station with respect to the middle of the volcano\n",
    "#         for i, ii in enumerate(stas):\n",
    "#             u,b,c = (gps2dist_azimuth(loc_lat, loc_lon, lats[i], lons[i], a=6378137.0, f=0.0033528106647474805))\n",
    "#             r.append(u)\n",
    "#             theta.append(b)\n",
    "\n",
    "#         #manipulating the data\n",
    "#         data = {'azimuth_deg':theta, 'freq':char_freq, 'station':stas, 'distance_m':r, 'weight':sharp_weight, 'SNR':SNR}\n",
    "#         DF = pd.DataFrame(data, index = None)\n",
    "#         DF2 = DF.sort_values('azimuth_deg')\n",
    "\n",
    "#         #Taking out stations that are too close to the location when looking at azimuth \n",
    "#         drops = []\n",
    "#         for i in range(len(DF2)):\n",
    "#             value = DF2.loc[i,'distance_m']\n",
    "#             if value < az_thr:\n",
    "#                 drops.append(i)\n",
    "#         DF3 = DF2.drop(drops)\n",
    "#         y_data =  DF3[\"freq\"].values.tolist()\n",
    "#         Sta2 = DF3[\"station\"].values.tolist()\n",
    "#         dist2 = DF3[\"distance_m\"].values.tolist()\n",
    "#         spike_weight = DF3[\"weight\"].values.tolist()\n",
    "#         SNR2 = DF3['SNR'].values.tolist()\n",
    "#         x_data =  np.asarray(DF3[\"azimuth_deg\"].values.tolist())\n",
    "#         x_points = np.linspace(0,360, 100)\n",
    "\n",
    "#         #optimizing parameters to fit data to test_function\n",
    "#         params, params_covariance = optimize.curve_fit(test_func, np.deg2rad(x_data), y_data, p0=None)\n",
    "#         perr = np.sqrt(np.diag(params_covariance))\n",
    "#         std_deviation = str(round(perr[0],9))+','+str(round(perr[1],9))+','+str(round(perr[2],9))\n",
    "#         d = test_func(np.deg2rad(x_points), params[0], params[1], params[2])\n",
    "#         len_r = int(max(r))\n",
    "#         line_length = np.linspace(0,len_r,len_r+1)\n",
    "#         rads = np.arange(0, (2 *pi), 0.01)\n",
    "#         direction=[]\n",
    "#         if params[0]<0:\n",
    "#             direction = [(params[1]+pi) for i in range(len_r+1)]\n",
    "#         else:\n",
    "#             direction = [(params[1]) for i in range(len_r+1)]\n",
    "#         fmax = max(d)\n",
    "#         fmin = min(d)\n",
    "#         v = v_s*((fmax-fmin)/(fmax+fmin))\n",
    "\n",
    "#         dy = len_r*np.sin(direction[0])\n",
    "#         dx = len_r*np.cos(direction[0])     \n",
    "\n",
    "#         #create figure showing effects of different weights on the data\n",
    "#         fig = plt.figure(figsize = (20,7), dpi=80)\n",
    "#         ax1 = plt.subplot(1,2,1)\n",
    "#         ax1.set_ylabel('characteristic frequency(Hz)')\n",
    "#         ax1.set_xlabel('azimuth(degrees)')\n",
    "#         for i in range (0,len(Sta2)):\n",
    "#             ax1.scatter(x_data[i], y_data[i], label=Sta2[i])\n",
    "#         ax1.plot(x_data,y_data, '--', label='rawdata')\n",
    "#         ax1.legend(loc='best')\n",
    "#         ax1.set_title('Original'+str(v)+'m/s')\n",
    "#         ax1.plot(x_points, d, label='Fitted function')\n",
    "\n",
    "#         ax5= plt.subplot(1,2,2, polar=True)\n",
    "#         ax5.set_title('Original'+str(v)+'m/s')\n",
    "#         ax5.set_theta_offset(pi/2)\n",
    "#         ax5.set_theta_direction(-1)\n",
    "#         for i in range(0,len(r)):\n",
    "#             ax5.plot(np.deg2rad(theta[i]),r[i], 'g.')\n",
    "#             ax5.text(np.deg2rad(theta[i]),r[i],stas[i]) \n",
    "#         ax5.plot(direction,line_length, 'k-')  #plot the estimated direction of the event\n",
    "#         for rad in rads:\n",
    "#             ax5.plot(rad,az_thr, 'b.', markersize = 2)\n",
    "\n",
    "#         fig = plt.figure(figsize = (20,20), dpi=80) \n",
    "#         # weighted plots\n",
    "#         e,g = 1,.02857142857\n",
    "#         title = 'Sharpness'\n",
    "#         v_sharp,direction_sharp = make_weight_plts(title,x_data,y_data,sharp_weight,test_func,x_points,v_s,theta,r,stas,az_thr,e,f,g)\n",
    "#         dy_sharp = len_r*np.sin(direction_sharp[0])\n",
    "#         dx_sharp = len_r*np.cos(direction_sharp[0])    \n",
    "\n",
    "\n",
    "#         SNR_weight = [int(i) for i in SNR] #larger SNRs have higher weight\n",
    "#         e,g = 2,1\n",
    "#         title = 'SNR'\n",
    "#         v_snr,direction_snr = make_weight_plts(title,x_data,y_data,SNR_weight,test_func,x_points,v_s,theta,r,stas,az_thr,e,f,g)  \n",
    "#         dy_snr = len_r*np.sin(direction_snr[0])\n",
    "#         dx_snr = len_r*np.cos(direction_snr[0]) \n",
    "\n",
    "#         # make a dataframe of the data\n",
    "#         evt_data = evt_data.append({'event_ID':[event_ID], \n",
    "#                     'location': [str(loc_lat)+','+ str(loc_lon)],\n",
    "#                     'location_uncertainty(m)':[diameter],\n",
    "#                     'origin_time': [min(offsets)-int(loc_idx[0])],\n",
    "#                     'direction(degrees)':[np.rad2deg(direction[0])],\n",
    "#                     'direction_sharpness(degrees)':[np.rad2deg(direction_sharp[0])],\n",
    "#                     'direction_snr(degrees)':[np.rad2deg(direction_snr[0])],\n",
    "#                     'duration':[avg_duration],\n",
    "#                     'params_std_deviation':[std_deviation], \n",
    "#                     'velocity(m/s)':[v], \n",
    "#                     'number_of_stations':[len(stas)]}, ignore_index = True)\n",
    "\n",
    "#            #plt.savefig('evtID:UW'+ event_ID+associated_volcano+'.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd0cb9",
   "metadata": {},
   "source": [
    "Plots the error contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33516950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with an earthquake to see what happens\n",
    "# run through a lot of events, make the data frame, and then make a lot of pngs using the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30fef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = dem_data_dict[associated_volcano]['crs']\n",
    "data = dem_data_dict[associated_volcano]['data']\n",
    "\n",
    "\n",
    "p2 = Proj(crs,preserve_units=False)\n",
    "p1 = Proj(proj='latlong',preserve_units=False)\n",
    "# gives the lower left grid point in the grid search\n",
    "left_x,bottom_y = transform(p1,p2,volc_grid[associated_volcano][1],volc_grid[associated_volcano][0])\n",
    "# gives the left right, bottom, top of the grid\n",
    "grid_bounds = [left_x, left_x+volc_grid[associated_volcano][2], bottom_y, bottom_y+volc_grid[associated_volcano][2]]\n",
    "left, right = dem_data_dict[associated_volcano]['left'],dem_data_dict[associated_volcano]['right']\n",
    "bottom, top = dem_data_dict[associated_volcano]['bottom'],dem_data_dict[associated_volcano]['top']\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(7,7))\n",
    "ax.imshow(data,extent=[left, right, bottom, top],cmap='jet')\n",
    "ax.imshow(np.log10(rss_mat[loc_idx[0],:,:].T),origin='lower',extent=grid_bounds,alpha = .5)\n",
    "contour_x,contour_y = np.meshgrid(left_x+x_vect,bottom_y+y_vect)\n",
    "contours = ax.contour(contour_x,contour_y,np.log10(rss_mat[int(loc_idx[0]),:,:].T),cmap='plasma')\n",
    "ax.clabel(contours)\n",
    "cbar = plt.colorbar(contours)\n",
    "cbar.set_label('elevation(m)', rotation=270)\n",
    "\n",
    "#plotting the stations on top of this as white triangles\n",
    "for i, ii in enumerate(stas):\n",
    "    sta_x,sta_y = transform(p1,p2,lons[i],lats[i])\n",
    "    ax.plot(sta_x,sta_y, c='k', marker=\"^\")\n",
    "    ax.text(sta_x,sta_y,ii, c='k')\n",
    "\n",
    "#Crater Lake\n",
    "ax.set_xlim(left+60000,right)\n",
    "ax.set_ylim(bottom+90000,top)\n",
    "\n",
    "# getting lat and lon tick marks on the axis\n",
    "tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "\n",
    "ticks_x = []\n",
    "ticks_y = []\n",
    "for i in range(len(tick_lons)):\n",
    "    tick_x,tick_y = transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "    ticks_x.append(tick_x)\n",
    "    ticks_y.append(tick_y)\n",
    "    tick_lons[i]=str(tick_lons[i])\n",
    "    tick_lats[i]=str(tick_lats[i])\n",
    "\n",
    "ax.set_title('Location Error')\n",
    "ax.set_xlabel('longitudes')\n",
    "ax.set_ylabel('latitudes')\n",
    "ax.set_xticks(ticks_x)\n",
    "ax.set_xticklabels(tick_lons)\n",
    "ax.set_yticks(ticks_y)\n",
    "ax.set_yticklabels(tick_lats)\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8dc7d",
   "metadata": {},
   "source": [
    "Plot the direction of Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = dem_data_dict[associated_volcano]['crs']\n",
    "data = dem_data_dict[associated_volcano]['data']\n",
    "\n",
    "\n",
    "p2 = Proj(crs,preserve_units=False)\n",
    "p1 = Proj(proj='latlong',preserve_units=False)\n",
    "# gives the lower left grid point in the grid search\n",
    "left_x,bottom_y = transform(p1,p2,volc_grid[associated_volcano][1],volc_grid[associated_volcano][0])\n",
    "# gives the left right, bottom, top of the grid\n",
    "#grid_bounds = [left_x, left_x+volc_grid[associated_volcano][2], bottom_y, bottom_y+volc_grid[associated_volcano][2]]\n",
    "left, right = dem_data_dict[associated_volcano]['left'],dem_data_dict[associated_volcano]['right']\n",
    "bottom, top = dem_data_dict[associated_volcano]['bottom'],dem_data_dict[associated_volcano]['top']\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(7,7))\n",
    "a = ax.imshow(data,extent=[left, right, bottom, top],cmap='Greys')\n",
    "cbar = plt.colorbar(a)\n",
    "cbar.set_label('elevation(m)', rotation=270)\n",
    "loc_x,loc_y = transform(p1,p2,loc_lon,loc_lat)\n",
    "duration = avg_duration\n",
    "length_factor = duration/10\n",
    "plt.arrow(loc_x,loc_y,dy*length_factor,dx*length_factor, color='w', width=200, label='no weight')\n",
    "plt.arrow(loc_x,loc_y,dy_sharp*length_factor,dx_sharp*length_factor, color='k', width=200, label='sharpness')\n",
    "plt.arrow(loc_x,loc_y,dy_snr*length_factor,dx_snr*length_factor, color='m', width=200, label='snr')\n",
    "ax.legend()\n",
    "\n",
    "#plotting the stations on top of this as white triangles\n",
    "for i, ii in enumerate(stas):\n",
    "    sta_x,sta_y = transform(p1,p2,lons[i],lats[i])\n",
    "    ax.plot(sta_x,sta_y, c='k', marker=\"^\")\n",
    "    ax.text(sta_x,sta_y,ii, c='k')\n",
    "\n",
    "ax.set_xlim(left+8000,right)\n",
    "ax.set_ylim(bottom+19000,top-7000)\n",
    "\n",
    "# getting lat and lon tick marks on the axis\n",
    "# tick_lons = lat_lon_dict[associated_volcano]['tick_lons']\n",
    "# tick_lats = lat_lon_dict[associated_volcano]['tick_lats']\n",
    "\n",
    "# ticks_x = []\n",
    "# ticks_y = []\n",
    "# for i in range(len(tick_lons)):\n",
    "#     tick_x,tick_y = transform(p1,p2,tick_lons[i],tick_lats[i])\n",
    "#     ticks_x.append(tick_x)\n",
    "#     ticks_y.append(tick_y)\n",
    "#     tick_lons[i]=str(tick_lons[i])\n",
    "#     tick_lats[i]=str(tick_lats[i])\n",
    "\n",
    "# ax.set_title('Direction of flow based on different weights')\n",
    "# ax.set_xlabel('longitudes')\n",
    "# ax.set_ylabel('latitudes')\n",
    "# ax.set_xticks(ticks_x)\n",
    "# ax.set_xticklabels(tick_lons)\n",
    "# ax.set_yticks(ticks_y)\n",
    "# ax.set_yticklabels(tick_lats)\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0788d24",
   "metadata": {},
   "source": [
    "## Events in time window of XD temporary station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262cd0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_volcano = 'Mt_St_Helens'\n",
    "#events during temp station time period:\n",
    "# 2014,7,27,19,14,52      eventID = 1780258   reference = CC.SEP\n",
    "# 2014,7,27,18,39,47      eventID = 1780253   reference = CC.SUG\n",
    "# 2014,7,27,13,20,43      eventID = 1781583   reference = CC.SEP\n",
    "# 2014,5,25,16,49,52      eventID = 1778978   reference = CC.SEP\n",
    "# 2014,7,25,6,49,53        eventID = 1779148   reference = CC.SEP\n",
    "# 2014,7,24,20,8,10        eventID = 1777563   reference = CC.SEP\n",
    "# 2014,7,22,16,51,36      eventID = 1792948   reference = CC.SEP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
